# æ–‡æœ¬é¢„å¤„ç†ï¼šä»é›¶æŒæ¡è¿™ä¸€è‡ªç„¶è¯­è¨€å¤„ç†æ ¸å¿ƒæŠ€æœ¯

## 1. åŸºç¡€æ¦‚å¿µç†è§£

### ä»€ä¹ˆæ˜¯æ–‡æœ¬é¢„å¤„ç†ï¼Ÿ

**æ–‡æœ¬é¢„å¤„ç†**æ˜¯å°†åŸå§‹æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥å¤„ç†çš„æ ¼å¼çš„è¿‡ç¨‹ã€‚è¿™æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ç®¡é“ä¸­çš„ç¬¬ä¸€æ­¥ï¼Œä¹Ÿæ˜¯æœ€å…³é”®çš„æ­¥éª¤ä¹‹ä¸€ã€‚åŸå§‹æ–‡æœ¬é€šå¸¸åŒ…å«å™ªéŸ³ã€ä¸è§„åˆ™æ ¼å¼ã€æ‹¼å†™é”™è¯¯ç­‰é—®é¢˜ï¼Œé¢„å¤„ç†çš„ç›®çš„æ˜¯æ¸…ç†å’Œæ ‡å‡†åŒ–è¿™äº›æ–‡æœ¬æ•°æ®ã€‚

### ä¸ºä»€ä¹ˆæ–‡æœ¬é¢„å¤„ç†å¦‚æ­¤é‡è¦ï¼Ÿ

1. **æé«˜æ¨¡å‹æ€§èƒ½**ï¼šå¹²å‡€ã€æ ‡å‡†åŒ–çš„æ•°æ®å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹å‡†ç¡®æ€§å’Œæ•ˆç‡
2. **å‡å°‘å™ªå£°å½±å“**ï¼šè¿‡æ»¤æ— æ„ä¹‰å†…å®¹ï¼Œä½¿æ¨¡å‹ä¸“æ³¨äºæœ‰ç”¨ä¿¡æ¯
3. **é™ä½è®¡ç®—æˆæœ¬**ï¼šå‡å°æ•°æ®ç»´åº¦ï¼Œæé«˜å¤„ç†æ•ˆç‡
4. **æé«˜å¯è§£é‡Šæ€§**ï¼šæ ‡å‡†åŒ–æ•°æ®ä½¿åˆ†æç»“æœæ›´å®¹æ˜“ç†è§£

### æ–‡æœ¬é¢„å¤„ç†çš„åŸºæœ¬æµç¨‹

![æ–‡æœ¬é¢„å¤„ç†æµç¨‹](https://i.imgur.com/WfNjnrQ.png)

ä¸€ä¸ªå®Œæ•´çš„æ–‡æœ¬é¢„å¤„ç†ç®¡é“é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š

1. **æ–‡æœ¬æ”¶é›†å’Œå¯¼å…¥**ï¼šä»å„ç§æ¥æºè·å–æ–‡æœ¬æ•°æ®
2. **æ–‡æœ¬æ¸…æ´—**ï¼šåˆ é™¤æˆ–æ›¿æ¢ä¸éœ€è¦çš„å­—ç¬¦ã€æ ‡è®°å’Œå™ªå£°
3. **æ–‡æœ¬è§„èŒƒåŒ–**ï¼šå°†ä¸åŒè¡¨è¾¾å½¢å¼çš„ç›¸åŒå†…å®¹è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
4. **åˆ†è¯**ï¼šå°†æ–‡æœ¬åˆ†å‰²æˆå•è¯æˆ–å­è¯å•å…ƒ
5. **åœç”¨è¯ç§»é™¤**ï¼šè¿‡æ»¤å¸¸è§ä½†ä¿¡æ¯é‡å°‘çš„è¯ï¼ˆå¦‚"the"ã€"is"ã€"and"ç­‰ï¼‰
6. **è¯å¹²æå–/è¯å½¢è¿˜åŸ**ï¼šå°†è¯è½¬æ¢ä¸ºå…¶åŸºæœ¬å½¢å¼
7. **ç‰¹å¾æå–/å‘é‡åŒ–**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤º
8. **ç‰¹å¾é€‰æ‹©/é™ç»´**ï¼šé€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾æˆ–å‡å°‘ç‰¹å¾ç»´åº¦

## 2. æŠ€æœ¯ç»†èŠ‚æ¢ç´¢

### 2.1 æ–‡æœ¬æ¸…æ´—

æ–‡æœ¬æ¸…æ´—æ˜¯ç§»é™¤æˆ–æ›¿æ¢æ–‡æœ¬ä¸­ä¸éœ€è¦çš„éƒ¨åˆ†çš„è¿‡ç¨‹ã€‚

#### å¸¸è§æ¸…æ´—æ“ä½œ

1. **åˆ é™¤HTMLæ ‡ç­¾**ï¼šç½‘ç»œæ–‡æœ¬å¸¸åŒ…å«HTMLæ ‡è®°ï¼Œéœ€è¦å»é™¤

```python
import re

def remove_html_tags(text):
    """ç§»é™¤HTMLæ ‡ç­¾"""
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

html_text = "<p>è¿™æ˜¯<b>ä¸€ä¸ª</b>ä¾‹å­</p>"
print(remove_html_tags(html_text))  # è¾“å‡º: è¿™æ˜¯ä¸€ä¸ªä¾‹å­
```

2. **åˆ é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—**ï¼šæ ¹æ®éœ€è¦ä¿ç•™æˆ–ç§»é™¤

```python
def remove_special_chars(text, remove_digits=False):
    """ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¯é€‰ç§»é™¤æ•°å­—"""
    pattern = r'[^a-zA-Z0-9\s]' if not remove_digits else r'[^a-zA-Z\s]'
    text = re.sub(pattern, '', text)
    return text

text = "Hello! This is a sample text with numbers (123) and symbols #@%."
print(remove_special_chars(text))  # ä¿ç•™æ•°å­—
print(remove_special_chars(text, remove_digits=True))  # ç§»é™¤æ•°å­—
```

3. **å¤§å°å†™è½¬æ¢**ï¼šé€šå¸¸è½¬ä¸ºå°å†™ä»¥å‡å°‘è¯æ±‡é‡

```python
def convert_to_lowercase(text):
    """è½¬æ¢ä¸ºå°å†™"""
    return text.lower()

text = "Text Preprocessing IS Important"
print(convert_to_lowercase(text))  # è¾“å‡º: text preprocessing is important
```

4. **åˆ é™¤å¤šä½™ç©ºæ ¼**ï¼šæ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦

```python
def remove_extra_spaces(text):
    """åˆ é™¤å¤šä½™ç©ºæ ¼"""
    return re.sub(r'\s+', ' ', text).strip()

text = "Too    many    spaces    here."
print(remove_extra_spaces(text))  # è¾“å‡º: Too many spaces here.
```

5. **åˆ é™¤URLå’Œç”µå­é‚®ä»¶åœ°å€**ï¼š

```python
def remove_urls_emails(text):
    """ç§»é™¤URLå’Œç”µå­é‚®ä»¶åœ°å€"""
    # URLæ¨¡å¼
    url_pattern = r'https?://\S+|www\.\S+'
    # ç”µå­é‚®ä»¶æ¨¡å¼
    email_pattern = r'\S+@\S+'
    
    text = re.sub(url_pattern, '', text)
    text = re.sub(email_pattern, '', text)
    return text

text = "è®¿é—® https://example.com æˆ–è”ç³» user@example.com è·å–å¸®åŠ©"
print(remove_urls_emails(text))  # è¾“å‡º: è®¿é—®  æˆ–è”ç³»  è·å–å¸®åŠ©
```

### 2.2 æ–‡æœ¬è§„èŒƒåŒ–

æ–‡æœ¬è§„èŒƒåŒ–æ˜¯å°†ä¸åŒå½¢å¼çš„ç›¸åŒå•è¯è½¬æ¢ä¸ºæ ‡å‡†å½¢å¼çš„è¿‡ç¨‹ã€‚

#### è¯å¹²æå–(Stemming)

è¯å¹²æå–é€šè¿‡åˆ é™¤è¯ç¼€æ¥è·å–å•è¯çš„è¯å¹²ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè§„åˆ™çš„å¿«é€Ÿä½†ä¸ç²¾ç¡®çš„æ–¹æ³•ã€‚

```python
from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer

def compare_stemmers(words):
    """æ¯”è¾ƒä¸åŒçš„è¯å¹²æå–å™¨"""
    porter = PorterStemmer()
    lancaster = LancasterStemmer()
    snowball = SnowballStemmer('english')
    
    print("åŸå§‹è¯\t\tPorter\t\tLancaster\tSnowball")
    print("-" * 60)
    
    for word in words:
        print(f"{word}\t\t{porter.stem(word)}\t\t{lancaster.stem(word)}\t\t{snowball.stem(word)}")

# æµ‹è¯•ä¸åŒè¯å¹²æå–å™¨
words = ["running", "runs", "runner", "ran", "easily", "fairly"]
compare_stemmers(words)
```

è¿è¡Œç»“æœç¤ºä¾‹ï¼š
```
åŸå§‹è¯          Porter          Lancaster       Snowball
------------------------------------------------------------
running         run             run             run
runs            run             run             run
runner          runner          run             runner
ran             ran             ran             ran
easily          easili          easy            easili
fairly          fairli          fair            fair
```

#### è¯å½¢è¿˜åŸ(Lemmatization)

è¯å½¢è¿˜åŸå°†å•è¯è½¬æ¢ä¸ºå…¶åŸºæœ¬è¯å…¸å½¢å¼ï¼ˆè¯å…ƒï¼‰ï¼Œé€šå¸¸åŸºäºè¯å…¸ï¼Œç²¾ç¡®åº¦æ›´é«˜ä½†é€Ÿåº¦è¾ƒæ…¢ã€‚

```python
import nltk
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

def lemmatize_words(words):
    """ä½¿ç”¨WordNetè¯å½¢è¿˜åŸå™¨"""
    lemmatizer = WordNetLemmatizer()
    
    print("åŸå§‹è¯\t\tåè¯å½¢å¼\t\tåŠ¨è¯å½¢å¼")
    print("-" * 50)
    
    for word in words:
        print(f"{word}\t\t{lemmatizer.lemmatize(word)}\t\t{lemmatizer.lemmatize(word, pos='v')}")

# æµ‹è¯•è¯å½¢è¿˜åŸ
words = ["running", "studies", "better", "worst", "caring", "forests"]
lemmatize_words(words)
```

è¿è¡Œç»“æœç¤ºä¾‹ï¼š
```
åŸå§‹è¯          åè¯å½¢å¼         åŠ¨è¯å½¢å¼
--------------------------------------------------
running         running         run
studies         study           study
better          better          better
worst           worst           worst
caring          caring          care
forests         forest          forest
```

#### æ‹¼å†™çº æ­£

å¤„ç†é”™åˆ«å­—å’Œæ‹¼å†™å˜ä½“ï¼š

```python
from spellchecker import SpellChecker

def correct_spelling(text):
    """ç®€å•æ‹¼å†™çº æ­£ç¤ºä¾‹"""
    spell = SpellChecker()
    corrected_text = []
    
    for word in text.split():
        # ä¿ç•™æ ‡ç‚¹
        punctuation = ''
        if not word[-1].isalnum():
            punctuation = word[-1]
            word = word[:-1]
        
        # çº æ­£æ‹¼å†™
        corrected_word = spell.correction(word)
        if corrected_word:
            corrected_text.append(corrected_word + punctuation)
        else:
            corrected_text.append(word + punctuation)
    
    return ' '.join(corrected_text)

misspelled_text = "Thiss is ann examplle of misspeled text."
print(correct_spelling(misspelled_text))  # è¾“å‡º: "This is an example of misspelled text."
```

#### æ–‡æœ¬è§„èŒƒåŒ–ä¸­çš„ç‰¹æ®Šæƒ…å†µå¤„ç†

1. **ç¼©ç•¥è¯­æ‰©å±•**ï¼šå°†ç¼©ç•¥è¯­å±•å¼€ä¸ºå®Œæ•´å½¢å¼

```python
def expand_contractions(text):
    """æ‰©å±•è‹±æ–‡å¸¸è§ç¼©ç•¥è¯­"""
    contractions_dict = {
        "don't": "do not", "doesn't": "does not", "didn't": "did not",
        "can't": "cannot", "won't": "will not", "shouldn't": "should not",
        "I'm": "I am", "you're": "you are", "he's": "he is",
        "we're": "we are", "they're": "they are", "it's": "it is"
    }
    
    for contraction, expansion in contractions_dict.items():
        text = text.replace(contraction, expansion)
    return text

text = "I don't think it's going to work if you're not careful."
print(expand_contractions(text))
# è¾“å‡º: "I do not think it is going to work if you are not careful."
```

2. **æ–‡æœ¬æ ‡å‡†åŒ–**ï¼šç»Ÿä¸€è¡¨è¾¾æ–¹å¼

```python
import unicodedata

def normalize_unicode(text):
    """Unicodeæ ‡å‡†åŒ–ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œå˜éŸ³ç¬¦å·"""
    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')

text = "CafÃ© has a rÃ©sumÃ© with naÃ¯ve coÃ¶rdination."
print(normalize_unicode(text))  # è¾“å‡º: "Cafe has a resume with naive coordination."
```

### 2.3 åˆ†è¯(Tokenization)

åˆ†è¯æ˜¯å°†æ–‡æœ¬åˆ†å‰²æˆæ›´å°å•å…ƒï¼ˆå¦‚å•è¯ã€å¥å­æˆ–å­è¯ï¼‰çš„è¿‡ç¨‹ã€‚è¿™äº›å•å…ƒç§°ä¸º"æ ‡è®°"(tokens)ã€‚

#### å•è¯åˆ†è¯

```python
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize, TreebankWordTokenizer, WhitespaceTokenizer

def compare_word_tokenizers(text):
    """æ¯”è¾ƒä¸åŒå•è¯åˆ†è¯å™¨"""
    # NLTKé»˜è®¤åˆ†è¯å™¨
    nltk_tokens = word_tokenize(text)
    
    # Treebankåˆ†è¯å™¨
    treebank_tokenizer = TreebankWordTokenizer()
    treebank_tokens = treebank_tokenizer.tokenize(text)
    
    # ç©ºæ ¼åˆ†è¯å™¨
    whitespace_tokenizer = WhitespaceTokenizer()
    whitespace_tokens = whitespace_tokenizer.tokenize(text)
    
    print("NLTKåˆ†è¯å™¨:", nltk_tokens)
    print("Treebankåˆ†è¯å™¨:", treebank_tokens)
    print("ç©ºæ ¼åˆ†è¯å™¨:", whitespace_tokens)

text = "Hello world! This is a test, with punctuation; and symbols."
compare_word_tokenizers(text)
```

#### å¥å­åˆ†è¯

```python
from nltk.tokenize import sent_tokenize

def sentence_tokenize(text):
    """å¥å­çº§åˆ†è¯"""
    sentences = sent_tokenize(text)
    for i, sent in enumerate(sentences, 1):
        print(f"å¥å­ {i}: {sent}")

long_text = """è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒå…³æ³¨è®¡ç®—æœºä¸äººç±»è¯­è¨€çš„äº¤äº’ã€‚
NLPæŠ€æœ¯è¢«å¹¿æ³›åº”ç”¨äºæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡ã€‚è¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼"""

sentence_tokenize(long_text)
```

#### å­è¯åˆ†è¯(Subword Tokenization)

å­è¯åˆ†è¯å°†å•è¯åˆ†æˆæ›´å°çš„å•å…ƒï¼Œé€‚ç”¨äºå¤„ç†æœªè§è¿‡çš„è¯å’Œå¤åˆè¯ã€‚

```python
# ä½¿ç”¨SentencePiece(éœ€è¦å®‰è£…: pip install sentencepiece)
import sentencepiece as spm

def train_and_use_sentencepiece(text_file, model_prefix, vocab_size=1000):
    """è®­ç»ƒå¹¶ä½¿ç”¨SentencePieceåˆ†è¯å™¨"""
    # è®­ç»ƒæ¨¡å‹
    spm.SentencePieceTrainer.train(
        f'--input={text_file} --model_prefix={model_prefix} --vocab_size={vocab_size}'
    )
    
    # åŠ è½½æ¨¡å‹
    sp = spm.SentencePieceProcessor()
    sp.load(f'{model_prefix}.model')
    
    # è¯»å–åŸå§‹æ–‡æœ¬è¿›è¡Œæµ‹è¯•
    with open(text_file, 'r', encoding='utf-8') as f:
        sample_text = f.read()[:100]  # ä»…ä½¿ç”¨å‰100ä¸ªå­—ç¬¦ä½œä¸ºç¤ºä¾‹
    
    # ç¼–ç å’Œè§£ç 
    encoded = sp.encode_as_pieces(sample_text)
    decoded = sp.decode(encoded)
    
    print("åŸæ–‡:", sample_text)
    print("åˆ†è¯ç»“æœ:", encoded)
    print("è§£ç å:", decoded)
    
    return sp

# æ³¨: éœ€è¦å…ˆå°†æ–‡æœ¬ä¿å­˜åˆ°æ–‡ä»¶ä¸­
# with open('sample.txt', 'w', encoding='utf-8') as f:
#     f.write("è¿™æ˜¯ç”¨äºè®­ç»ƒå­è¯åˆ†è¯å™¨çš„ç¤ºä¾‹æ–‡æœ¬ã€‚å®ƒåº”è¯¥åŒ…å«è¶³å¤Ÿçš„å†…å®¹æ¥å­¦ä¹ å¸¸è§æ¨¡å¼ã€‚")
#
# tokenizer = train_and_use_sentencepiece('sample.txt', 'subword_model')
```

### 2.4 åœç”¨è¯ç§»é™¤

åœç”¨è¯æ˜¯å‡ºç°é¢‘ç‡é«˜ä½†æä¾›å¾ˆå°‘è¯­ä¹‰ä»·å€¼çš„è¯ï¼ˆå¦‚"the"ã€"a"ã€"is"ç­‰ï¼‰ã€‚

```python
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

def remove_stopwords(text, language='english', custom_stopwords=None):
    """ç§»é™¤åœç”¨è¯"""
    # è·å–åœç”¨è¯åˆ—è¡¨
    stop_words = set(stopwords.words(language))
    
    # æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯
    if custom_stopwords:
        stop_words.update(custom_stopwords)
    
    # åˆ†è¯
    word_tokens = word_tokenize(text)
    
    # è¿‡æ»¤åœç”¨è¯
    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]
    
    return ' '.join(filtered_text)

text = "This is an example sentence demonstrating stop word removal."
print("åŸå§‹æ–‡æœ¬:", text)
print("ç§»é™¤åœç”¨è¯å:", remove_stopwords(text))

# ä¸­æ–‡ç¤ºä¾‹
nltk.download('stopwords')
from nltk.corpus import stopwords

def remove_chinese_stopwords(text, custom_stopwords=None):
    """ç§»é™¤ä¸­æ–‡åœç”¨è¯"""
    # åˆ›å»ºä¸­æ–‡åœç”¨è¯åˆ—è¡¨(ç¤ºä¾‹)
    chinese_stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'ä»¬', 'åˆ°', 'ä¸€', 'ä¸Š', 'è¿™', 'ä¸º', 'é‚£'}
    
    # æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯
    if custom_stopwords:
        chinese_stopwords.update(custom_stopwords)
    
    # ä½¿ç”¨jiebaåˆ†è¯(éœ€è¦å®‰è£…: pip install jieba)
    import jieba
    word_tokens = list(jieba.cut(text))
    
    # è¿‡æ»¤åœç”¨è¯
    filtered_text = [word for word in word_tokens if word not in chinese_stopwords]
    
    return ''.join(filtered_text)

chinese_text = "æˆ‘æ˜¯ä¸€ä¸ªä¸­æ–‡å¥å­ï¼Œç”¨æ¥æ¼”ç¤ºåœç”¨è¯çš„ç§»é™¤æ•ˆæœã€‚"
print("åŸå§‹ä¸­æ–‡:", chinese_text)
print("ç§»é™¤åœç”¨è¯å:", remove_chinese_stopwords(chinese_text))
```

### 2.5 æ–‡æœ¬å‘é‡åŒ–

å‘é‡åŒ–æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡çš„è¿‡ç¨‹ï¼Œä½¿æœºå™¨å­¦ä¹ ç®—æ³•èƒ½å¤Ÿå¤„ç†æ–‡æœ¬æ•°æ®ã€‚

#### è¯è¢‹æ¨¡å‹(Bag of Words)

```python
from sklearn.feature_extraction.text import CountVectorizer

def bag_of_words_example():
    """è¯è¢‹æ¨¡å‹ç¤ºä¾‹"""
    corpus = [
        'This is the first document.',
        'This document is the second document.',
        'And this is the third one.',
        'Is this the first document?'
    ]
    
    # åˆ›å»ºè¯è¢‹æ¨¡å‹
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(corpus)
    
    # è·å–ç‰¹å¾åï¼ˆè¯æ±‡è¡¨ï¼‰
    feature_names = vectorizer.get_feature_names_out()
    
    print("è¯æ±‡è¡¨:")
    print(feature_names)
    
    print("\næ–‡æ¡£-è¯çŸ©é˜µ:")
    print(X.toarray())
    
    # æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„è¯é¢‘
    print("\nç¬¬ä¸€ä¸ªæ–‡æ¡£çš„è¯é¢‘:")
    for word, count in zip(feature_names, X.toarray()[0]):
        if count > 0:
            print(f"{word}: {count}")

bag_of_words_example()
```

#### TF-IDF (è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡)

```python
from sklearn.feature_extraction.text import TfidfVectorizer

def tfidf_example():
    """TF-IDFç¤ºä¾‹"""
    corpus = [
        'This is the first document.',
        'This document is the second document.',
        'And this is the third one.',
        'Is this the first document?'
    ]
    
    # åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(corpus)
    
    # è·å–ç‰¹å¾å
    feature_names = vectorizer.get_feature_names_out()
    
    print("è¯æ±‡è¡¨:")
    print(feature_names)
    
    print("\nTF-IDFçŸ©é˜µ:")
    print(X.toarray())
    
    # æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„TF-IDFå€¼
    print("\nç¬¬ä¸€ä¸ªæ–‡æ¡£çš„TF-IDFå€¼:")
    for word, tfidf in zip(feature_names, X.toarray()[0]):
        if tfidf > 0:
            print(f"{word}: {tfidf:.4f}")

tfidf_example()
```

#### Word2Vecè¯åµŒå…¥

```python
from gensim.models import Word2Vec

def word2vec_example():
    """Word2Vecç¤ºä¾‹"""
    # å‡†å¤‡è®­ç»ƒè¯­æ–™åº“
    sentences = [
        ['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],
        ['this', 'is', 'the', 'second', 'sentence'],
        ['yet', 'another', 'sentence'],
        ['one', 'more', 'sentence'],
        ['and', 'the', 'final', 'sentence']
    ]
    
    # è®­ç»ƒWord2Vecæ¨¡å‹
    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
    
    # ä¿å­˜å’ŒåŠ è½½æ¨¡å‹
    model.save("word2vec.model")
    new_model = Word2Vec.load("word2vec.model")
    
    # è·å–è¯å‘é‡
    vector = new_model.wv['sentence']
    print("'sentence'çš„è¯å‘é‡(å‰10ç»´):")
    print(vector[:10])
    
    # æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„è¯
    similar_words = new_model.wv.most_similar('sentence', topn=3)
    print("\nä¸'sentence'æœ€ç›¸ä¼¼çš„è¯:")
    for word, similarity in similar_words:
        print(f"{word}: {similarity:.4f}")

    # åˆ é™¤ä¸´æ—¶æ¨¡å‹æ–‡ä»¶
    import os
    os.remove("word2vec.model")
    os.remove("word2vec.model.vectors.npy")

word2vec_example()
```

## 3. å®è·µä¸å®ç°

### 3.1 å®Œæ•´çš„æ–‡æœ¬é¢„å¤„ç†ç®¡é“

ä¸‹é¢æ˜¯ä¸€ä¸ªé›†æˆå¤šç§é¢„å¤„ç†æŠ€æœ¯çš„ç¤ºä¾‹ï¼š

```python
import re
import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pandas as pd

# ä¸‹è½½å¿…è¦çš„NLTKèµ„æº
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

class TextPreprocessor:
    """å®Œæ•´çš„æ–‡æœ¬é¢„å¤„ç†ç®¡é“"""
    
    def __init__(self, language='english', remove_stopwords=True, do_lemmatization=True):
        self.language = language
        self.remove_stopwords = remove_stopwords
        self.do_lemmatization = do_lemmatization
        self.stopwords = set(stopwords.words(language)) if remove_stopwords else set()
        self.lemmatizer = WordNetLemmatizer() if do_lemmatization else None
    
    def preprocess(self, text):
        """åº”ç”¨å®Œæ•´çš„é¢„å¤„ç†ç®¡é“"""
        if not text or not isinstance(text, str):
            return ""
        
        # è½¬æ¢ä¸ºå°å†™
        text = text.lower()
        
        # åˆ é™¤URL
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        
        # åˆ é™¤ç”µå­é‚®ä»¶
        text = re.sub(r'\S+@\S+', '', text)
        
        # åˆ é™¤æ ‡ç‚¹ç¬¦å·
        text = text.translate(str.maketrans('', '', string.punctuation))
        
        # åˆ é™¤æ•°å­—
        text = re.sub(r'\d+', '', text)
        
        # åˆ é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        
        # åˆ†è¯
        tokens = word_tokenize(text)
        
        # ç§»é™¤åœç”¨è¯
        if self.remove_stopwords:
            tokens = [word for word in tokens if word not in self.stopwords]
        
        # è¯å½¢è¿˜åŸ
        if self.do_lemmatization:
            tokens = [self.lemmatizer.lemmatize(word) for word in tokens]
        
        # é‡æ–°ç»„åˆä¸ºæ–‡æœ¬
        processed_text = ' '.join(tokens)
        
        return processed_text
    
    def preprocess_dataframe(self, df, text_column):
        """é¢„å¤„ç†DataFrameä¸­çš„æ–‡æœ¬åˆ—"""
        df['processed_text'] = df[text_column].apply(self.preprocess)
        return df

# ä½¿ç”¨ç¤ºä¾‹
preprocessor = TextPreprocessor()

# é¢„å¤„ç†å•ä¸ªæ–‡æœ¬
sample_text = "Hello! This is an example text with numbers (123) and a URL: https://example.com."
processed = preprocessor.preprocess(sample_text)
print("åŸæ–‡:", sample_text)
print("å¤„ç†å:", processed)

# é¢„å¤„ç†DataFrame(å‡è®¾æœ‰ä¸€ä¸ªåŒ…å«æ–‡æœ¬çš„DataFrame)
data = {
    'id': [1, 2, 3],
    'text': [
        "First example with some punctuation!",
        "Second example with a URL: https://example.org",
        "Third example with numbers 42 and special chars @#$"
    ]
}
df = pd.DataFrame(data)
processed_df = preprocessor.preprocess_dataframe(df, 'text')
print("\nDataFrameå¤„ç†ç»“æœ:")
print(processed_df)
```

### 3.2 ä½¿ç”¨spaCyè¿›è¡Œæ›´é«˜çº§çš„é¢„å¤„ç†

spaCyæ˜¯ä¸€ä¸ªæ›´ç°ä»£çš„NLPåº“ï¼Œæä¾›äº†é«˜çº§çš„é¢„å¤„ç†åŠŸèƒ½ï¼š

```python
# éœ€è¦å®‰è£…: pip install spacy
# è¿˜éœ€è¦ä¸‹è½½è¯­è¨€æ¨¡å‹: python -m spacy download en_core_web_sm
import spacy

def preprocess_with_spacy(text):
    """ä½¿ç”¨spaCyè¿›è¡Œæ–‡æœ¬é¢„å¤„ç†"""
    # åŠ è½½spaCyæ¨¡å‹
    nlp = spacy.load("en_core_web_sm")
    
    # è§£ææ–‡æœ¬
    doc = nlp(text)
    
    # åŸºæœ¬é¢„å¤„ç†
    tokens = []
    for token in doc:
        # è¿‡æ»¤æ‰æ ‡ç‚¹å’Œåœç”¨è¯
        if not token.is_punct and not token.is_stop:
            # ä½¿ç”¨è¯å½¢è¿˜åŸ
            tokens.append(token.lemma_)
    
    return " ".join(tokens)

def analyze_text_with_spacy(text):
    """ä½¿ç”¨spaCyåˆ†ææ–‡æœ¬"""
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)
    
    print("TokenText\tPOS\tLemma\tIsStop")
    print("-" * 50)
    
    for token in doc:
        print(f"{token.text}\t{token.pos_}\t{token.lemma_}\t{token.is_stop}")
    
    print("\nå‘½åå®ä½“è¯†åˆ«ç»“æœ:")
    for ent in doc.ents:
        print(f"{ent.text}\t{ent.label_}")

text = "Apple Inc. was founded by Steve Jobs in California. He was a brilliant entrepreneur."
print("spaCyé¢„å¤„ç†ç»“æœ:", preprocess_with_spacy(text))
print("\nspaCyè¯¦ç»†åˆ†æ:")
analyze_text_with_spacy(text)
```

### 3.3 ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†ç¤ºä¾‹

```python
# éœ€è¦å®‰è£…: pip install jieba
import jieba
import re

def preprocess_chinese_text(text):
    """ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†ç¤ºä¾‹"""
    # æ¸…ç†æ–‡æœ¬
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)  # ä»…ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’Œæ•°å­—
    text = re.sub(r'\s+', ' ', text).strip()  # è§„èŒƒåŒ–ç©ºæ ¼
    
    # åˆ†è¯
    words = jieba.cut(text)
    
    # ç®€å•çš„åœç”¨è¯åˆ—è¡¨
    stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'ä»¬', 'åˆ°'}
    
    # è¿‡æ»¤åœç”¨è¯
    filtered_words = [word for word in words if word not in stopwords and word != ' ']
    
    return ' '.join(filtered_words)

chinese_text = "ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œæˆ‘åœ¨åŒ—äº¬ä¸‰é‡Œå±¯çš„å’–å•¡å…å–å’–å•¡ã€‚"
print("åŸå§‹ä¸­æ–‡:", chinese_text)
print("å¤„ç†å:", preprocess_chinese_text(chinese_text))
```

### 3.4 å¤„ç†å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®

å¤„ç†å¤§å‹æ–‡æœ¬è¯­æ–™åº“éœ€è¦è€ƒè™‘æ•ˆç‡å’Œå†…å­˜ä½¿ç”¨ï¼š

```python
def process_large_file(input_file, output_file, batch_size=1000):
    """æ‰¹é‡å¤„ç†å¤§å‹æ–‡æœ¬æ–‡ä»¶"""
    preprocessor = TextPreprocessor()
    
    with open(input_file, 'r', encoding='utf-8') as fin, \
         open(output_file, 'w', encoding='utf-8') as fout:
        
        batch = []
        for i, line in enumerate(fin):
            batch.append(line.strip())
            
            # å½“è¾¾åˆ°æ‰¹å¤„ç†å¤§å°æ—¶ï¼Œè¿›è¡Œå¤„ç†
            if len(batch) >= batch_size:
                processed_batch = [preprocessor.preprocess(text) for text in batch]
                fout.write('\n'.join(processed_batch) + '\n')
                batch = []  # é‡ç½®æ‰¹å¤„ç†
                print(f"å·²å¤„ç† {i+1} è¡Œ")
        
        # å¤„ç†å‰©ä½™çš„æ–‡æœ¬
        if batch:
            processed_batch = [preprocessor.preprocess(text) for text in batch]
            fout.write('\n'.join(processed_batch))
            print(f"å®Œæˆå¤„ç†ï¼Œå…± {i+1} è¡Œ")

# ä½¿ç”¨ç¤ºä¾‹
# process_large_file('large_corpus.txt', 'processed_corpus.txt', batch_size=5000)
```

## 4. é«˜çº§åº”ç”¨ä¸å˜ä½“

### 4.1 ä½¿ç”¨é¢„è®­ç»ƒè¯åµŒå…¥

```python
# éœ€è¦å®‰è£…: pip install gensim
from gensim.models import KeyedVectors
import numpy as np

def use_pretrained_word2vec():
    """ä½¿ç”¨é¢„è®­ç»ƒçš„Word2Vecæ¨¡å‹"""
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    # æ³¨: éœ€è¦å…ˆä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œå¦‚Googleçš„Word2Vecæˆ–GloVe
    # model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
    
    # ç”±äºæ–‡ä»¶å¤ªå¤§ï¼Œè¿™é‡Œä»…å±•ç¤ºæ¦‚å¿µä»£ç 
    print("åŠ è½½é¢„è®­ç»ƒçš„Word2Vecæ¨¡å‹...")
    
    # å‡è®¾æ¨¡å‹å·²åŠ è½½ï¼Œè·å–è¯å‘é‡
    # vector = model['computer']
    
    # ä¸¤ä¸ªè¯çš„ç›¸ä¼¼åº¦
    # similarity = model.similarity('computer', 'laptop')
    
    # ç›¸ä¼¼è¯æŸ¥æ‰¾
    # similar_words = model.most_similar('computer', topn=5)
    
    # å¹³å‡å¤šä¸ªè¯å‘é‡æ¥è¡¨ç¤ºä¸€ä¸ªå¥å­
    def get_sentence_vector(sentence, model):
        """è·å–å¥å­çš„å‘é‡è¡¨ç¤º"""
        words = sentence.lower().split()
        word_vectors = []
        
        for word in words:
            if word in model.key_to_index:
                word_vectors.append(model[word])
        
        if word_vectors:
            # æ‰€æœ‰è¯å‘é‡çš„å¹³å‡å€¼
            return np.mean(word_vectors, axis=0)
        else:
            # å¦‚æœæ²¡æœ‰ä»»ä½•è¯åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™è¿”å›é›¶å‘é‡
            return np.zeros(model.vector_size)

# use_pretrained_word2vec()
```

### 4.2 æ–‡æœ¬å¢å¼º(Text Augmentation)

æ–‡æœ¬å¢å¼ºé€šè¿‡åˆ›å»ºæ ·æœ¬çš„å˜ä½“æ¥å¢åŠ è®­ç»ƒæ•°æ®é‡ï¼š

```python
import random
import nltk
from nltk.corpus import wordnet
nltk.download('wordnet')

def synonym_replacement(words, n):
    """éšæœºæ›¿æ¢nä¸ªéåœç”¨è¯ä¸ºå…¶åŒä¹‰è¯"""
    new_words = words.copy()
    stopwords_english = set(stopwords.words('english'))
    # ä¸å±äºåœç”¨è¯çš„è¯æ±‡ç´¢å¼•
    non_stop_indices = [i for i, word in enumerate(words) if word.lower() not in stopwords_english]
    
    # éšæœºé€‰æ‹©nä¸ªéåœç”¨è¯è¿›è¡Œæ›¿æ¢(å¦‚æœå¯èƒ½)
    n = min(n, len(non_stop_indices))
    replace_indices = random.sample(non_stop_indices, n)
    
    for idx in replace_indices:
        word = words[idx]
        synonyms = []
        
        # è·å–åŒä¹‰è¯
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonym = lemma.name().replace('_', ' ')
                if synonym != word:
                    synonyms.append(synonym)
        
        if synonyms:
            new_words[idx] = random.choice(synonyms)
    
    return new_words

def random_deletion(words, p=0.1):
    """ä»¥æ¦‚ç‡péšæœºåˆ é™¤è¯è¯­"""
    if len(words) == 1:
        return words
    
    new_words = []
    for word in words:
        # ä»¥1-pçš„æ¦‚ç‡ä¿ç•™è¯è¯­
        if random.random() > p:
            new_words.append(word)
    
    # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€ä¸ªè¯
    if not new_words:
        new_words.append(random.choice(words))
    
    return new_words

def random_swap(words, n=1):
    """éšæœºäº¤æ¢nå¯¹è¯è¯­ä½ç½®"""
    new_words = words.copy()
    length = len(new_words)
    
    for _ in range(n):
        if length > 1:  # è‡³å°‘éœ€è¦ä¸¤ä¸ªè¯æ‰èƒ½äº¤æ¢
            idx1, idx2 = random.sample(range(length), 2)
            new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]
    
    return new_words

def text_augmentation_demo():
    """æ–‡æœ¬å¢å¼ºç¤ºä¾‹"""
    text = "The quick brown fox jumps over the lazy dog"
    words = text.split()
    
    print("åŸå§‹æ–‡æœ¬:", text)
    
    # åŒä¹‰è¯æ›¿æ¢
    augmented1 = ' '.join(synonym_replacement(words, 2))
    print("åŒä¹‰è¯æ›¿æ¢:", augmented1)
    
    # éšæœºåˆ é™¤
    augmented2 = ' '.join(random_deletion(words))
    print("éšæœºåˆ é™¤:", augmented2)
    
    # éšæœºäº¤æ¢
    augmented3 = ' '.join(random_swap(words, 2))
    print("éšæœºäº¤æ¢:", augmented3)

text_augmentation_demo()
```

### 4.3 å¤„ç†ç‰¹æ®Šé¢†åŸŸæ–‡æœ¬

```python
def preprocess_medical_text(text):
    """åŒ»ç–—æ–‡æœ¬é¢„å¤„ç†ç¤ºä¾‹"""
    # åŒ»å­¦æœ¯è¯­å’Œç¼©å†™å­—å…¸(ç¤ºä¾‹)
    medical_terms = {
        "MI": "myocardial infarction",
        "HTN": "hypertension",
        "DM": "diabetes mellitus",
        "COPD": "chronic obstructive pulmonary disease"
    }
    
    # è½¬æ¢ä¸ºå°å†™ä½†ä¿ç•™ç¼©å†™
    for term in medical_terms.keys():
        text = re.sub(rf'\b{term}\b', f"__{term}__", text)
    
    text = text.lower()
    
    # æ¢å¤ç¼©å†™å¹¶å±•å¼€
    for term, expansion in medical_terms.items():
        text = text.replace(f"__{term.lower()}__", expansion)
    
    # æ ‡å‡†æ¸…æ´—æ­¥éª¤
    text = re.sub(r'[^\w\s]', ' ', text)  # ç§»é™¤æ ‡ç‚¹
    text = re.sub(r'\s+', ' ', text).strip()  # è§„èŒƒåŒ–ç©ºæ ¼
    
    return text

def preprocess_social_media_text(text):
    """ç¤¾äº¤åª’ä½“æ–‡æœ¬é¢„å¤„ç†ç¤ºä¾‹"""
    # å¤„ç†è¡¨æƒ…ç¬¦å·(ç®€åŒ–ç¤ºä¾‹)
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # è¡¨æƒ…
                               u"\U0001F300-\U0001F5FF"  # ç¬¦å·å’Œè±¡å½¢æ–‡å­—
                               u"\U0001F680-\U0001F6FF"  # äº¤é€šå’Œåœ°å›¾
                               u"\U0001F700-\U0001F77F"  # å­—æ¯ç¬¦å·
                               u"\U0001F780-\U0001F7FF"  # å‡ ä½•ç¬¦å·
                               u"\U0001F800-\U0001F8FF"  # ç®­å¤´
                               u"\U0001F900-\U0001F9FF"  # è¡¥å……ç¬¦å·
                               u"\U0001FA00-\U0001FA6F"  # è±¡å½¢æ–‡å­—æ‰©å±•
                               u"\U0001FA70-\U0001FAFF"  # ç¬¦å·å’Œè±¡å½¢æ–‡å­—æ‰©å±•
                               u"\U00002702-\U000027B0"  # è£…é¥°ç¬¦å·
                               u"\U000024C2-\U0001F251" 
                               "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'EMOJI', text)
    
    # å¤„ç†ä¸»é¢˜æ ‡ç­¾
    text = re.sub(r'#(\w+)', r'\1', text)
    
    # å¤„ç†@æåŠ
    text = re.sub(r'@(\w+)', r'USER', text)
    
    # å±•å¼€å¸¸è§ç½‘ç»œç”¨è¯­(ç¤ºä¾‹)
    slang_dict = {
        "lol": "laugh out loud",
        "brb": "be right back",
        "idk": "i do not know",
        "tbh": "to be honest"
    }
    
    words = text.lower().split()
    for i, word in enumerate(words):
        if word in slang_dict:
            words[i] = slang_dict[word]
    
    text = ' '.join(words)
    
    # å¸¸è§„æ¸…æ´
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# ç¤ºä¾‹
medical_text = "Pt with HTN presented with MI and DM. COPD exacerbation."
print("åŒ»ç–—æ–‡æœ¬åŸæ–‡:", medical_text)
print("å¤„ç†å:", preprocess_medical_text(medical_text))

social_text = "OMG! This is sooo funny ğŸ˜‚ #awesome #nlp @user lol"
print("\nç¤¾äº¤åª’ä½“åŸæ–‡:", social_text)
print("å¤„ç†å:", preprocess_social_media_text(social_text))
```

### 4.4 ä½¿ç”¨Transformersè¿›è¡Œç°ä»£æ–‡æœ¬å¤„ç†

```python
# éœ€è¦å®‰è£…: pip install transformers
from transformers import AutoTokenizer

def transformer_tokenization():
    """ä½¿ç”¨Transformeræ¨¡å‹çš„åˆ†è¯å™¨"""
    # åŠ è½½BERTåˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    
    # ç¤ºä¾‹æ–‡æœ¬
    text = "Transformer models have revolutionized NLP."
    
    # ä½¿ç”¨åˆ†è¯å™¨
    tokens = tokenizer.tokenize(text)
    print("BERTæ ‡è®°åŒ–ç»“æœ:", tokens)
    
    # è½¬æ¢ä¸ºID
    input_ids = tokenizer.encode(text)
    print("Token IDs:", input_ids)
    
    # è§£ç å›æ–‡æœ¬
    decoded = tokenizer.decode(input_ids)
    print("è§£ç å:", decoded)
    
    # å¤„ç†é•¿æ–‡æœ¬
    long_text = "This is a very long text " * 50 + "that exceeds the maximum context length."
    
    # ä½¿ç”¨æˆªæ–­
    encoded = tokenizer(long_text, truncation=True, max_length=20)
    print("\næˆªæ–­åçš„Token IDs:", encoded['input_ids'])
    print("è§£ç å:", tokenizer.decode(encoded['input_ids']))

transformer_tokenization()
```

### 4.5 åˆ›å»ºè‡ªå®šä¹‰é¢„å¤„ç†ç®¡é“

```python
class CustomPreprocessingPipeline:
    """å¯é…ç½®çš„è‡ªå®šä¹‰é¢„å¤„ç†ç®¡é“"""
    
    def __init__(self, steps=None):
        """
        åˆå§‹åŒ–é¢„å¤„ç†ç®¡é“
        steps: é¢„å¤„ç†æ­¥éª¤åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯(åç§°, å‡½æ•°)å…ƒç»„
        """
        self.steps = steps or []
    
    def add_step(self, name, func):
        """æ·»åŠ é¢„å¤„ç†æ­¥éª¤"""
        self.steps.append((name, func))
        return self
    
    def remove_step(self, name):
        """ç§»é™¤é¢„å¤„ç†æ­¥éª¤"""
        self.steps = [step for step in self.steps if step[0] != name]
        return self
    
    def process(self, text):
        """åº”ç”¨å®Œæ•´é¢„å¤„ç†ç®¡é“"""
        result = text
        for name, func in self.steps:
            result = func(result)
        return result
    
    def process_batch(self, texts):
        """æ‰¹é‡å¤„ç†æ–‡æœ¬"""
        return [self.process(text) for text in texts]

# å®šä¹‰ä¸€äº›é¢„å¤„ç†å‡½æ•°
def lowercase(text):
    return text.lower()

def remove_special_chars(text):
    return re.sub(r'[^a-zA-Z0-9\s]', '', text)

def remove_extra_whitespace(text):
    return re.sub(r'\s+', ' ', text).strip()

# ä½¿ç”¨ç¤ºä¾‹
pipeline = CustomPreprocessingPipeline()
pipeline.add_step('lowercase', lowercase)
pipeline.add_step('remove_special', remove_special_chars)
pipeline.add_step('clean_whitespace', remove_extra_whitespace)

text = "Hello, World!  This is an   example."
processed = pipeline.process(text)
print("åŸæ–‡:", text)
print("å¤„ç†å:", processed)

# æ·»åŠ æˆ–ç§»é™¤æ­¥éª¤
pipeline.add_step('custom', lambda x: x.replace('example', 'sample'))
processed = pipeline.process(text)
print("æ·»åŠ æ­¥éª¤å:", processed)

pipeline.remove_step('remove_special')
processed = pipeline.process(text)
print("ç§»é™¤æ­¥éª¤å:", processed)
```

## æ€»ç»“

æ–‡æœ¬é¢„å¤„ç†æ˜¯NLPç®¡é“çš„åŸºç¡€ç¯èŠ‚ï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½æœ‰ç€å†³å®šæ€§å½±å“ã€‚æœ¬æ–‡ä»åŸºç¡€æ¦‚å¿µã€æŠ€æœ¯ç»†èŠ‚ã€å®è·µå®ç°åˆ°é«˜çº§åº”ç”¨å…¨é¢ä»‹ç»äº†æ–‡æœ¬é¢„å¤„ç†æŠ€æœ¯ã€‚

### ä¸»è¦æŠ€æœ¯è¦ç‚¹

1. **æ–‡æœ¬æ¸…æ´—**ï¼šç§»é™¤å™ªå£°ã€æ ‡ç‚¹ã€ç‰¹æ®Šå­—ç¬¦ç­‰
2. **æ–‡æœ¬è§„èŒƒåŒ–**ï¼šè¯å¹²æå–ã€è¯å½¢è¿˜åŸã€å¤§å°å†™è½¬æ¢ç­‰
3. **åˆ†è¯**ï¼šå¥å­çº§ã€å•è¯çº§ã€å­è¯çº§åˆ†è¯æŠ€æœ¯
4. **åœç”¨è¯ç§»é™¤**ï¼šè¿‡æ»¤é«˜é¢‘ä½†ä½ä¿¡æ¯é‡çš„è¯è¯­
5. **å‘é‡åŒ–**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤º(è¯è¢‹æ¨¡å‹ã€TF-IDFã€è¯åµŒå…¥)

### è¿›é˜¶æŠ€æœ¯

1. **é¢†åŸŸé€‚åº”**ï¼šå¤„ç†åŒ»ç–—ã€æ³•å¾‹ã€ç¤¾äº¤åª’ä½“ç­‰ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬
2. **æ–‡æœ¬å¢å¼º**ï¼šé€šè¿‡åŒä¹‰è¯æ›¿æ¢ã€éšæœºåˆ é™¤ç­‰æŠ€æœ¯æ‰©å……è®­ç»ƒæ•°æ®
3. **å¤šè¯­è¨€é¢„å¤„ç†**ï¼šé’ˆå¯¹ä¸åŒè¯­è¨€çš„ç‰¹å®šæŠ€æœ¯å’ŒæŒ‘æˆ˜
4. **ç°ä»£é¢„å¤„ç†æŠ€æœ¯**ï¼šä½¿ç”¨Transformersç­‰å…ˆè¿›æ¨¡å‹çš„åˆ†è¯å’Œè¡¨ç¤º

### æœ€ä½³å®è·µ

1. **åˆ›å»ºçµæ´»çš„é¢„å¤„ç†ç®¡é“**ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„é¢„å¤„ç†æ­¥éª¤
2. **æ€§èƒ½ä¸è´¨é‡å¹³è¡¡**ï¼šåœ¨å¤§è§„æ¨¡å¤„ç†æ—¶è€ƒè™‘æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å¹³è¡¡
3. **æŒç»­è¯„ä¼°**ï¼šé€šè¿‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½è¯„ä¼°é¢„å¤„ç†æ•ˆæœ
4. **å…³æ³¨æ•°æ®ç‰¹æ€§**ï¼šæ ¹æ®æ–‡æœ¬ç±»å‹å’Œæ¥æºè°ƒæ•´é¢„å¤„ç†ç­–ç•¥

æŒæ¡æ–‡æœ¬é¢„å¤„ç†æŠ€æœ¯æ˜¯æˆä¸ºNLPä¸“å®¶çš„å¿…ç»ä¹‹è·¯ï¼Œçµæ´»è¿ç”¨è¿™äº›æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚éšç€é¢†åŸŸçš„å‘å±•ï¼Œé¢„å¤„ç†æŠ€æœ¯ä¹Ÿåœ¨ä¸æ–­æ¼”è¿›ï¼Œä¿æŒå­¦ä¹ æ–°æŠ€æœ¯çš„ä¹ æƒ¯å°†ä½¿æ‚¨åœ¨NLPé¢†åŸŸä¿æŒç«äº‰åŠ›ã€‚

Similar code found with 1 license type