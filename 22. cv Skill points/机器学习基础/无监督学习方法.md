# 无监督学习方法

无监督学习是机器学习的一种类型，其中模型仅使用输入数据进行训练，而没有明确的输出标签。无监督学习算法试图从数据中发现隐藏的结构或模式。

## 目录
1. [聚类算法](#聚类算法)
   - [K-Means聚类](#k-means聚类)
   - [层次聚类](#层次聚类)
   - [DBSCAN](#dbscan)
2. [降维技术](#降维技术)
   - [主成分分析(PCA)](#主成分分析pca)
   - [t-SNE](#t-sne)
3. [关联规则学习](#关联规则学习)
   - [Apriori算法](#apriori算法)
4. [生成模型](#生成模型)
   - [自编码器](#自编码器)
   - [变分自编码器(VAE)](#变分自编码器vae)

## 聚类算法

聚类是将相似的数据点分组到一起的过程。以下是几种常见的聚类算法：

### K-Means聚类

K-Means是最简单和最常用的聚类算法之一。

**基本原理：**
1. 选择K个初始中心点
2. 将每个数据点分配到最近的中心点，形成K个簇
3. 重新计算每个簇的中心点
4. 重复步骤2和3，直到中心点几乎不再变化

**优点：**
- 简单易实现
- 计算效率高

**缺点：**
- 需要预先指定K值
- 对初始中心点的选择敏感
- 只适用于凸形簇

**Python代码示例：**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# 生成示例数据
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

# 创建KMeans模型
kmeans = KMeans(n_clusters=4)
# 训练模型
kmeans.fit(X)
# 获取聚类标签
y_kmeans = kmeans.predict(X)
# 获取聚类中心
centers = kmeans.cluster_centers_

# 可视化结果
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')
plt.title('K-Means 聚类结果')
plt.show()
```

### 层次聚类

层次聚类不需要预先指定簇的数量，它创建一个聚类层次结构。

**基本原理：**
- **自底向上(凝聚)：** 每个数据点开始是一个簇，然后逐步合并最相似的簇
- **自顶向下(分裂)：** 所有数据点开始是一个簇，然后递归地将簇分裂

**优点：**
- 不需要预先指定簇的数量
- 可以产生层次结构（树状图）
- 适用于任意形状的簇

**缺点：**
- 计算复杂度高，通常为O(n³)
- 不适合大数据集

**Python代码示例：**

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs

# 生成示例数据
X, _ = make_blobs(n_samples=100, centers=4, random_state=0)

# 创建层次聚类模型
model = AgglomerativeClustering(n_clusters=4)
# 训练模型
clusters = model.fit_predict(X)

# 创建层次聚类树状图
linked = linkage(X, 'ward')

# 可视化树状图
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis')
plt.title('层次聚类结果')

plt.subplot(1, 2, 2)
dendrogram(linked)
plt.title('层次聚类树状图')
plt.show()
```

### DBSCAN

DBSCAN(基于密度的空间聚类应用与噪声)是一种基于密度的聚类算法，可以识别任意形状的簇并检测异常值。

**基本原理：**
1. 对于每个点，检查其ε半径邻域内的点数
2. 如果数量超过MinPts，则该点是核心点
3. 直接密度可达的点被分配到同一个簇
4. 非核心点且不可达的点被视为噪声点

**优点：**
- 不需要预先指定簇的数量
- 可以识别任意形状的簇
- 能够识别噪声点
- 对异常值不敏感

**缺点：**
- 对参数ε和MinPts敏感
- 对密度变化较大的数据表现不佳

**Python代码示例：**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

# 生成非球形数据
X, _ = make_moons(n_samples=200, noise=0.05, random_state=0)

# 创建DBSCAN模型
dbscan = DBSCAN(eps=0.3, min_samples=5)
# 训练模型
clusters = dbscan.fit_predict(X)

# 可视化结果
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis')
plt.title('DBSCAN 聚类结果 (噪声点标记为-1)')
plt.show()
```

## 降维技术

降维是将高维数据转换为低维表示的过程，同时保留关键信息。

### 主成分分析(PCA)

PCA是最常用的线性降维技术，它通过找到数据方差最大的方向来降低维度。

**基本原理：**
1. 计算数据的协方差矩阵
2. 计算协方差矩阵的特征值和特征向量
3. 选择最大的k个特征值对应的特征向量
4. 将数据投影到这k个特征向量组成的子空间

**优点：**
- 降低数据维度，减少存储空间
- 消除特征间的相关性
- 保留数据的主要变异性

**缺点：**
- 只能捕获线性关系
- 对异常值敏感
- 降维后的特征难以解释

**Python代码示例：**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits

# 加载数字数据集
digits = load_digits()
X = digits.data
y = digits.target

# 创建PCA模型，降至2维
pca = PCA(n_components=2)
# 应用PCA
X_pca = pca.fit_transform(X)

# 可视化结果
plt.figure(figsize=(10, 6))
colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'orange', 'purple', 'brown']
for i in range(10):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], c=colors[i], label=str(i))
plt.legend()
plt.title('PCA降维后的手写数字数据')
plt.xlabel('主成分1')
plt.ylabel('主成分2')
plt.show()

# 查看方差解释率
print(f"前两个主成分解释的方差比例: {pca.explained_variance_ratio_}")
print(f"累计方差解释率: {sum(pca.explained_variance_ratio_):.4f}")
```

### t-SNE

t-SNE (t-distributed Stochastic Neighbor Embedding) 是一种非线性降维技术，特别适合高维数据的可视化。

**基本原理：**
1. 在高维空间中计算点对之间的相似度（通常使用高斯分布）
2. 在低维空间中构建类似的分布（使用t分布）
3. 最小化两个分布之间的KL散度

**优点：**
- 保留数据的局部结构
- 能够揭示非线性关系
- 可视化效果优秀

**缺点：**
- 计算复杂度高
- 结果依赖于参数和随机初始化
- 不保留全局结构

**Python代码示例：**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits

# 加载数字数据集
digits = load_digits()
X = digits.data
y = digits.target

# 创建t-SNE模型
tsne = TSNE(n_components=2, random_state=0)
# 应用t-SNE
X_tsne = tsne.fit_transform(X)

# 可视化结果
plt.figure(figsize=(10, 6))
colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'orange', 'purple', 'brown']
for i in range(10):
    plt.scatter(X_tsne[y == i, 0], X_tsne[y == i, 1], c=colors[i], label=str(i))
plt.legend()
plt.title('t-SNE降维后的手写数字数据')
plt.show()
```

## 关联规则学习

关联规则学习发现数据集中项目之间的关系，最常见的应用是市场篮子分析。

### Apriori算法

Apriori算法用于发现频繁项集并生成关联规则。

**基本原理：**
1. 找出所有频繁项集（支持度大于阈值的项集）
2. 从频繁项集生成强关联规则（置信度大于阈值的规则）

**重要概念：**
- **支持度(Support)**: 项集在所有交易中出现的比例
- **置信度(Confidence)**: 规则X→Y的准确性，即包含X的交易中也包含Y的比例
- **提升度(Lift)**: 规则X→Y相对于随机的提升程度

**Python代码示例：**

```python
from mlxtend.frequent_patterns import apriori, association_rules
import pandas as pd

# 创建一个简单的购物篮数据
data = {
    '顾客1': [1, 0, 1, 1, 0],
    '顾客2': [1, 1, 0, 1, 1],
    '顾客3': [1, 0, 1, 0, 1],
    '顾客4': [1, 1, 1, 0, 0],
    '顾客5': [1, 1, 0, 1, 1]
}
df = pd.DataFrame(data, index=['苹果', '牛奶', '面包', '饼干', '咖啡']).T

# 应用Apriori算法找出频繁项集
frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)

# 生成关联规则
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)
print("频繁项集：")
print(frequent_itemsets)
print("\n关联规则：")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
```

## 生成模型

生成模型是一类无监督学习技术，通过学习数据的分布来生成新样本。

### 自编码器

自编码器是一种神经网络，学习压缩数据然后重建它。

**基本原理：**
1. 编码器将输入压缩为低维表示（潜在空间）
2. 解码器从潜在空间重建原始输入
3. 训练目标是最小化重建误差

**应用：**
- 降维
- 特征学习
- 去噪
- 异常检测

**Python代码示例 (使用TensorFlow)：**

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

# 加载MNIST数据
(x_train, _), (x_test, _) = mnist.load_data()

# 数据预处理
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((x_train.shape[0], -1))
x_test = x_test.reshape((x_test.shape[0], -1))

# 定义自编码器结构
input_dim = x_train.shape[1]  # 784
encoding_dim = 32  # 压缩至32维

# 编码器
input_img = Input(shape=(input_dim,))
encoded = Dense(128, activation='relu')(input_img)
encoded = Dense(64, activation='relu')(encoded)
encoded = Dense(encoding_dim, activation='relu')(encoded)

# 解码器
decoded = Dense(64, activation='relu')(encoded)
decoded = Dense(128, activation='relu')(decoded)
decoded = Dense(input_dim, activation='sigmoid')(decoded)

# 构建自编码器模型
autoencoder = Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# 训练模型
autoencoder.fit(x_train, x_train,
                epochs=5,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))

# 使用模型重建图像
encoded_imgs = autoencoder.predict(x_test)
decoded_imgs = autoencoder.predict(encoded_imgs)

# 可视化结果
n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    # 原始图像
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    
    # 重建图像
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()
```

### 变分自编码器(VAE)

VAE是自编码器的概率版本，能够生成新的样本。

**基本原理：**
1. 编码器将输入映射到潜在空间的均值和方差
2. 使用均值和方差参数化一个概率分布
3. 从这个分布中采样一个潜在向量
4. 解码器将潜在向量重建为输入

**优点：**
- 可以生成新样本
- 学习有意义的潜在空间

**Python代码示例 (使用TensorFlow)：**

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
from tensorflow.keras.datasets import mnist

# 加载MNIST数据
(x_train, _), (x_test, _) = mnist.load_data()

# 数据预处理
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((x_train.shape[0], -1))
x_test = x_test.reshape((x_test.shape[0], -1))

# 网络参数
input_dim = 784
intermediate_dim = 256
latent_dim = 2

# 定义采样函数
def sampling(args):
    z_mean, z_log_var = args
    batch = K.shape(z_mean)[0]
    dim = K.shape(z_mean)[1]
    epsilon = K.random_normal(shape=(batch, dim))
    return z_mean + K.exp(0.5 * z_log_var) * epsilon

# 编码器
inputs = Input(shape=(input_dim,))
h = Dense(intermediate_dim, activation='relu')(inputs)
z_mean = Dense(latent_dim)(h)
z_log_var = Dense(latent_dim)(h)
z = Lambda(sampling)([z_mean, z_log_var])

# 解码器
decoder_h = Dense(intermediate_dim, activation='relu')
decoder_mean = Dense(input_dim, activation='sigmoid')
h_decoded = decoder_h(z)
x_decoded_mean = decoder_mean(h_decoded)

# VAE模型
vae = Model(inputs, x_decoded_mean)

# VAE损失函数
def vae_loss(x, x_decoded_mean):
    xent_loss = input_dim * tf.keras.losses.binary_crossentropy(x, x_decoded_mean)
    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
    return xent_loss + kl_loss

vae.compile(optimizer='adam', loss=vae_loss)

# 训练模型
vae.fit(x_train, x_train,
        epochs=5,
        batch_size=128,
        validation_data=(x_test, x_test))

# 可视化潜在空间
encoder = Model(inputs, z_mean)
x_test_encoded = encoder.predict(x_test)

plt.figure(figsize=(10, 8))
plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)
plt.colorbar()
plt.title('二维潜在空间')
plt.show()

# 从潜在空间生成新图像
decoder_input = Input(shape=(latent_dim,))
_h_decoded = decoder_h(decoder_input)
_x_decoded_mean = decoder_mean(_h_decoded)
generator = Model(decoder_input, _x_decoded_mean)

# 在潜在空间中采样并生成图像
n = 15
digit_size = 28
figure = np.zeros((digit_size * n, digit_size * n))

# 在[-4, 4]^2范围内创建网格
grid_x = np.linspace(-4, 4, n)
grid_y = np.linspace(-4, 4, n)

for i, yi in enumerate(grid_x):
    for j, xi in enumerate(grid_y):
        z_sample = np.array([[xi, yi]])
        x_decoded = generator.predict(z_sample)
        digit = x_decoded[0].reshape(digit_size, digit_size)
        figure[i * digit_size: (i + 1) * digit_size,
               j * digit_size: (j + 1) * digit_size] = digit

plt.figure(figsize=(10, 10))
plt.imshow(figure)
plt.title('从潜在空间生成的数字')
plt.show()
```

## 总结

无监督学习是发现数据中隐藏结构的强大工具：

1. **聚类算法** 将相似数据点分组，应用于客户细分、图像分割等
2. **降维技术** 减少数据维度，便于可视化和加速其他算法
3. **关联规则学习** 发现项目间关系，广泛用于市场篮子分析
4. **生成模型** 学习数据分布并生成新样本，用于图像生成、异常检测等

无监督学习的关键挑战是评估模型性能，因为没有真实标签可供比较。通常需要使用内部评估指标（如轮廓系数、DB指数）或领域知识来评估结果的质量。