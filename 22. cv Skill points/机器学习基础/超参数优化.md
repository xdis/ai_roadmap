# 超参数优化

## 什么是超参数？

在机器学习中，有两类参数：
1. **模型参数**：通过训练数据学习得到的参数（如神经网络的权重）
2. **超参数**：在训练前手动设置的参数，无法从数据中直接学习

常见的超参数包括：
- 学习率（learning rate）
- 正则化系数（regularization parameter）
- 神经网络中的层数和每层的神经元数量
- 决策树的深度
- 随机森林的树的数量
- 支持向量机的核函数类型

## 为什么需要超参数优化？

超参数的选择对模型性能有显著影响。不同的超参数组合会导致：
- 模型欠拟合或过拟合
- 训练速度快慢
- 模型精度高低

## 常见的超参数优化方法

### 1. 网格搜索 (Grid Search)

网格搜索是最简单直观的方法，它系统地搜索超参数空间中的所有点。

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# 加载数据
iris = load_iris()
X, y = iris.data, iris.target

# 构建管道
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC())
])

# 定义超参数网格
param_grid = {
    'svm__C': [0.1, 1, 10, 100],
    'svm__gamma': [0.001, 0.01, 0.1, 1],
    'svm__kernel': ['rbf', 'linear']
}

# 网格搜索
grid_search = GridSearchCV(
    pipeline, param_grid, cv=5, scoring='accuracy', verbose=1
)

# 训练
grid_search.fit(X, y)

# 输出最佳参数和得分
print("最佳参数:", grid_search.best_params_)
print("最佳得分:", grid_search.best_score_)
```

**优点**：彻底、易于理解
**缺点**：计算成本高，尤其是参数多时

### 2. 随机搜索 (Random Search)

随机搜索从超参数空间随机采样，而不是穷尽所有组合。

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# 定义参数分布
param_dist = {
    'svm__C': uniform(0.1, 100),
    'svm__gamma': uniform(0.001, 1),
    'svm__kernel': ['rbf', 'linear']
}

# 随机搜索
random_search = RandomizedSearchCV(
    pipeline, param_dist, n_iter=20, cv=5, scoring='accuracy', verbose=1, random_state=42
)

# 训练
random_search.fit(X, y)

# 输出最佳参数和得分
print("最佳参数:", random_search.best_params_)
print("最佳得分:", random_search.best_score_)
```

**优点**：比网格搜索更高效，在高维空间更有效
**缺点**：可能错过最优解

### 3. 贝叶斯优化 (Bayesian Optimization)

贝叶斯优化根据之前的评估结果来指导下一次搜索，更智能地探索参数空间。

```python
# 使用scikit-optimize进行贝叶斯优化
from skopt import BayesSearchCV
from skopt.space import Real, Categorical

# 定义搜索空间
search_spaces = {
    'svm__C': Real(0.1, 100, prior='log-uniform'),
    'svm__gamma': Real(0.001, 1, prior='log-uniform'),
    'svm__kernel': Categorical(['rbf', 'linear'])
}

# 贝叶斯搜索
bayes_search = BayesSearchCV(
    pipeline, search_spaces, n_iter=20, cv=5, scoring='accuracy', verbose=1, random_state=42
)

# 训练
bayes_search.fit(X, y)

# 输出最佳参数和得分
print("最佳参数:", bayes_search.best_params_)
print("最佳得分:", bayes_search.best_score_)
```

**优点**：高效利用之前的评估结果，通常比随机搜索更快找到好的参数
**缺点**：实现相对复杂

### 4. 遗传算法 (Genetic Algorithm)

遗传算法模拟生物进化过程来优化超参数。

```python
# 使用DEAP库进行遗传算法优化
import numpy as np
from deap import base, creator, tools, algorithms
from sklearn.model_selection import cross_val_score

# 定义适应度函数（这里是要最大化的交叉验证分数）
def evaluate(individual):
    # 将个体转换为超参数
    C = individual[0]
    gamma = individual[1]
    kernel = ['rbf', 'linear'][individual[2]]
    
    # 创建SVM模型
    svm = SVC(C=C, gamma=gamma, kernel=kernel)
    
    # 计算交叉验证分数
    score = cross_val_score(svm, X, y, cv=5, scoring='accuracy').mean()
    return (score,)

# 设置遗传算法
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
# 定义基因
toolbox.register("attr_C", np.random.uniform, 0.1, 100)
toolbox.register("attr_gamma", np.random.uniform, 0.001, 1)
toolbox.register("attr_kernel", np.random.randint, 0, 2)  # 0=rbf, 1=linear

# 定义个体和种群
toolbox.register("individual", tools.initCycle, creator.Individual,
                 (toolbox.attr_C, toolbox.attr_gamma, toolbox.attr_kernel), n=1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

# 注册遗传操作
toolbox.register("evaluate", evaluate)
toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=[10, 0.1, 0.5], indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)

# 创建初始种群
population = toolbox.population(n=20)

# 运行遗传算法
NGEN = 5  # 生成次数
result = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=NGEN, verbose=True)

# 获取最佳个体
best_ind = tools.selBest(population, 1)[0]
print("最佳个体:", best_ind)
print("最佳参数: C=%f, gamma=%f, kernel=%s" % 
      (best_ind[0], best_ind[1], ['rbf', 'linear'][best_ind[2]]))
print("最佳得分:", best_ind.fitness.values[0])
```

**优点**：可以处理复杂、非凸的参数空间
**缺点**：实现复杂，调整算法本身也需要经验

## 实用技巧

1. **先粗后细**：先用较大步长进行粗略搜索，然后在最佳参数附近用更小步长进行精细搜索

2. **使用对数尺度**：对于某些参数（如学习率），在对数尺度上均匀采样通常更有效

3. **早停策略**：使用早停来避免计算资源浪费

4. **并行计算**：利用多核CPU或多GPU并行评估不同参数组合

5. **优先优化重要参数**：某些超参数（如学习率）比其他参数更重要，应优先调整

## 使用PyTorch实现学习率调整的例子

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# 定义简单神经网络
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        
    def forward(self, x):
        x = x.view(-1, 28*28)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# 加载MNIST数据集
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000)

# 定义不同学习率的训练函数
def train_with_lr(learning_rate, epochs=5):
    model = SimpleNN()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss()
    
    final_accuracy = 0
    for epoch in range(epochs):
        # 训练
        model.train()
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
        # 评估
        model.eval()
        correct = 0
        with torch.no_grad():
            for data, target in test_loader:
                output = model(data)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
        
        accuracy = 100. * correct / len(test_loader.dataset)
        final_accuracy = accuracy
        print(f'学习率: {learning_rate}, Epoch: {epoch}, 准确率: {accuracy:.2f}%')
    
    return final_accuracy

# 尝试不同的学习率
learning_rates = [1e-2, 1e-3, 1e-4]
results = {}

for lr in learning_rates:
    accuracy = train_with_lr(lr)
    results[lr] = accuracy

# 输出最佳学习率
best_lr = max(results, key=results.get)
print(f"\n最佳学习率: {best_lr}, 准确率: {results[best_lr]:.2f}%")
```

## 总结

超参数优化是机器学习流程中的关键步骤，它可以显著提高模型性能。常见的方法有网格搜索、随机搜索、贝叶斯优化和遗传算法等。

选择哪种方法取决于：
- 计算资源的可用性
- 超参数空间的大小
- 模型训练的时间成本
- 所需优化的精确度

在实践中，通常先使用随机搜索快速探索，然后在有希望的区域使用更精细的方法（如贝叶斯优化）进行调优。