# 监督学习算法

监督学习是机器学习的基础，也是应用最广泛的一类算法。本文将以通俗易懂的方式介绍几种常见的监督学习算法，并配以Python代码示例。

## 什么是监督学习？

监督学习是指利用**已标记的训练数据**（包含输入特征和正确的输出标签）来训练模型，使其能够对新的、未见过的数据进行预测。

> 简单来说：就像老师（标记数据）教学生（模型）做题，学生通过不断练习，最终能够独立解决新题目。

## 主要的监督学习算法

### 1. 线性回归

线性回归是最基础的监督学习算法，用于预测连续值（如房价、温度等）。

#### 原理简述
尝试找到一条直线（或高维空间中的超平面），使得所有数据点到这条线的距离之和最小。

#### 数学表达
对于特征 X 和目标值 y，线性回归模型可表示为：
```
y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
```
其中β是模型参数，ε是误差项。

#### 代码示例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# 生成示例数据
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练模型
model = LinearRegression()
model.fit(X_train, y_train)

# 打印模型参数
print(f"截距: {model.intercept_}")
print(f"系数: {model.coef_}")

# 预测
y_pred = model.predict(X_test)

# 可视化
plt.scatter(X_test, y_test, color='blue', label='实际值')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='预测值')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title('线性回归预测')
plt.show()
```

### 2. 逻辑回归

尽管名字中有"回归"，逻辑回归实际上是一种分类算法，常用于二分类问题。

#### 原理简述
逻辑回归通过sigmoid函数将线性模型的输出转换为0-1之间的概率值，用于预测样本属于某个类别的概率。

#### 数学表达
对于特征X，逻辑回归模型计算概率：
```
P(y=1|X) = 1 / (1 + e^(-(β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ)))
```

#### 代码示例

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# 生成二分类数据集
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, 
                           n_informative=2, random_state=1, n_clusters_per_class=1)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 训练逻辑回归模型
model = LogisticRegression()
model.fit(X_train, y_train)

# 预测测试集
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"准确率: {accuracy:.4f}")

# 绘制决策边界
def plot_decision_boundary(X, y, model):
    # 确定边界
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    h = 0.02  # 网格步长
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # 绘制结果
    plt.figure(figsize=(10, 7))
    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')
    plt.xlabel('特征1')
    plt.ylabel('特征2')
    plt.title('逻辑回归决策边界')
    plt.show()

plot_decision_boundary(X_test, y_test, model)
```

### 3. 决策树

决策树是一种直观的算法，通过一系列问题将数据分割成不同的组。

#### 原理简述
决策树通过挑选最佳特征划分数据，构建一个树状结构。从根节点到叶节点的每条路径代表一条分类规则。

#### 代码示例

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 训练决策树模型
dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_model.fit(X_train, y_train)

# 预测
accuracy = dt_model.score(X_test, y_test)
print(f"决策树准确率: {accuracy:.4f}")

# 可视化决策树
plt.figure(figsize=(15, 10))
plot_tree(dt_model, filled=True, feature_names=iris.feature_names, class_names=list(iris.target_names))
plt.title("决策树可视化")
plt.show()
```

### 4. 随机森林

随机森林是多个决策树的集成，通过投票机制提高预测准确率。

#### 原理简述
随机森林通过以下步骤工作：
1. 从原始数据集构建多个随机样本（bootstrap抽样）
2. 为每个样本构建一个决策树
3. 对分类问题，采用多数投票；对回归问题，取平均值

随机森林的优势在于降低了单一决策树的过拟合风险，提高了模型的鲁棒性。

#### 代码示例

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# 加载乳腺癌数据集
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target

# 划分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 训练随机森林模型
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 预测
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"随机森林准确率: {accuracy:.4f}")

# 查看特征重要性
feature_importance = rf_model.feature_importances_
sorted_idx = feature_importance.argsort()[-10:]  # 展示前10个重要特征
plt.figure(figsize=(10, 6))
plt.barh(range(10), feature_importance[sorted_idx])
plt.yticks(range(10), [cancer.feature_names[i] for i in sorted_idx])
plt.xlabel("特征重要性")
plt.title("随机森林特征重要性")
plt.show()
```

### 5. 支持向量机(SVM)

支持向量机是强大的分类算法，特别适合处理高维数据。

#### 原理简述
SVM尝试找到一个超平面，使得不同类别的数据点之间的间隔最大化。对于非线性可分的数据，SVM使用核技巧将数据映射到高维空间。

#### 代码示例

```python
from sklearn.svm import SVC
from sklearn.datasets import make_moons
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV

# 生成非线性数据
X, y = make_moons(n_samples=1000, noise=0.3, random_state=42)

# 划分数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建SVM分类器
svm_pipeline = Pipeline([
    ('scaler', StandardScaler()),  # 标准化
    ('svm', SVC(kernel='rbf', gamma='scale', probability=True))  # 使用RBF核的SVM
])

# 训练模型
svm_pipeline.fit(X_train, y_train)

# 评估模型
accuracy = svm_pipeline.score(X_test, y_test)
print(f"SVM准确率: {accuracy:.4f}")

# 绘制决策边界
def plot_svm_boundary(X, y, model):
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    h = 0.02
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.figure(figsize=(10, 7))
    plt.contourf(xx, yy, Z, alpha=0.3)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
    plt.xlabel('特征1')
    plt.ylabel('特征2')
    plt.title('SVM决策边界')
    plt.show()

plot_svm_boundary(X_test, y_test, svm_pipeline)
```

### 6. K近邻算法 (KNN)

KNN是一种基于实例的学习算法，通过测量新样本与训练样本之间的距离来进行分类或回归。

#### 原理简述
KNN的基本思想是：一个样本的分类由其K个最近邻的多数类别决定。

#### 代码示例

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# 加载葡萄酒数据集
wine = load_wine()
X = wine.data
y = wine.target

# 划分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 标准化数据
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 训练KNN模型
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train_scaled, y_train)

# 预测
y_pred = knn_model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"KNN准确率: {accuracy:.4f}")

# 查看不同K值对准确率的影响
k_range = range(1, 31)
k_scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)
    k_scores.append(knn.score(X_test_scaled, y_test))

plt.figure(figsize=(10, 6))
plt.plot(k_range, k_scores)
plt.xlabel('K值')
plt.ylabel('准确率')
plt.title('不同K值对KNN性能的影响')
plt.grid(True)
plt.show()
```

## 如何选择合适的监督学习算法？

选择合适的算法需要考虑以下几点：

1. **数据规模**：线性模型适合大数据集，决策树和KNN适合小到中等规模数据集
2. **数据维度**：SVM在高维数据上表现良好，KNN在高维空间效果下降
3. **数据类型**：数值型、分类型或混合型
4. **可解释性需求**：决策树提供良好的可解释性，而神经网络则是"黑盒"
5. **准确度vs速度**：有些算法更准确但训练更慢，需根据实际需求平衡

## 模型评估方法

评估监督学习模型性能常用的指标：

- **分类问题**：准确率、精确率、召回率、F1分数、ROC曲线、AUC值
- **回归问题**：平均绝对误差(MAE)、均方误差(MSE)、均方根误差(RMSE)、R²值

## 总结

监督学习是机器学习的基础和主要部分，通过已标记的数据训练模型，用于预测未知数据。不同的算法有不同的适用场景，需要根据具体问题选择合适的算法。

熟练掌握这些算法，并了解它们的优缺点，将帮助你解决大多数机器学习问题。