# 语义相似度计算

语义相似度计算是自然语言处理(NLP)中的一项基础任务，用于衡量两个文本在语义层面上的相似程度。无论是搜索引擎、问答系统、文本聚类还是推荐系统，都需要计算文本之间的语义相似度。

## 1. 语义相似度的概念

语义相似度指的是两段文本在"含义"上的接近程度，而不仅仅是字面上的相似。例如：

- "我喜欢吃苹果" 和 "我爱吃苹果" - 语义非常相似
- "天气很好" 和 "今天阳光明媚" - 语义相似
- "我喜欢狗" 和 "我养了一只猫" - 语义有一定差异
- "机器学习很有趣" 和 "明天要下雨" - 语义完全不同

语义相似度通常用0到1之间的数值表示，1表示完全相同，0表示完全不相关。

## 2. 语义相似度计算方法

### 2.1 基于词袋模型(Bag of Words)的方法

最简单的方法是将文本表示为词频向量，然后计算向量之间的相似度。

```python
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def bow_similarity(text1, text2):
    """使用词袋模型计算两个文本的相似度"""
    
    # 创建词袋模型
    vectorizer = CountVectorizer()
    
    # 拟合并转换文本
    bow = vectorizer.fit_transform([text1, text2])
    
    # 计算余弦相似度
    similarity = cosine_similarity(bow[0:1], bow[1:2])[0][0]
    
    return similarity

# 示例
text1 = "我喜欢吃苹果和香蕉"
text2 = "我非常喜欢吃香蕉和苹果"
text3 = "我养了一只小猫"

print(f"文本1和文本2的相似度: {bow_similarity(text1, text2):.4f}")
print(f"文本1和文本3的相似度: {bow_similarity(text1, text3):.4f}")
```

**优缺点**：
- 优点：简单易实现，计算效率高
- 缺点：忽略了词序和语法结构，无法捕捉深层语义

### 2.2 TF-IDF方法

TF-IDF在词袋模型的基础上，考虑了词频和逆文档频率，能更好地表示词语的重要性。

```python
from sklearn.feature_extraction.text import TfidfVectorizer

def tfidf_similarity(text1, text2):
    """使用TF-IDF计算两个文本的相似度"""
    
    # 创建TF-IDF向量化器
    vectorizer = TfidfVectorizer()
    
    # 拟合并转换文本
    tfidf = vectorizer.fit_transform([text1, text2])
    
    # 计算余弦相似度
    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]
    
    return similarity

# 示例
print(f"TF-IDF - 文本1和文本2的相似度: {tfidf_similarity(text1, text2):.4f}")
print(f"TF-IDF - 文本1和文本3的相似度: {tfidf_similarity(text1, text3):.4f}")
```

**优缺点**：
- 优点：考虑了词语的权重，减少了常见词的影响
- 缺点：仍然忽略词序和上下文信息

### 2.3 词嵌入(Word Embeddings)方法

词嵌入是将词语映射到一个连续的向量空间中，能够更好地捕捉词语的语义信息。

#### 2.3.1 使用预训练的Word2Vec模型

```python
import gensim.downloader as api
import numpy as np

def word2vec_similarity(text1, text2):
    """使用Word2Vec词嵌入计算文本相似度"""
    
    # 加载预训练的Word2Vec模型
    model = api.load("word2vec-google-news-300")
    
    # 简单的文本预处理（实际应用中应更复杂）
    words1 = text1.lower().split()
    words2 = text2.lower().split()
    
    # 移除模型词汇表中不存在的词
    words1 = [word for word in words1 if word in model.key_to_index]
    words2 = [word for word in words2 if word in model.key_to_index]
    
    if not words1 or not words2:
        return 0.0
    
    # 计算每个文本的平均词向量
    vec1 = np.mean([model[word] for word in words1], axis=0)
    vec2 = np.mean([model[word] for word in words2], axis=0)
    
    # 计算余弦相似度
    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    
    return similarity

# 示例（英文文本，因为使用的是英文预训练模型）
en_text1 = "I like to eat apples and bananas"
en_text2 = "I really enjoy eating bananas and apples"
en_text3 = "I have a small cat"

# 注意：实际运行时需要下载模型，可能较慢
# print(f"Word2Vec - 文本1和文本2的相似度: {word2vec_similarity(en_text1, en_text2):.4f}")
# print(f"Word2Vec - 文本1和文本3的相似度: {word2vec_similarity(en_text1, en_text3):.4f}")
```

#### 2.3.2 中文词嵌入示例（使用jieba分词）

```python
import jieba
import numpy as np
from gensim.models import KeyedVectors

def chinese_word2vec_similarity(text1, text2, model_path):
    """使用中文Word2Vec计算文本相似度"""
    
    # 加载预训练的中文词向量
    model = KeyedVectors.load_word2vec_format(model_path, binary=False)
    
    # 使用jieba进行分词
    words1 = list(jieba.cut(text1))
    words2 = list(jieba.cut(text2))
    
    # 移除词汇表中不存在的词
    words1 = [word for word in words1 if word in model]
    words2 = [word for word in words2 if word in model]
    
    if not words1 or not words2:
        return 0.0
    
    # 计算每个文本的平均词向量
    vec1 = np.mean([model[word] for word in words1], axis=0)
    vec2 = np.mean([model[word] for word in words2], axis=0)
    
    # 计算余弦相似度
    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    
    return similarity

# 注意：需要下载中文词向量模型，如腾讯AILab的词向量
# model_path = "path_to_chinese_word2vec_model.txt"
# similarity = chinese_word2vec_similarity(text1, text2, model_path)
```

**优缺点**：
- 优点：能够捕捉词语之间的语义关系
- 缺点：使用平均词向量可能丢失句子结构信息

### 2.4 基于Transformer的语义相似度计算

现代NLP通常使用基于Transformer的预训练模型（如BERT、RoBERTa等）计算语义相似度，这些模型能更好地理解上下文和深层语义。

#### 2.4.1 使用BERT模型计算语义相似度

```python
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F

def bert_similarity(text1, text2, model_name="bert-base-uncased"):
    """使用BERT模型计算两个文本的语义相似度"""
    
    # 加载预训练的BERT模型和分词器
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    
    # 对文本进行编码
    encoded_input = tokenizer([text1, text2], padding=True, truncation=True, return_tensors="pt")
    
    # 获取模型输出
    with torch.no_grad():
        model_output = model(**encoded_input)
    
    # 使用CLS token的输出作为句子表示
    sentence_embeddings = model_output.last_hidden_state[:, 0, :]
    
    # 对句子嵌入进行归一化
    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)
    
    # 计算余弦相似度
    similarity = torch.matmul(
        sentence_embeddings[0].unsqueeze(0), 
        sentence_embeddings[1].unsqueeze(0).T
    ).item()
    
    return similarity

# 示例
# en_similarity = bert_similarity(en_text1, en_text2)
# print(f"BERT - 文本1和文本2的相似度: {en_similarity:.4f}")
```

#### 2.4.2 使用中文BERT计算相似度

```python
def chinese_bert_similarity(text1, text2, model_name="bert-base-chinese"):
    """使用中文BERT模型计算语义相似度"""
    
    # 原理同上，只是使用中文预训练模型
    return bert_similarity(text1, text2, model_name)

# 示例
# cn_similarity = chinese_bert_similarity(text1, text2)
# print(f"中文BERT - 文本1和文本2的相似度: {cn_similarity:.4f}")
```

#### 2.4.3 使用Sentence-BERT简化计算

Sentence-BERT是专门针对句子级别表示进行优化的BERT变体，更适合计算语义相似度。

```python
from sentence_transformers import SentenceTransformer, util

def sbert_similarity(text1, text2, model_name="paraphrase-MiniLM-L6-v2"):
    """使用Sentence-BERT计算语义相似度"""
    
    # 加载Sentence-BERT模型
    model = SentenceTransformer(model_name)
    
    # 计算句子嵌入
    embedding1 = model.encode(text1, convert_to_tensor=True)
    embedding2 = model.encode(text2, convert_to_tensor=True)
    
    # 计算余弦相似度
    similarity = util.pytorch_cos_sim(embedding1, embedding2).item()
    
    return similarity

# 示例
# sbert_sim = sbert_similarity(en_text1, en_text2)
# print(f"Sentence-BERT - 文本1和文本2的相似度: {sbert_sim:.4f}")
```

**优缺点**：
- 优点：能够理解深层次的语义和上下文关系，性能最佳
- 缺点：计算资源需求大，模型体积大

### 2.5 实用场景：多文本相似度计算

在实际应用中，我们经常需要计算一个查询文本与多个候选文本的相似度。

```python
def find_most_similar(query, candidates, method="tfidf"):
    """在候选文本中找出与查询文本最相似的内容"""
    
    similarities = []
    
    if method == "bow":
        for candidate in candidates:
            sim = bow_similarity(query, candidate)
            similarities.append(sim)
    
    elif method == "tfidf":
        # 创建TF-IDF向量化器并拟合所有文本
        vectorizer = TfidfVectorizer()
        all_texts = [query] + candidates
        tfidf_matrix = vectorizer.fit_transform(all_texts)
        
        # 计算查询与每个候选的相似度
        query_vec = tfidf_matrix[0:1]
        for i in range(1, len(all_texts)):
            candidate_vec = tfidf_matrix[i:i+1]
            sim = cosine_similarity(query_vec, candidate_vec)[0][0]
            similarities.append(sim)
    
    # 找出最相似的文本
    best_match_idx = np.argmax(similarities)
    best_match = candidates[best_match_idx]
    best_similarity = similarities[best_match_idx]
    
    return best_match, best_similarity, similarities

# 示例
query = "机器学习算法如何工作"
candidates = [
    "机器学习是人工智能的一个子领域，研究计算机如何从数据中学习",
    "深度学习是机器学习的一种方法，使用多层神经网络来学习数据表示",
    "自然语言处理是计算机科学和人工智能的分支，专注于让计算机理解人类语言",
    "算法是解决问题的一系列明确步骤，机器学习算法通过数据找出模式"
]

best_match, best_similarity, all_similarities = find_most_similar(query, candidates, "tfidf")

print(f"查询: {query}")
print(f"最佳匹配: {best_match}")
print(f"相似度: {best_similarity:.4f}")
print("所有候选的相似度:")
for i, (candidate, sim) in enumerate(zip(candidates, all_similarities)):
    print(f"{i+1}. 相似度 {sim:.4f}: {candidate[:50]}...")
```

## 3. 语义相似度的评估指标

评估语义相似度算法的常用指标包括：

1. **皮尔逊相关系数(Pearson Correlation)**: 衡量预测相似度与人工标注相似度的线性相关性
2. **斯皮尔曼秩相关系数(Spearman Correlation)**: 衡量预测相似度与人工标注相似度的排序一致性
3. **准确率(Accuracy)**: 在二分类设置下，预测是否相似的准确率

```python
from scipy.stats import pearsonr, spearmanr
import numpy as np

def evaluate_similarity_metrics(predicted_similarities, ground_truth_similarities):
    """评估语义相似度指标的性能"""
    
    # 计算皮尔逊相关系数
    pearson_corr, p_value1 = pearsonr(predicted_similarities, ground_truth_similarities)
    
    # 计算斯皮尔曼秩相关系数
    spearman_corr, p_value2 = spearmanr(predicted_similarities, ground_truth_similarities)
    
    # 二分类准确率（假设阈值为0.5）
    binary_predictions = [1 if sim >= 0.5 else 0 for sim in predicted_similarities]
    binary_ground_truth = [1 if sim >= 0.5 else 0 for sim in ground_truth_similarities]
    accuracy = np.mean([1 if p == g else 0 for p, g in zip(binary_predictions, binary_ground_truth)])
    
    return {
        "pearson": pearson_corr,
        "spearman": spearman_corr,
        "accuracy": accuracy
    }

# 示例
# predicted = [0.8, 0.3, 0.6, 0.9, 0.1]
# ground_truth = [0.9, 0.2, 0.7, 0.8, 0.3]
# results = evaluate_similarity_metrics(predicted, ground_truth)
# print(f"评估结果: {results}")
```

## 4. 语义相似度在实际应用中的挑战

1. **领域适应性**: 通用模型在特定领域的表现可能不佳
2. **语言差异**: 跨语言相似度计算的挑战
3. **计算效率**: 大规模文本相似度计算的效率问题
4. **语言理解的局限性**: 理解隐喻、讽刺等高级语言特性的困难

## 5. 优化语义相似度计算的技巧

### 5.1 领域适应

针对特定领域，可以使用领域数据微调通用模型。

```python
# 示例伪代码：微调BERT模型用于医学文本相似度计算
"""
from transformers import BertForSequenceClassification, AdamW

# 加载预训练模型
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# 准备医学领域的文本对数据
# train_dataloader = ...

# 微调
optimizer = AdamW(model.parameters(), lr=2e-5)
for epoch in range(3):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# 保存微调后的模型
model.save_pretrained('medical-bert-similarity')
"""
```

### 5.2 使用索引加速大规模相似度搜索

对于大规模文本集合，可以使用近似最近邻(ANN)搜索技术。

```python
# 使用Faiss库进行高效相似度检索
"""
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# 加载模型
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# 假设有大量文本
texts = ["文本1", "文本2", ..., "文本N"]

# 计算所有文本的嵌入
embeddings = model.encode(texts)
embeddings = np.array(embeddings).astype('float32')

# 构建索引
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)  # L2距离
index.add(embeddings)

# 查询
query_text = "查询文本"
query_embedding = model.encode([query_text])[0].reshape(1, -1).astype('float32')
distances, indices = index.search(query_embedding, k=5)  # 找出最相似的5个文本

# 结果
for i, idx in enumerate(indices[0]):
    print(f"Top {i+1}: {texts[idx]} (距离: {distances[0][i]})")
"""
```

## 6. 总结

语义相似度计算是NLP的基础任务，从简单的词袋模型到复杂的Transformer模型，不同方法有不同的适用场景：

- **词袋模型和TF-IDF**：简单快速，适合初步筛选或资源受限的场景
- **词嵌入方法**：平衡了性能和效率，适合大多数应用场景
- **Transformer模型**：性能最佳，适合对精度要求高的场景

随着预训练语言模型的发展，语义相似度计算的性能不断提高，但在实际应用中，仍需要根据具体需求选择合适的方法，并考虑计算资源、领域特性等因素。对于追求高性能的应用，结合多种方法或采用模型集成可能会获得更好的效果。