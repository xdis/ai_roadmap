# 文本摘要与生成

文本摘要与生成是自然语言处理(NLP)中的重要任务，它们帮助我们处理大量文本信息并创建新的有意义的内容。

## 1. 文本摘要

文本摘要是将长文本压缩成短文本的过程，同时保留原文的关键信息和核心意义。

### 1.1 文本摘要的分类

文本摘要主要分为两大类：

1. **抽取式摘要(Extractive Summarization)**：
   - 直接从原文中提取关键句子或短语
   - 不生成新的句子，只选择原文中的内容
   - 优点：保持原文风格，不会产生语法错误
   - 缺点：摘要句子之间可能缺乏连贯性

2. **生成式摘要(Abstractive Summarization)**：
   - 理解原文后生成新的句子来概括内容
   - 类似人类撰写摘要的方式
   - 优点：可以创建更流畅、连贯的摘要
   - 缺点：可能产生与原文不符的内容或语法错误

### 1.2 抽取式摘要方法

#### 1.2.1 基于TF-IDF的抽取式摘要

```python
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.tokenize import sent_tokenize

# 下载必要的资源（首次运行需要）
nltk.download('punkt')

def extractive_summarization_tfidf(text, num_sentences=3):
    """使用TF-IDF为句子评分并提取重要句子作为摘要"""
    
    # 分割文本为句子
    sentences = sent_tokenize(text)
    
    # 如果句子数少于要求的摘要句子数，直接返回全文
    if len(sentences) <= num_sentences:
        return " ".join(sentences)
    
    # 创建TF-IDF向量化器
    vectorizer = TfidfVectorizer(stop_words='english')
    
    # 计算每个句子的TF-IDF矩阵
    tfidf_matrix = vectorizer.fit_transform(sentences)
    
    # 计算每个句子的得分（TF-IDF值的总和）
    sentence_scores = np.array(tfidf_matrix.sum(axis=1)).flatten()
    
    # 选择得分最高的句子的索引
    top_n_indices = sentence_scores.argsort()[-num_sentences:]
    
    # 根据原始顺序排序选中的句子
    top_n_indices = sorted(top_n_indices)
    
    # 组合选中的句子形成摘要
    summary = " ".join([sentences[i] for i in top_n_indices])
    
    return summary

# 示例文本
article = """
机器学习是人工智能的一个分支，它使用数据和算法来模仿人类学习的方式，逐步提高其准确性。机器学习是人工智能的一个核心部分，使计算机能够通过经验自动改进。深度学习是机器学习的一个子集，它使用人工神经网络，这些神经网络在结构上模仿人类大脑的神经元网络。神经网络具有多个层次，能够处理大量的数据并从中学习复杂的模式。机器学习模型通常需要大量的标记数据来训练，而深度学习模型则需要更多的数据和更强大的计算资源。然而，深度学习模型通常在复杂任务（如图像识别和自然语言处理）上表现得更好。随着技术的发展，机器学习和深度学习正被应用于各个领域，从医疗诊断到自动驾驶汽车。
"""

# 生成摘要
summary = extractive_summarization_tfidf(article, num_sentences=2)
print("摘要:", summary)
```

#### 1.2.2 TextRank算法的抽取式摘要

TextRank是一种基于图的排序算法，类似于Google的PageRank，用于评估句子的重要性。

```python
import numpy as np
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

def textrank_summarization(text, num_sentences=3):
    """使用TextRank算法进行抽取式摘要"""
    
    # 分割文本为句子
    sentences = sent_tokenize(text)
    
    # 如果句子数少于要求的摘要句子数，直接返回全文
    if len(sentences) <= num_sentences:
        return " ".join(sentences)
    
    # 创建句子向量
    vectorizer = CountVectorizer().fit_transform(sentences)
    
    # 计算句子之间的余弦相似度
    similarity_matrix = cosine_similarity(vectorizer)
    
    # 创建相似度图
    graph = nx.from_numpy_array(similarity_matrix)
    
    # 使用PageRank算法计算句子得分
    scores = nx.pagerank(graph)
    
    # 根据得分对句子索引排序
    ranked_sentences = sorted(((scores[i], i) for i in range(len(sentences))), reverse=True)
    
    # 选择得分最高的num_sentences个句子
    selected_indices = [ranked_sentences[i][1] for i in range(min(num_sentences, len(ranked_sentences)))]
    
    # 按原始顺序排序选中的句子
    selected_indices.sort()
    
    # 组合选中的句子形成摘要
    summary = " ".join([sentences[i] for i in selected_indices])
    
    return summary

# 使用TextRank生成摘要
summary_textrank = textrank_summarization(article, num_sentences=2)
print("TextRank摘要:", summary_textrank)
```

### 1.3 生成式摘要方法

现代生成式摘要主要依赖于深度学习，特别是Transformer架构和预训练语言模型。

#### 1.3.1 使用Transformer模型进行生成式摘要

```python
from transformers import pipeline

def abstractive_summarization(text, max_length=150, min_length=50):
    """使用预训练的T5模型进行生成式摘要"""
    
    # 初始化摘要管道
    summarizer = pipeline("summarization", model="t5-small")
    
    # 生成摘要
    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)
    
    return summary[0]['summary_text']

# 英文示例文本
english_article = """
Machine learning is a branch of artificial intelligence that focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. Machine learning is a key component of AI, enabling computers to improve automatically through experience. Deep learning is a subset of machine learning that uses artificial neural networks, which are structured to mimic the neural networks of the human brain. Neural networks have multiple layers that can process vast amounts of data and learn complex patterns. Machine learning models typically require large amounts of labeled data to train, while deep learning models require even more data and computational power. However, deep learning models often perform better on complex tasks like image recognition and natural language processing. As technology advances, machine learning and deep learning are being applied across various fields, from medical diagnostics to self-driving cars.
"""

# 生成摘要
abstractive_summary = abstractive_summarization(english_article)
print("生成式摘要:", abstractive_summary)
```

#### 1.3.2 中文生成式摘要

对于中文摘要，可以使用专门针对中文的预训练模型。

```python
from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer

# 加载预训练的中文T5模型
tokenizer = AutoTokenizer.from_pretrained("Langboat/mengzi-t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("Langboat/mengzi-t5-base")

def chinese_abstractive_summarization(text, max_length=150):
    """使用预训练的中文T5模型进行生成式摘要"""
    
    # 准备输入
    inputs = tokenizer("summarize: " + text, return_tensors="pt", max_length=512, truncation=True)
    
    # 生成摘要
    summary_ids = model.generate(
        inputs.input_ids, 
        max_length=max_length, 
        min_length=30,
        num_beams=4,
        no_repeat_ngram_size=2
    )
    
    # 解码摘要
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    
    return summary

# 中文示例文本
chinese_article = """
机器学习是人工智能的一个分支，它使用数据和算法来模仿人类学习的方式，逐步提高其准确性。机器学习是人工智能的一个核心部分，使计算机能够通过经验自动改进。深度学习是机器学习的一个子集，它使用人工神经网络，这些神经网络在结构上模仿人类大脑的神经元网络。神经网络具有多个层次，能够处理大量的数据并从中学习复杂的模式。机器学习模型通常需要大量的标记数据来训练，而深度学习模型则需要更多的数据和更强大的计算资源。然而，深度学习模型通常在复杂任务（如图像识别和自然语言处理）上表现得更好。随着技术的发展，机器学习和深度学习正被应用于各个领域，从医疗诊断到自动驾驶汽车。
"""

# 注意：实际运行时需要安装相应的模型
# chinese_summary = chinese_abstractive_summarization(chinese_article)
# print("中文生成式摘要:", chinese_summary)
```

### 1.4 文本摘要评估指标

文本摘要的评估通常使用以下指标：

1. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**：
   - ROUGE-N: 评估n元语法的重叠程度
   - ROUGE-L: 评估最长公共子序列
   - ROUGE-S: 评估跳跃二元语法的重叠

2. **BLEU (Bilingual Evaluation Understudy)**：
   - 主要用于机器翻译，但也适用于摘要评估
   - 评估生成文本与参考文本之间的精确度

```python
from rouge import Rouge

def evaluate_summary(reference, candidate):
    """使用ROUGE评估摘要质量"""
    
    rouge = Rouge()
    scores = rouge.get_scores(candidate, reference)
    
    return scores[0]

# 参考摘要
reference_summary = "机器学习是人工智能的一个分支，使用数据和算法模仿人类学习。深度学习是一个子集，使用模仿人脑的神经网络。"

# 评估之前生成的摘要
# evaluation = evaluate_summary(reference_summary, summary)
# print("ROUGE评分:", evaluation)
```

## 2. 文本生成

文本生成是创建新的、自然流畅的文本内容的过程。与摘要不同，生成不需要源文本，可以从零开始或基于提示创建内容。

### 2.1 文本生成的分类

文本生成可以分为多种类型：

1. **条件式生成**：基于给定提示或条件生成文本
2. **无条件生成**：从零开始生成文本，没有特定提示
3. **特定领域生成**：生成特定领域的文本，如诗歌、故事、新闻报道等

### 2.2 基于马尔可夫链的文本生成

马尔可夫链是一种简单但有效的传统文本生成方法。

```python
import random
import re

def build_markov_model(text, order=2):
    """构建马尔可夫链模型"""
    
    # 将文本分割成单词
    words = re.findall(r'\w+', text.lower())
    
    # 构建马尔可夫链
    markov_dict = {}
    
    for i in range(len(words) - order):
        # 创建状态（前order个单词）
        state = tuple(words[i:i+order])
        # 找到下一个单词
        next_word = words[i+order]
        
        # 更新模型
        if state in markov_dict:
            markov_dict[state].append(next_word)
        else:
            markov_dict[state] = [next_word]
    
    return markov_dict, words

def generate_text_markov(model, words, start=None, length=100, order=2):
    """使用马尔可夫链模型生成文本"""
    
    # 如果没有提供起始状态，随机选择一个
    if not start:
        idx = random.randint(0, len(words) - order)
        start = tuple(words[idx:idx+order])
    
    # 生成文本
    result = list(start)
    current_state = start
    
    for _ in range(length):
        if current_state in model:
            # 随机选择下一个单词
            next_word = random.choice(model[current_state])
            result.append(next_word)
            
            # 更新当前状态
            current_state = tuple(result[-order:])
        else:
            # 如果当前状态不在模型中，随机选择一个新状态
            idx = random.randint(0, len(words) - order)
            current_state = tuple(words[idx:idx+order])
            result.extend(current_state)
    
    return " ".join(result)

# 示例文本
sample_text = """
自然语言处理（NLP）是人工智能的一个子领域，它关注计算机理解、解释和生成人类语言的能力。NLP结合了计算机科学、人工智能和语言学的元素。它使机器能够阅读文本，听取语音，解释含义，测量情感，并确定哪些部分是重要的。一些常见的NLP任务包括文本分类、情感分析、自动摘要、机器翻译、命名实体识别、问答系统和语音识别。随着深度学习的进步，尤其是Transformer架构的发展，NLP技术在近年来取得了显著的进步。
"""

# 构建马尔可夫链模型
markov_model, word_list = build_markov_model(sample_text, order=2)

# 生成文本
generated_text = generate_text_markov(markov_model, word_list, length=50, order=2)
print("马尔可夫链生成的文本:", generated_text)
```

### 2.3 使用RNN/LSTM进行文本生成

循环神经网络(RNN)和长短期记忆网络(LSTM)是早期深度学习文本生成的主要方法。

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

def prepare_text_for_lstm(text, seq_length=40):
    """准备文本数据用于LSTM训练"""
    
    # 创建字符到索引的映射
    chars = sorted(list(set(text)))
    char_to_idx = {c: i for i, c in enumerate(chars)}
    idx_to_char = {i: c for i, c in enumerate(chars)}
    
    # 准备训练数据
    X = []
    y = []
    
    for i in range(0, len(text) - seq_length):
        # 输入序列
        sequence = text[i:i+seq_length]
        # 目标字符（下一个字符）
        target = text[i+seq_length]
        
        X.append([char_to_idx[c] for c in sequence])
        y.append(char_to_idx[target])
    
    # 对输入进行one-hot编码
    X = np.reshape(X, (len(X), seq_length, 1))
    X = X / float(len(chars))
    # 对目标进行one-hot编码
    y = tf.keras.utils.to_categorical(y)
    
    return X, y, chars, char_to_idx, idx_to_char

def build_lstm_model(seq_length, vocab_size):
    """构建LSTM模型用于文本生成"""
    
    model = Sequential()
    model.add(LSTM(256, input_shape=(seq_length, 1), return_sequences=True))
    model.add(Dropout(0.2))
    model.add(LSTM(256))
    model.add(Dropout(0.2))
    model.add(Dense(vocab_size, activation='softmax'))
    
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    
    return model

def generate_text_lstm(model, start_text, char_to_idx, idx_to_char, seq_length, temperature=1.0, length=300):
    """使用训练好的LSTM模型生成文本"""
    
    generated = start_text
    
    # 生成指定长度的文本
    for i in range(length):
        # 准备输入序列
        x = np.zeros((1, seq_length, 1))
        for t, char in enumerate(start_text):
            x[0, t, 0] = char_to_idx[char] / float(len(idx_to_char))
        
        # 预测下一个字符
        preds = model.predict(x, verbose=0)[0]
        
        # 应用温度参数调整概率分布
        preds = np.asarray(preds).astype('float64')
        preds = np.log(preds) / temperature
        exp_preds = np.exp(preds)
        preds = exp_preds / np.sum(exp_preds)
        
        # 根据概率采样下一个字符
        next_index = np.random.choice(len(preds), p=preds)
        next_char = idx_to_char[next_index]
        
        # 添加生成的字符并更新序列
        generated += next_char
        start_text = start_text[1:] + next_char
    
    return generated

# 注意：实际训练LSTM模型需要大量数据和计算资源
# 这里仅展示概念性代码
# X, y, chars, char_to_idx, idx_to_char = prepare_text_for_lstm(sample_text)
# model = build_lstm_model(40, len(chars))
# model.fit(X, y, batch_size=128, epochs=50)
# generated_lstm_text = generate_text_lstm(model, "自然语言处理", char_to_idx, idx_to_char, 40)
# print("LSTM生成的文本:", generated_lstm_text)
```

### 2.4 使用Transformer模型进行文本生成

现代文本生成主要使用基于Transformer架构的预训练语言模型，如GPT系列。

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def generate_text_gpt(prompt, max_length=200):
    """使用GPT-2模型生成文本"""
    
    # 加载预训练的GPT-2模型和分词器
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    
    # 对提示进行编码
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    # 生成文本
    outputs = model.generate(
        inputs,
        max_length=max_length,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        temperature=0.8,
        top_p=0.9,
        do_sample=True
    )
    
    # 解码生成的文本
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_text

# 使用GPT-2生成文本
# prompt = "Natural language processing is"
# generated_gpt_text = generate_text_gpt(prompt)
# print("GPT-2生成的文本:", generated_gpt_text)
```

### 2.5 中文文本生成

对于中文文本生成，可以使用专门的中文预训练模型。

```python
from transformers import BertTokenizer, GPT2LMHeadModel

def generate_chinese_text(prompt, max_length=200):
    """使用中文GPT模型生成文本"""
    
    # 加载预训练的中文GPT模型和分词器
    tokenizer = BertTokenizer.from_pretrained("ckiplab/gpt2-base-chinese")
    model = GPT2LMHeadModel.from_pretrained("ckiplab/gpt2-base-chinese")
    
    # 对提示进行编码
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    # 生成文本
    outputs = model.generate(
        inputs,
        max_length=max_length,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    
    # 解码生成的文本
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_text

# 使用中文GPT模型生成文本
# chinese_prompt = "自然语言处理是"
# generated_chinese_text = generate_chinese_text(chinese_prompt)
# print("中文GPT生成的文本:", generated_chinese_text)
```

### 2.6 文本生成的控制参数

文本生成过程中，有几个重要参数可以控制生成文本的质量和多样性：

1. **温度(Temperature)**：
   - 控制输出分布的随机性，较低的温度使文本更确定，较高的温度增加多样性
   - 范围通常为0.5-1.0

2. **Top-k采样**：
   - 只从概率最高的k个词中采样下一个词
   - 限制选择范围，防止生成低概率的词

3. **Top-p (nucleus) 采样**：
   - 从累积概率达到p的词集合中采样
   - 比Top-k更灵活，能适应不同概率分布

4. **重复惩罚**：
   - 降低已生成词的概率，避免重复
   - 对长文本生成特别有用

```python
def advanced_text_generation(prompt, model_name="gpt2", max_length=200, temperature=0.7, top_k=50, top_p=0.95, repetition_penalty=1.2):
    """使用高级参数控制的文本生成"""
    
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    outputs = model.generate(
        inputs,
        max_length=max_length,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        do_sample=True,
        num_return_sequences=1
    )
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_text

# 使用不同参数生成文本
# prompt = "Artificial intelligence is transforming"
# 
# # 低温度，更确定性的输出
# low_temp_text = advanced_text_generation(prompt, temperature=0.3)
# print("低温度生成:", low_temp_text)
# 
# # 高温度，更多样化的输出
# high_temp_text = advanced_text_generation(prompt, temperature=1.0)
# print("高温度生成:", high_temp_text)
# 
# # 使用不同的top-k和top-p值
# diverse_text = advanced_text_generation(prompt, top_k=10, top_p=0.85)
# print("不同采样策略生成:", diverse_text)
```

## 3. 实际应用案例

### 3.1 新闻摘要系统

```python
def news_summarization_system(news_article, summary_ratio=0.3):
    """新闻摘要系统"""
    
    # 文本预处理
    sentences = sent_tokenize(news_article)
    
    # 确定摘要长度
    num_sentences = max(1, int(len(sentences) * summary_ratio))
    
    # 使用TextRank算法进行摘要
    summary = textrank_summarization(news_article, num_sentences=num_sentences)
    
    return summary

# 示例新闻文章
news = """
中国科学家近日在量子计算研究领域取得重大突破。研究团队开发了一种新型量子处理器，实现了量子霸权的里程碑。这项突破使得某些特定计算任务的处理速度比最先进的超级计算机快数百万倍。量子计算利用量子力学原理，如叠加和纠缠，来处理信息。与传统计算机使用的二进制位不同，量子计算机使用量子比特，可以同时表示多个状态。这使得量子计算机在解决某些复杂问题时具有指数级优势。该研究团队表示，这一突破为未来开发实用化量子计算机铺平了道路，有望在密码学、药物开发和人工智能等领域带来革命性进展。虽然全功能量子计算机的商业化应用还需要克服许多技术障碍，但这一研究成果标志着量子计算向实用化迈出了重要一步。
"""

# 生成新闻摘要
news_summary = news_summarization_system(news, summary_ratio=0.3)
print("新闻摘要:", news_summary)
```

### 3.2 故事生成器

```python
def story_generator(theme, character, setting, model_name="gpt2-medium", length=300):
    """基于给定元素的故事生成器"""
    
    # 构建提示
    prompt = f"Write a short story about {character} in {setting} with the theme of {theme}. "
    
    # 使用预训练模型生成故事
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    outputs = model.generate(
        inputs,
        max_length=length,
        temperature=0.8,
        top_p=0.9,
        do_sample=True
    )
    
    story = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # 移除原始提示
    story = story.replace(prompt, "")
    
    return story

# 生成故事
# theme = "friendship"
# character = "a young programmer"
# setting = "a futuristic city"
# generated_story = story_generator(theme, character, setting)
# print("生成的故事:", generated_story)
```

## 4. 挑战与未来发展

### 4.1 当前挑战

1. **幻觉问题(Hallucination)**：生成不准确或虚构的信息
2. **评估困难**：自动评估生成文本质量的挑战
3. **控制性**：难以精确控制生成内容的风格和主题
4. **长文本生成**：维持长文本的连贯性和一致性
5. **计算需求**：大型语言模型需要大量计算资源

### 4.2 未来发展

1. **融合检索的生成**：结合知识库提高准确性
2. **多模态生成**：结合文本、图像、音频等多种模态
3. **可控文本生成**：更精确地控制生成内容的属性
4. **个性化生成**：适应特定用户偏好和风格的生成
5. **高效模型**：开发更高效的模型架构降低资源需求

## 5. 总结

文本摘要和生成是NLP的两个关键任务，它们在处理信息过载和创建内容方面发挥着重要作用。随着深度学习和预训练语言模型的发展，这些技术已经取得了显著进步，并在多个领域得到了应用。

- **文本摘要**技术帮助我们从大量文本中提取关键信息，分为抽取式和生成式两大类。
- **文本生成**技术能够创建新的、自然流畅的文本内容，从简单的马尔可夫链到复杂的Transformer模型。

未来，随着技术的进一步发展，我们有望看到更加智能、可控和高效的文本摘要与生成系统，为信息处理和内容创作提供更强大的支持。