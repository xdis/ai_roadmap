# LLMåº”ç”¨å¼€å‘è¯¦è§£

å¤§è¯­è¨€æ¨¡å‹(LLM)åº”ç”¨å¼€å‘æ˜¯æŒ‡åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºå®ç”¨çš„AIåº”ç”¨ç¨‹åºã€‚è¿™ä¸ªé¢†åŸŸç»“åˆäº†AIæŠ€æœ¯ä¸è½¯ä»¶å·¥ç¨‹ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿåˆ›å»ºæ™ºèƒ½ã€äº¤äº’å¼çš„åº”ç”¨ã€‚ä¸‹é¢æˆ‘å°†è¯¦ç»†è§£é‡ŠLLMåº”ç”¨å¼€å‘çš„æ ¸å¿ƒæ¦‚å¿µã€å¸¸ç”¨æŠ€æœ¯å’Œå®è·µæ–¹æ³•ã€‚

## 1. LLMåº”ç”¨å¼€å‘åŸºç¡€

### 1.1 LLMåº”ç”¨çš„åŸºæœ¬å·¥ä½œæµç¨‹

LLMåº”ç”¨çš„åŸºæœ¬å·¥ä½œæµç¨‹åŒ…æ‹¬ï¼š
1. æ¥æ”¶ç”¨æˆ·è¾“å…¥
2. é¢„å¤„ç†è¾“å…¥æ•°æ®
3. æ„å»ºé€‚å½“çš„æç¤º(Prompt)
4. è°ƒç”¨LLMè·å–å›å¤
5. å¤„ç†LLMè¾“å‡º
6. å°†å¤„ç†åçš„ä¿¡æ¯è¿”å›ç»™ç”¨æˆ·

```python
import openai

def basic_llm_app(user_input):
    """ç®€å•çš„LLMåº”ç”¨ç¤ºä¾‹"""
    # 1. æ¥æ”¶ç”¨æˆ·è¾“å…¥ - å·²é€šè¿‡å‚æ•°ä¼ å…¥
    
    # 2. é¢„å¤„ç†è¾“å…¥(ç®€åŒ–ç‰ˆ)
    processed_input = user_input.strip()
    
    # 3. æ„å»ºæç¤º
    prompt = f"ç”¨æˆ·: {processed_input}\nåŠ©æ‰‹:"
    
    # 4. è°ƒç”¨LLM
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": processed_input}
        ],
        max_tokens=500,
        temperature=0.7
    )
    
    # 5. å¤„ç†LLMè¾“å‡º
    assistant_response = response.choices[0].message.content
    
    # 6. è¿”å›ç»“æœç»™ç”¨æˆ·
    return assistant_response

# ä½¿ç”¨ç¤ºä¾‹
result = basic_llm_app("å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹?")
print(result)
```

### 1.2 å¸¸ç”¨çš„LLMæ¥å£

å¼€å‘LLMåº”ç”¨æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å¤šç§æ¥å£ï¼š

```python
# OpenAI APIè°ƒç”¨ç¤ºä¾‹
def call_openai_api(prompt):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )
    return response.choices[0].message.content

# Hugging Faceæ¨¡å‹æœ¬åœ°éƒ¨ç½²è°ƒç”¨ç¤ºä¾‹
def call_local_huggingface_model(prompt):
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model_name = "meta-llama/Llama-2-7b-chat-hf"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
    
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_length=200)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return response

# Anthropic Claude APIè°ƒç”¨ç¤ºä¾‹
def call_anthropic_api(prompt):
    import anthropic
    
    client = anthropic.Client(api_key="your_api_key")
    response = client.messages.create(
        model="claude-2",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=1000
    )
    
    return response.content[0].text
```

## 2. LLMåº”ç”¨æ¶æ„è®¾è®¡

### 2.1 åŸºæœ¬çš„Webåº”ç”¨æ¶æ„

ä½¿ç”¨FastAPIæ„å»ºLLMåº”ç”¨çš„åç«¯ï¼š

```python
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import openai
import uvicorn

# å®šä¹‰è¯·æ±‚å’Œå“åº”æ¨¡å‹
class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    response: str

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI(title="LLMèŠå¤©åº”ç”¨")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶æ¥æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®OpenAI API
openai.api_key = "your_api_key"

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    try:
        # è°ƒç”¨LLM
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": request.message}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        # æå–å›å¤
        assistant_response = response.choices[0].message.content
        
        return ChatResponse(response=assistant_response)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2.2 å‰ç«¯ç•Œé¢é›†æˆ

ä½¿ç”¨HTMLã€JavaScriptå’ŒVue.jsæ„å»ºç®€å•çš„å‰ç«¯ï¼š

```html
<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLMèŠå¤©åº”ç”¨</title>
    <script src="https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/axios/dist/axios.min.js"></script>
    <style>
        .chat-container {
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .message {
            margin-bottom: 15px;
            padding: 10px;
            border-radius: 5px;
        }
        .user-message {
            background-color: #e3f2fd;
            text-align: right;
        }
        .assistant-message {
            background-color: #f5f5f5;
        }
        .input-area {
            display: flex;
            margin-top: 20px;
        }
        input {
            flex-grow: 1;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        button {
            margin-left: 10px;
            padding: 10px 20px;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        button:disabled {
            background-color: #cccccc;
        }
    </style>
</head>
<body>
    <div id="app" class="chat-container">
        <h1>LLMèŠå¤©åº”ç”¨</h1>
        
        <div class="chat-history">
            <div v-for="(message, index) in messages" :key="index" 
                 :class="['message', message.role === 'user' ? 'user-message' : 'assistant-message']">
                <strong>{{ message.role === 'user' ? 'ä½ ' : 'AIåŠ©æ‰‹' }}:</strong> {{ message.content }}
            </div>
        </div>
        
        <div class="input-area">
            <input v-model="userInput" @keyup.enter="sendMessage" placeholder="è¾“å…¥æ¶ˆæ¯..." :disabled="loading">
            <button @click="sendMessage" :disabled="loading || !userInput.trim()">
                {{ loading ? 'å¤„ç†ä¸­...' : 'å‘é€' }}
            </button>
        </div>
    </div>

    <script>
        new Vue({
            el: '#app',
            data: {
                userInput: '',
                messages: [],
                loading: false
            },
            methods: {
                sendMessage() {
                    if (this.loading || !this.userInput.trim()) return;
                    
                    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                    const userMessage = this.userInput.trim();
                    this.messages.push({
                        role: 'user',
                        content: userMessage
                    });
                    this.userInput = '';
                    
                    // æ˜¾ç¤ºåŠ è½½çŠ¶æ€
                    this.loading = true;
                    
                    // è°ƒç”¨åç«¯API
                    axios.post('http://localhost:8000/chat', {
                        message: userMessage
                    })
                    .then(response => {
                        // æ·»åŠ åŠ©æ‰‹å›å¤
                        this.messages.push({
                            role: 'assistant',
                            content: response.data.response
                        });
                    })
                    .catch(error => {
                        console.error('Error:', error);
                        alert('å‘é€æ¶ˆæ¯æ—¶å‡ºé”™');
                    })
                    .finally(() => {
                        this.loading = false;
                    });
                }
            }
        });
    </script>
</body>
</html>
```

## 3. é«˜çº§LLMåº”ç”¨åŠŸèƒ½

### 3.1 å¯¹è¯çŠ¶æ€ç®¡ç†

ç®¡ç†å¯¹è¯å†å²è®°å½•ä»¥å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¹è¯ï¼š

```python
class ConversationManager:
    """ç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡çš„å·¥å…·ç±»"""
    
    def __init__(self, max_history=10, system_prompt="ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"):
        self.conversations = {}  # ä½¿ç”¨å­—å…¸å­˜å‚¨ä¸åŒç”¨æˆ·çš„å¯¹è¯
        self.max_history = max_history
        self.system_prompt = system_prompt
    
    def add_message(self, conversation_id, role, content):
        """æ·»åŠ æ¶ˆæ¯åˆ°æŒ‡å®šå¯¹è¯"""
        # å¦‚æœæ˜¯æ–°å¯¹è¯ï¼Œåˆå§‹åŒ–
        if conversation_id not in self.conversations:
            self.conversations[conversation_id] = []
        
        # æ·»åŠ æ¶ˆæ¯
        self.conversations[conversation_id].append({
            "role": role,
            "content": content
        })
        
        # ä¿æŒå¯¹è¯å†å²åœ¨é™åˆ¶èŒƒå›´å†…
        if len(self.conversations[conversation_id]) > self.max_history:
            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼Œåˆ é™¤æœ€æ—©çš„ç”¨æˆ·-åŠ©æ‰‹å¯¹è¯
            if self.conversations[conversation_id][0]["role"] == "system":
                # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼Œç§»é™¤æœ€æ—©çš„ç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯
                self.conversations[conversation_id] = [
                    self.conversations[conversation_id][0]
                ] + self.conversations[conversation_id][3:]
            else:
                # æ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œç›´æ¥ç§»é™¤æœ€æ—©çš„ä¸¤æ¡
                self.conversations[conversation_id] = self.conversations[conversation_id][2:]
    
    def get_messages(self, conversation_id):
        """è·å–æŒ‡å®šå¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯"""
        if conversation_id not in self.conversations:
            # æ–°å¯¹è¯ï¼Œåˆå§‹åŒ–ç³»ç»Ÿæç¤º
            return [{"role": "system", "content": self.system_prompt}]
        
        return self.conversations[conversation_id]
    
    def clear_conversation(self, conversation_id):
        """æ¸…é™¤æŒ‡å®šå¯¹è¯å†å²"""
        if conversation_id in self.conversations:
            del self.conversations[conversation_id]

# ä½¿ç”¨ç¤ºä¾‹
conversation_manager = ConversationManager()

# ç”¨æˆ·å‘é€ç¬¬ä¸€æ¡æ¶ˆæ¯
conversation_id = "user123"
user_message = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹Pythonã€‚"
conversation_manager.add_message(conversation_id, "user", user_message)

# è·å–å®Œæ•´å¯¹è¯å†å²ï¼ˆåŒ…æ‹¬ç³»ç»Ÿæç¤ºï¼‰
messages = conversation_manager.get_messages(conversation_id)

# è°ƒç”¨OpenAI API
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=messages,
    max_tokens=500
)
assistant_response = response.choices[0].message.content

# ä¿å­˜åŠ©æ‰‹å›å¤
conversation_manager.add_message(conversation_id, "assistant", assistant_response)
```

### 3.2 æµå¼å“åº”å¤„ç†

ä½¿ç”¨æµå¼å“åº”æä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒï¼š

```python
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import openai
import uvicorn
import asyncio
import json

app = FastAPI()

class StreamRequest(BaseModel):
    message: str
    conversation_id: str

@app.post("/chat/stream")
async def stream_chat(request: StreamRequest):
    async def generate():
        try:
            # è·å–å¯¹è¯å†å²(è¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…ä¸­åº”ä½¿ç”¨ConversationManager)
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": request.message}
            ]
            
            # åˆ›å»ºæµå¼è¯·æ±‚
            response = await openai.ChatCompletion.acreate(
                model="gpt-3.5-turbo",
                messages=messages,
                max_tokens=1000,
                temperature=0.7,
                stream=True  # å¯ç”¨æµå¼ä¼ è¾“
            )
            
            # æµå¼è¿”å›ç»“æœ
            async for chunk in response:
                if chunk.choices and len(chunk.choices) > 0:
                    content = chunk.choices[0].delta.get("content", "")
                    if content:
                        # ä½¿ç”¨SSEæ ¼å¼è¿”å›æ•°æ®
                        yield f"data: {json.dumps({'content': content})}\n\n"
            
            # å‘é€å®Œæˆä¿¡å·
            yield f"data: {json.dumps({'content': '', 'done': True})}\n\n"
        
        except Exception as e:
            # å‘é€é”™è¯¯ä¿¡æ¯
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )

# å‰ç«¯JavaScriptç¤ºä¾‹
"""
// ä½¿ç”¨EventSourceå¤„ç†æµå¼å“åº”
const eventSource = new EventSource('/chat/stream?message=è®²è§£Pythonç¼–ç¨‹&conversation_id=123');

let fullResponse = '';

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);
    
    if (data.error) {
        console.error('Error:', data.error);
        eventSource.close();
        return;
    }
    
    // è¿½åŠ å†…å®¹
    fullResponse += data.content;
    
    // æ›´æ–°UIæ˜¾ç¤º
    document.getElementById('response').textContent = fullResponse;
    
    // æ£€æŸ¥æ˜¯å¦å®Œæˆ
    if (data.done) {
        eventSource.close();
    }
};

eventSource.onerror = (error) => {
    console.error('EventSource error:', error);
    eventSource.close();
};
"""
```

### 3.3 å·¥å…·é›†æˆ(Function Calling)

è®©LLMè°ƒç”¨å¤–éƒ¨å·¥å…·æå‡åº”ç”¨èƒ½åŠ›ï¼š

```python
import openai
import json
import requests
from datetime import datetime

# å®šä¹‰å¯ç”¨å·¥å…·(å‡½æ•°)
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°ï¼Œä¾‹å¦‚ï¼šåŒ—äº¬ã€ä¸Šæµ·ã€å¹¿å·"
                    }
                },
                "required": ["city"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "search_products",
            "description": "æœç´¢å•†å“ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯"
                    },
                    "category": {
                        "type": "string",
                        "description": "å•†å“ç±»åˆ«ï¼Œä¾‹å¦‚ï¼šç”µå­äº§å“ã€æœè£…ã€é£Ÿå“",
                        "enum": ["ç”µå­äº§å“", "æœè£…", "é£Ÿå“", "å®¶å±…", "å…¨éƒ¨"]
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "æœ€å¤§è¿”å›ç»“æœæ•°é‡"
                    }
                },
                "required": ["query"]
            }
        }
    }
]

# å®ç°å·¥å…·å‡½æ•°
def get_weather(city):
    """è·å–åŸå¸‚å¤©æ°”ä¿¡æ¯(æ¨¡æ‹Ÿå®ç°)"""
    # å®é™…åº”ç”¨ä¸­åº”è°ƒç”¨çœŸå®çš„å¤©æ°”API
    weather_data = {
        "åŒ—äº¬": {"temperature": "23Â°C", "condition": "æ™´æœ—", "humidity": "45%"},
        "ä¸Šæµ·": {"temperature": "26Â°C", "condition": "å¤šäº‘", "humidity": "60%"},
        "å¹¿å·": {"temperature": "29Â°C", "condition": "é˜µé›¨", "humidity": "75%"}
    }
    
    if city in weather_data:
        return weather_data[city]
    else:
        return {"error": f"æœªæ‰¾åˆ°{city}çš„å¤©æ°”ä¿¡æ¯"}

def search_products(query, category="å…¨éƒ¨", max_results=5):
    """æœç´¢å•†å“ä¿¡æ¯(æ¨¡æ‹Ÿå®ç°)"""
    # å®é™…åº”ç”¨ä¸­åº”æŸ¥è¯¢æ•°æ®åº“æˆ–è°ƒç”¨ç”µå•†API
    products = [
        {"name": "iPhone 15", "category": "ç”µå­äº§å“", "price": 5999},
        {"name": "MacBook Pro", "category": "ç”µå­äº§å“", "price": 12999},
        {"name": "Nikeè·‘é‹", "category": "æœè£…", "price": 899},
        {"name": "ç¾½ç»’æœ", "category": "æœè£…", "price": 1299},
        {"name": "å·§å…‹åŠ›", "category": "é£Ÿå“", "price": 58},
        {"name": "èŒ¶å¶", "category": "é£Ÿå“", "price": 128},
        {"name": "æ²™å‘", "category": "å®¶å±…", "price": 3499}
    ]
    
    # è¿‡æ»¤å•†å“
    if category != "å…¨éƒ¨":
        filtered_products = [p for p in products if p["category"] == category]
    else:
        filtered_products = products
    
    # å…³é”®è¯æœç´¢
    results = [p for p in filtered_products if query.lower() in p["name"].lower()]
    
    # é™åˆ¶ç»“æœæ•°é‡
    return results[:max_results]

# å¤„ç†å·¥å…·è°ƒç”¨
def handle_tool_calls(tool_calls):
    """å¤„ç†å·¥å…·è°ƒç”¨è¯·æ±‚å¹¶è¿”å›ç»“æœ"""
    results = []
    
    for tool_call in tool_calls:
        function_name = tool_call.function.name
        function_args = json.loads(tool_call.function.arguments)
        
        if function_name == "get_weather":
            result = get_weather(function_args.get("city"))
        elif function_name == "search_products":
            result = search_products(
                function_args.get("query"),
                function_args.get("category", "å…¨éƒ¨"),
                function_args.get("max_results", 5)
            )
        else:
            result = {"error": f"æœªçŸ¥å‡½æ•°: {function_name}"}
        
        results.append({
            "tool_call_id": tool_call.id,
            "function": {"name": function_name, "arguments": tool_call.function.arguments},
            "result": result
        })
    
    return results

# ç¤ºä¾‹ï¼šå¤„ç†ç”¨æˆ·æŸ¥è¯¢
def process_user_query(query):
    """å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œéœ€è¦æ—¶è°ƒç”¨å·¥å…·"""
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”é—®é¢˜å¹¶æä¾›å¸®åŠ©ã€‚éœ€è¦æ—¶ï¼Œä½ å¯ä»¥è°ƒç”¨å·¥å…·æ¥è·å–ä¿¡æ¯ã€‚"},
        {"role": "user", "content": query}
    ]
    
    # ç¬¬ä¸€è½®ï¼šè®©æ¨¡å‹å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages,
        tools=tools,
        tool_choice="auto"
    )
    
    response_message = response.choices[0].message
    messages.append(response_message)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
    if hasattr(response_message, 'tool_calls') and response_message.tool_calls:
        # å¤„ç†å·¥å…·è°ƒç”¨
        tool_results = handle_tool_calls(response_message.tool_calls)
        
        # å°†ç»“æœæ·»åŠ åˆ°å¯¹è¯ä¸­
        for result in tool_results:
            messages.append({
                "role": "tool",
                "tool_call_id": result["tool_call_id"],
                "content": json.dumps(result["result"])
            })
        
        # ç¬¬äºŒè½®ï¼šè®©æ¨¡å‹åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
        final_response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=messages
        )
        
        return final_response.choices[0].message.content
    else:
        # ä¸éœ€è¦å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å›å¤
        return response_message.content

# ä½¿ç”¨ç¤ºä¾‹
print(process_user_query("åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"))
print(process_user_query("æœ‰å“ªäº›ç”µå­äº§å“æ¨èï¼Ÿ"))
```

## 4. å¢å¼ºLLMåº”ç”¨åŠŸèƒ½

### 4.1 ä½¿ç”¨RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)

RAGé€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£å¢å¼ºLLMå›ç­”çš„å‡†ç¡®æ€§ï¼š

```python
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
import openai
import os

class RAGSystem:
    """åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, docs_dir, embed_model="openai", collection_name="my_documents"):
        self.docs_dir = docs_dir
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings()
        self.vector_store = None
    
    def load_documents(self):
        """åŠ è½½æ–‡æ¡£"""
        loader = DirectoryLoader(self.docs_dir, glob="**/*.txt")
        documents = loader.load()
        print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def process_documents(self, documents):
        """å¤„ç†æ–‡æ¡£å¹¶åˆ›å»ºå‘é‡å­˜å‚¨"""
        # æ–‡æ¡£åˆ†å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(documents)
        print(f"æ–‡æ¡£è¢«åˆ†å‰²æˆ {len(chunks)} ä¸ªå—")
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            collection_name=self.collection_name,
            persist_directory="./chroma_db"
        )
        self.vector_store.persist()
    
    def setup(self):
        """è®¾ç½®RAGç³»ç»Ÿ"""
        documents = self.load_documents()
        self.process_documents(documents)
    
    def query(self, user_question, num_docs=3):
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        if not self.vector_store:
            raise ValueError("Vector store not initialized. Call setup() first.")
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.vector_store.similarity_search(user_question, k=num_docs)
        
        # æå–æ–‡æ¡£å†…å®¹
        context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
        # æ„å»ºæç¤º
        prompt = f"""
        å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ä¿¡æ¯ä¸è¶³ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚

        ä¸Šä¸‹æ–‡ä¿¡æ¯:
        {context}

        é—®é¢˜: {user_question}
        """
        
        # è°ƒç”¨LLMç”Ÿæˆå›ç­”
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†ä¸°å¯Œçš„åŠ©æ‰‹ï¼Œåªä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5
        )
        
        answer = response.choices[0].message.content
        
        return {
            "question": user_question,
            "answer": answer,
            "sources": [{"content": doc.page_content, "source": doc.metadata.get("source", "Unknown")} for doc in retrieved_docs]
        }

# ä½¿ç”¨ç¤ºä¾‹
def rag_example():
    # ç¡®ä¿æ–‡æ¡£ç›®å½•å­˜åœ¨
    docs_dir = "./knowledge_base"
    os.makedirs(docs_dir, exist_ok=True)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£(å®é™…åº”ç”¨ä¸­è¿™äº›æ–‡æ¡£å·²ç»å­˜åœ¨)
    with open(f"{docs_dir}/python_info.txt", "w") as f:
        f.write("Pythonæ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„è§£é‡Šå‹ã€é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚Pythonçš„è®¾è®¡å¼ºè°ƒä»£ç çš„å¯è¯»æ€§å’Œç®€æ´çš„è¯­æ³•ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿç”¨æ›´å°‘çš„ä»£ç è¡¨è¾¾æƒ³æ³•ã€‚Pythonæ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ï¼ŒåŒ…æ‹¬é¢å‘å¯¹è±¡ã€å‘½ä»¤å¼ã€å‡½æ•°å¼å’Œè¿‡ç¨‹å¼ç¼–ç¨‹ã€‚")
    
    with open(f"{docs_dir}/machine_learning.txt", "w") as f:
        f.write("æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºå¼€å‘èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›çš„ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ï¼Œè€Œæ— éœ€æ˜¾å¼ç¼–ç¨‹ã€‚å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œå’Œæ”¯æŒå‘é‡æœºç­‰ã€‚")
    
    # åˆ›å»ºå¹¶è®¾ç½®RAGç³»ç»Ÿ
    rag_system = RAGSystem(docs_dir)
    rag_system.setup()
    
    # æŸ¥è¯¢ç¤ºä¾‹
    result = rag_system.query("ä»€ä¹ˆæ˜¯Pythonç¼–ç¨‹è¯­è¨€ï¼Ÿ")
    print(f"é—®é¢˜: {result['question']}")
    print(f"å›ç­”: {result['answer']}")
    print("å‚è€ƒæ¥æº:")
    for source in result['sources']:
        print(f"- {source['content'][:100]}...")

# è¿è¡Œç¤ºä¾‹
rag_example()
```

### 4.2 ä½¿ç”¨ä»£ç†(Agent)æ¨¡å¼

æ„å»ºèƒ½è‡ªä¸»è§£å†³å¤æ‚ä»»åŠ¡çš„AIä»£ç†ï¼š

```python
import openai
import json
import re
import time
from typing import List, Dict, Any

class Tool:
    """å·¥å…·åŸºç±»"""
    def __init__(self, name: str, description: str, parameters: Dict):
        self.name = name
        self.description = description
        self.parameters = parameters
    
    def execute(self, **kwargs):
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°executeæ–¹æ³•")

class WeatherTool(Tool):
    """å¤©æ°”æŸ¥è¯¢å·¥å…·"""
    def __init__(self):
        super().__init__(
            name="get_weather",
            description="è·å–æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”ä¿¡æ¯",
            parameters={
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°ï¼Œä¾‹å¦‚ï¼šåŒ—äº¬ã€ä¸Šæµ·ã€å¹¿å·"
                    }
                },
                "required": ["city"]
            }
        )
    
    def execute(self, city):
        # æ¨¡æ‹Ÿå®ç°ï¼Œå®é™…åº”è°ƒç”¨å¤©æ°”API
        weather_data = {
            "åŒ—äº¬": {"temperature": "23Â°C", "condition": "æ™´æœ—", "humidity": "45%"},
            "ä¸Šæµ·": {"temperature": "26Â°C", "condition": "å¤šäº‘", "humidity": "60%"},
            "å¹¿å·": {"temperature": "29Â°C", "condition": "é˜µé›¨", "humidity": "75%"}
        }
        
        if city in weather_data:
            return weather_data[city]
        else:
            return {"error": f"æœªæ‰¾åˆ°{city}çš„å¤©æ°”ä¿¡æ¯"}

class CalculatorTool(Tool):
    """è®¡ç®—å™¨å·¥å…·"""
    def __init__(self):
        super().__init__(
            name="calculator",
            description="æ‰§è¡Œæ•°å­¦è®¡ç®—ï¼Œæ”¯æŒåŠ å‡ä¹˜é™¤å’ŒåŸºæœ¬æ•°å­¦å‡½æ•°",
            parameters={
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "æ•°å­¦è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ï¼š2 + 2 * 3"
                    }
                },
                "required": ["expression"]
            }
        )
    
    def execute(self, expression):
        try:
            # å®‰å…¨åœ°è®¡ç®—è¡¨è¾¾å¼
            # æ³¨æ„ï¼šå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å®‰å…¨çš„å®ç°
            # è¿™é‡Œä½¿ç”¨äº†éå¸¸ç®€åŒ–çš„æ–¹æ³•ï¼Œæœ‰å®‰å…¨é£é™©
            allowed_chars = set("0123456789+-*/() .")
            if not all(c in allowed_chars for c in expression):
                return {"error": "è¡¨è¾¾å¼åŒ…å«ä¸å…è®¸çš„å­—ç¬¦"}
            
            result = eval(expression)
            return {"result": result}
        except Exception as e:
            return {"error": f"è®¡ç®—é”™è¯¯: {str(e)}"}

class WikipediaTool(Tool):
    """ç»´åŸºç™¾ç§‘æŸ¥è¯¢å·¥å…·"""
    def __init__(self):
        super().__init__(
            name="search_wikipedia",
            description="æœç´¢ç»´åŸºç™¾ç§‘è·å–ä¿¡æ¯",
            parameters={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯"
                    },
                    "max_results": {
                        "type": "integer",
                        "description": "æœ€å¤§è¿”å›ç»“æœæ•°é‡"
                    }
                },
                "required": ["query"]
            }
        )
    
    def execute(self, query, max_results=1):
        # æ¨¡æ‹Ÿå®ç°ï¼Œå®é™…åº”è°ƒç”¨ç»´åŸºç™¾ç§‘API
        wiki_data = {
            "äººå·¥æ™ºèƒ½": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡ä»¿äººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚",
            "æœºå™¨å­¦ä¹ ": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºå¼€å‘èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›çš„ç®—æ³•ã€‚",
            "Python": "Pythonæ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„è§£é‡Šå‹ã€é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥ç®€æ´æ˜“è¯»çš„è¯­æ³•è‘—ç§°ã€‚"
        }
        
        # ç®€å•çš„æ¨¡ç³ŠåŒ¹é…
        results = []
        for key, value in wiki_data.items():
            if query.lower() in key.lower():
                results.append({"title": key, "summary": value})
                if len(results) >= max_results:
                    break
        
        if results:
            return {"results": results}
        else:
            return {"error": f"æœªæ‰¾åˆ°å…³äº'{query}'çš„ä¿¡æ¯"}

class Agent:
    """LLMä»£ç†ï¼Œèƒ½å¤Ÿè§„åˆ’å’Œæ‰§è¡Œä»»åŠ¡"""
    
    def __init__(self, tools: List[Tool], model="gpt-3.5-turbo"):
        self.tools = tools
        self.model = model
        self.messages = []
        self.tools_for_llm = [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.parameters
                }
            } for tool in tools
        ]
        self.tool_map = {tool.name: tool for tool in tools}
    
    def add_message(self, role, content):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²"""
        self.messages.append({"role": role, "content": content})
    
    def format_tool_results(self, results):
        """æ ¼å¼åŒ–å·¥å…·æ‰§è¡Œç»“æœï¼Œç”¨äºå±•ç¤º"""
        formatted = []
        for result in results:
            tool_name = result.get("function", {}).get("name", "unknown")
            tool_result = result.get("result", {})
            formatted.append(f"å·¥å…·: {tool_name}\nç»“æœ: {json.dumps(tool_result, ensure_ascii=False, indent=2)}")
        
        return "\n\n".join(formatted)
    
    def run(self, task, max_steps=5):
        """è¿è¡Œä»£ç†å®Œæˆä»»åŠ¡"""
        self.messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»£ç†ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥è§£å†³é—®é¢˜ã€‚åˆ†æä»»åŠ¡ï¼Œç¡®å®šéœ€è¦ä½¿ç”¨çš„å·¥å…·ï¼Œæ‰§è¡Œå·¥å…·å¹¶åˆ©ç”¨ç»“æœæ¥å®Œæˆä»»åŠ¡ã€‚"},
            {"role": "user", "content": task}
        ]
        
        steps_taken = 0
        final_answer = None
        
        while steps_taken < max_steps:
            steps_taken += 1
            print(f"\n===== æ­¥éª¤ {steps_taken} =====")
            
            # è°ƒç”¨LLMè·å–ä¸‹ä¸€æ­¥è¡ŒåŠ¨
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=self.messages,
                tools=self.tools_for_llm,
                tool_choice="auto"
            )
            
            response_message = response.choices[0].message
            self.messages.append(response_message)
            
            # æ£€æŸ¥æ˜¯å¦ç›´æ¥ç»™å‡ºäº†æœ€ç»ˆç­”æ¡ˆ
            if not response_message.get("tool_calls"):
                final_answer = response_message.get("content")
                print(f"ä»£ç†ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ: {final_answer}")
                break
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            print("ä»£ç†å†³å®šä½¿ç”¨å·¥å…·:")
            tool_results = []
            
            for tool_call in response_message.get("tool_calls", []):
                function_name = tool_call.get("function", {}).get("name")
                function_args = json.loads(tool_call.get("function", {}).get("arguments", "{}"))
                
                print(f"- è°ƒç”¨å·¥å…·: {function_name}, å‚æ•°: {function_args}")
                
                if function_name in self.tool_map:
                    # æ‰§è¡Œå·¥å…·
                    tool = self.tool_map[function_name]
                    result = tool.execute(**function_args)
                    
                    tool_results.append({
                        "tool_call_id": tool_call.get("id"),
                        "function": {"name": function_name, "arguments": function_args},
                        "result": result
                    })
                else:
                    print(f"è­¦å‘Š: æœªçŸ¥å·¥å…· {function_name}")
            
            # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯ä¸­
            for result in tool_results:
                self.messages.append({
                    "role": "tool",
                    "tool_call_id": result["tool_call_id"],
                    "content": json.dumps(result["result"])
                })
            
            formatted_results = self.format_tool_results(tool_results)
            print(f"å·¥å…·æ‰§è¡Œç»“æœ:\n{formatted_results}")
            
            # å¦‚æœè¿™æ˜¯æœ€åä¸€æ­¥ï¼Œè®©ä»£ç†ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ
            if steps_taken == max_steps:
                final_response = openai.ChatCompletion.create(
                    model=self.model,
                    messages=self.messages + [
                        {"role": "user", "content": "ä½ å·²ç»ä½¿ç”¨äº†æ‰€æœ‰å¯ç”¨æ­¥éª¤ã€‚è¯·åŸºäºæ”¶é›†åˆ°çš„ä¿¡æ¯ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"}
                    ]
                )
                final_answer = final_response.choices[0].message.get("content")
                print(f"è¾¾åˆ°æœ€å¤§æ­¥éª¤æ•°ã€‚ä»£ç†ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ: {final_answer}")
        
        return {
            "task": task,
            "steps_taken": steps_taken,
            "final_answer": final_answer,
            "conversation": self.messages
        }

# ä½¿ç”¨ç¤ºä¾‹
def agent_example():
    # åˆ›å»ºå·¥å…·
    tools = [
        WeatherTool(),
        CalculatorTool(),
        WikipediaTool()
    ]
    
    # åˆ›å»ºä»£ç†
    agent = Agent(tools)
    
    # è¿è¡Œä»£ç†å®Œæˆä»»åŠ¡
    result = agent.run("åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿè®¡ç®—ä¸€ä¸‹(23+17)*2ï¼Œä»¥åŠæŸ¥è¯¢ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚")
    
    print("\n===== æœ€ç»ˆç»“æœ =====")
    print(f"ä»»åŠ¡: {result['task']}")
    print(f"æ­¥éª¤æ•°: {result['steps_taken']}")
    print(f"æœ€ç»ˆç­”æ¡ˆ: {result['final_answer']}")

# è¿è¡Œç¤ºä¾‹
agent_example()
```

## 5. æ„å»ºå®é™…åº”ç”¨

### 5.1 æ–‡æ¡£é—®ç­”åº”ç”¨

æ„å»ºä¸€ä¸ªèƒ½å›ç­”ä¸ç‰¹å®šæ–‡æ¡£ç›¸å…³é—®é¢˜çš„åº”ç”¨ï¼š

```python
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import os
import shutil
import uuid
from langchain.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
import openai

app = FastAPI(title="æ–‡æ¡£é—®ç­”åº”ç”¨")

# è®¾ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ–‡æ¡£å­˜å‚¨è·¯å¾„
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# å‘é‡æ•°æ®åº“å­˜å‚¨
vectorstores = {}

# é…ç½®OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")

# æ–‡æ¡£åŠ è½½å™¨æ˜ å°„
LOADER_MAPPING = {
    ".pdf": PyPDFLoader,
    ".txt": TextLoader,
    ".docx": Docx2txtLoader
}

@app.post("/upload-document")
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£å¹¶å¤„ç†"""
    try:
        # ç”Ÿæˆå”¯ä¸€æ–‡æ¡£ID
        document_id = str(uuid.uuid4())
        
        # è·å–æ–‡ä»¶æ‰©å±•å
        _, ext = os.path.splitext(file.filename)
        if ext.lower() not in LOADER_MAPPING:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}")
        
        # ä¿å­˜æ–‡ä»¶
        file_path = os.path.join(UPLOAD_DIR, f"{document_id}{ext}")
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # å¤„ç†æ–‡æ¡£
        result = await process_document(document_id, file_path, ext)
        
        return {
            "document_id": document_id,
            "filename": file.filename,
            "chunks": result["chunks"],
            "message": "æ–‡æ¡£ä¸Šä¼ å¹¶å¤„ç†æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

async def process_document(document_id, file_path, extension):
    """å¤„ç†æ–‡æ¡£å¹¶åˆ›å»ºå‘é‡å­˜å‚¨"""
    try:
        # é€‰æ‹©åˆé€‚çš„åŠ è½½å™¨
        loader_class = LOADER_MAPPING[extension.lower()]
        loader = loader_class(file_path)
        
        # åŠ è½½æ–‡æ¡£
        documents = loader.load()
        
        # åˆ†å‰²æ–‡æ¡£
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(documents)
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_documents(chunks, embeddings)
        
        # ä¿å­˜å‘é‡å­˜å‚¨
        vectorstores[document_id] = vectorstore
        
        return {
            "chunks": len(chunks),
            "message": "æ–‡æ¡£å¤„ç†æˆåŠŸ"
        }
    except Exception as e:
        # æ¸…ç†æ–‡ä»¶
        if os.path.exists(file_path):
            os.remove(file_path)
        raise Exception(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

@app.post("/ask")
async def ask_question(document_id: str = Form(...), question: str = Form(...)):
    """åŸºäºæ–‡æ¡£å›ç­”é—®é¢˜"""
    try:
        if document_id not in vectorstores:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
        
        # ä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢ç›¸å…³å†…å®¹
        vectorstore = vectorstores[document_id]
        docs = vectorstore.similarity_search(question, k=3)
        
        # æå–æ–‡æ¡£å†…å®¹
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # æ„å»ºæç¤º
        prompt = f"""
        ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚

        ä¸Šä¸‹æ–‡:
        {context}

        é—®é¢˜: {question}
        """
        
        # è°ƒç”¨LLMç”Ÿæˆå›ç­”
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼Œæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5
        )
        
        answer = response.choices[0].message.content
        
        return {
            "question": question,
            "answer": answer,
            "context": [doc.page_content for doc in docs]
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/documents/{document_id}")
async def delete_document(document_id: str):
    """åˆ é™¤æ–‡æ¡£"""
    if document_id in vectorstores:
        # ç§»é™¤å‘é‡å­˜å‚¨
        del vectorstores[document_id]
        
        # å°è¯•åˆ é™¤æ–‡ä»¶
        for ext in LOADER_MAPPING.keys():
            file_path = os.path.join(UPLOAD_DIR, f"{document_id}{ext}")
            if os.path.exists(file_path):
                os.remove(file_path)
                return {"message": f"æ–‡æ¡£ {document_id} å·²åˆ é™¤"}
        
        return {"message": f"å‘é‡å­˜å‚¨å·²åˆ é™¤ï¼Œä½†æœªæ‰¾åˆ°å¯¹åº”æ–‡ä»¶"}
    else:
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")

# è¿è¡ŒæœåŠ¡å™¨
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 5.2 AIåŠ©æ‰‹æœºå™¨äºº

æ„å»ºèƒ½åœ¨å¤šå¹³å°éƒ¨ç½²çš„AIåŠ©æ‰‹æœºå™¨äººï¼š

```python
import os
import json
import logging
import asyncio
import openai
from fastapi import FastAPI, Request, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
import uvicorn
import requests
from typing import Dict, List, Optional, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ai-assistant")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="AIåŠ©æ‰‹æœºå™¨äºº")

# é…ç½®OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")

# ç”¨æˆ·ä¼šè¯å­˜å‚¨
user_sessions = {}

# å®šä¹‰åŠ©æ‰‹é…ç½®
ASSISTANT_CONFIG = {
    "name": "æ™ºèƒ½å°åŠ©æ‰‹",
    "description": "ä¸€ä¸ªä¹äºå¸®åŠ©ç”¨æˆ·çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œå¸®åŠ©å®Œæˆä»»åŠ¡ã€‚",
    "instructions": """
    ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨ã€ç¤¼è²Œã€å‹å¥½çš„åŠ©æ‰‹ã€‚å›ç­”ç”¨æˆ·çš„é—®é¢˜æ—¶ï¼š
    1. ä¿æŒå›ç­”ç®€æ´æ¸…æ™°
    2. å½“ä¸ç¡®å®šæ—¶ï¼Œæ‰¿è®¤ä½ çš„å±€é™æ€§
    3. é¿å…æœ‰å®³ã€ä¸å½“æˆ–å…·æœ‰æ­§è§†æ€§çš„å›ç­”
    4. ä½¿ç”¨ç›¸å…³äº‹å®å’Œä¿¡æ¯å›ç­”
    5. å½“ç”¨æˆ·é—®å€™æ—¶ï¼Œçƒ­æƒ…å›åº”
    6. ä½¿ç”¨emojiè¡¨æƒ…ä½¿å¯¹è¯æ›´å‹å¥½ ğŸ™‚
    """
}

# æ¶ˆæ¯è®°å¿†ç®¡ç†
class MessageMemory:
    """ç®¡ç†ç”¨æˆ·ä¼šè¯çš„æ¶ˆæ¯å†å²"""
    
    def __init__(self, max_messages=20):
        self.max_messages = max_messages
        self.messages = [
            {"role": "system", "content": ASSISTANT_CONFIG["instructions"]}
        ]
    
    def add_message(self, role, content):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        self.messages.append({"role": role, "content": content})
        
        # å¦‚æœè¶…è¿‡æœ€å¤§æ¶ˆæ¯æ•°ï¼Œç§»é™¤æœ€æ—©çš„ç”¨æˆ·å’ŒåŠ©æ‰‹æ¶ˆæ¯
        if len(self.messages) > self.max_messages + 1:  # +1æ˜¯å› ä¸ºç³»ç»Ÿæ¶ˆæ¯
            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
            excess = len(self.messages) - self.max_messages - 1
            self.messages = [self.messages[0]] + self.messages[excess+1:]
    
    def get_messages(self):
        """è·å–æ‰€æœ‰æ¶ˆæ¯"""
        return self.messages
    
    def clear(self):
        """æ¸…é™¤æ‰€æœ‰å†å²ï¼Œä½†ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯"""
        self.messages = [self.messages[0]]

# å¹³å°é€‚é…å™¨
class PlatformAdapter:
    """ä¸åŒå¹³å°çš„æ¶ˆæ¯é€‚é…å™¨åŸºç±»"""
    
    def format_response(self, content, user_id):
        """å°†åŠ©æ‰‹å›å¤æ ¼å¼åŒ–ä¸ºå¹³å°ç‰¹å®šæ ¼å¼"""
        raise NotImplementedError
    
    def parse_message(self, payload):
        """ä»å¹³å°ç‰¹å®šæ ¼å¼è§£æç”¨æˆ·æ¶ˆæ¯"""
        raise NotImplementedError

class WebAdapter(PlatformAdapter):
    """Webå¹³å°é€‚é…å™¨"""
    
    def format_response(self, content, user_id):
        return {"message": content, "user_id": user_id}
    
    def parse_message(self, payload):
        return {
            "user_id": payload.get("user_id"),
            "message": payload.get("message"),
            "platform": "web"
        }

class SlackAdapter(PlatformAdapter):
    """Slackå¹³å°é€‚é…å™¨"""
    
    def format_response(self, content, user_id):
        return {
            "blocks": [
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": content
                    }
                }
            ]
        }
    
    def parse_message(self, payload):
        event = payload.get("event", {})
        return {
            "user_id": event.get("user"),
            "message": event.get("text"),
            "platform": "slack",
            "channel": event.get("channel")
        }

# å¹³å°é€‚é…å™¨æ˜ å°„
platform_adapters = {
    "web": WebAdapter(),
    "slack": SlackAdapter()
}

# AIå¤„ç†é€»è¾‘
async def process_message(user_message, user_id, platform="web"):
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶ç”Ÿæˆå›å¤"""
    # è·å–æˆ–åˆ›å»ºç”¨æˆ·ä¼šè¯
    if user_id not in user_sessions:
        user_sessions[user_id] = MessageMemory()
    
    memory = user_sessions[user_id]
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    memory.add_message("user", user_message)
    
    try:
        # è°ƒç”¨OpenAIç”Ÿæˆå›å¤
        response = await openai.ChatCompletion.acreate(
            model="gpt-3.5-turbo",
            messages=memory.get_messages(),
            temperature=0.7,
            max_tokens=500
        )
        
        # æå–åŠ©æ‰‹å›å¤
        assistant_message = response.choices[0].message.content
        
        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        memory.add_message("assistant", assistant_message)
        
        # ä½¿ç”¨é€‚é…å™¨æ ¼å¼åŒ–å›å¤
        adapter = platform_adapters.get(platform, platform_adapters["web"])
        formatted_response = adapter.format_response(assistant_message, user_id)
        
        return formatted_response
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")
        error_message = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”ã€‚è¯·ç¨åå†è¯•ã€‚"
        memory.add_message("assistant", error_message)
        
        adapter = platform_adapters.get(platform, platform_adapters["web"])
        return adapter.format_response(error_message, user_id)

# APIç«¯ç‚¹
@app.post("/chat")
async def chat_endpoint(request: Request, background_tasks: BackgroundTasks):
    """é€šç”¨èŠå¤©ç«¯ç‚¹ï¼Œæ”¯æŒä¸åŒå¹³å°"""
    try:
        payload = await request.json()
        
        # ç¡®å®šå¹³å°
        platform = payload.get("platform", "web")
        
        if platform not in platform_adapters:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„å¹³å°: {platform}")
        
        # è§£ææ¶ˆæ¯
        adapter = platform_adapters[platform]
        parsed_message = adapter.parse_message(payload)
        
        user_id = parsed_message.get("user_id")
        message = parsed_message.get("message")
        
        if not user_id or not message:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘user_idæˆ–message")
        
        # å¤„ç†æ¶ˆæ¯
        response = await process_message(message, user_id, platform)
        
        # å¦‚æœæ˜¯Slackï¼Œéœ€è¦å¼‚æ­¥å‘é€å›å¤
        if platform == "slack":
            background_tasks.add_task(
                send_slack_response,
                response,
                parsed_message.get("channel")
            )
            return {"status": "processing"}
        
        return response
    
    except Exception as e:
        logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

async def send_slack_response(response, channel):
    """å‘Slackå‘é€å›å¤"""
    slack_token = os.getenv("SLACK_BOT_TOKEN")
    headers = {
        "Content-Type": "application/json; charset=utf-8",
        "Authorization": f"Bearer {slack_token}"
    }
    
    payload = {
        "channel": channel,
        "blocks": response["blocks"]
    }
    
    try:
        requests.post(
            "https://slack.com/api/chat.postMessage",
            headers=headers,
            json=payload
        )
    except Exception as e:
        logger.error(f"å‘é€Slackæ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")

@app.delete("/sessions/{user_id}")
async def clear_session(user_id: str):
    """æ¸…é™¤ç”¨æˆ·ä¼šè¯å†å²"""
    if user_id in user_sessions:
        user_sessions[user_id].clear()
        return {"message": f"ç”¨æˆ· {user_id} çš„ä¼šè¯å·²æ¸…é™¤"}
    else:
        raise HTTPException(status_code=404, detail="ç”¨æˆ·ä¼šè¯æœªæ‰¾åˆ°")

# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## 6. LLMåº”ç”¨å¼€å‘æœ€ä½³å®è·µ

### 6.1 æç¤ºå·¥ç¨‹ä¸ç³»ç»Ÿè®¾è®¡

- **æ˜ç¡®çš„ç³»ç»Ÿæç¤º**ï¼šä¸ºæ¯ä¸ªåº”ç”¨å®šä¹‰æ¸…æ™°çš„ç³»ç»Ÿæç¤ºï¼Œè®¾å®šæ¨¡å‹è¡Œä¸ºè¾¹ç•Œ
- **åˆ†å±‚æç¤ºç­–ç•¥**ï¼šå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªæ­¥éª¤ï¼Œæ¯æ­¥ä½¿ç”¨ä¸“é—¨è®¾è®¡çš„æç¤º
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šå°†åº”ç”¨åˆ†è§£ä¸ºåŠŸèƒ½ç‹¬ç«‹çš„æ¨¡å—ï¼Œä¾¿äºæµ‹è¯•å’Œç»´æŠ¤
- **é”™è¯¯å¤„ç†**ï¼šè®¾è®¡å¥å£®çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œé˜²æ­¢æ¨¡å‹å¤±æ•ˆæ—¶åº”ç”¨å´©æºƒ
- **ç”¨æˆ·åé¦ˆæœºåˆ¶**ï¼šæ”¶é›†ç”¨æˆ·åé¦ˆæ”¹è¿›æç¤ºå’Œåº”ç”¨åŠŸèƒ½

### 6.2 æ€§èƒ½ä¼˜åŒ–ä¸æˆæœ¬æ§åˆ¶

- **ç¼“å­˜ç›¸ä¼¼è¯·æ±‚**ï¼šå¯¹é¢‘ç¹æŸ¥è¯¢çš„é—®é¢˜ç¼“å­˜ç»“æœï¼Œå‡å°‘APIè°ƒç”¨
- **æ‰¹å¤„ç†è¯·æ±‚**ï¼šå°†å¤šä¸ªè¯·æ±‚åˆå¹¶å¤„ç†ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
- **è°ƒæ•´æ¨¡å‹å‚æ•°**ï¼šæ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¤§å°å’Œå‚æ•°è®¾ç½®
- **æœ¬åœ°éƒ¨ç½²è½»é‡çº§æ¨¡å‹**ï¼šå¯¹äºç®€å•ä»»åŠ¡ï¼Œè€ƒè™‘ä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„å°å‹æ¨¡å‹
- **ç›‘æ§ä½¿ç”¨é‡**ï¼šå»ºç«‹ç›‘æ§ç³»ç»Ÿï¼Œè·Ÿè¸ªAPIè°ƒç”¨å’Œæˆæœ¬

### 6.3 å®‰å…¨æ€§ä¸éšç§ä¿æŠ¤

- **è¾“å…¥è¿‡æ»¤**ï¼šè¿‡æ»¤æ•æ„Ÿæˆ–æœ‰å®³çš„ç”¨æˆ·è¾“å…¥
- **è¾“å‡ºå®¡æŸ¥**ï¼šæ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦ç¬¦åˆå®‰å…¨æ ‡å‡†
- **æ•°æ®æœ€å°åŒ–**ï¼šåªæ”¶é›†å’Œå¤„ç†å¿…è¦çš„ç”¨æˆ·æ•°æ®
- **åŠ å¯†å­˜å‚¨**ï¼šåŠ å¯†å­˜å‚¨æ•æ„Ÿä¿¡æ¯å’Œå¯¹è¯å†å²
- **è®¿é—®æ§åˆ¶**ï¼šå®æ–½ç»†ç²’åº¦çš„è®¿é—®æ§åˆ¶æœºåˆ¶

## æ€»ç»“

LLMåº”ç”¨å¼€å‘æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œèåˆäº†AIæŠ€æœ¯ã€è½¯ä»¶å·¥ç¨‹å’Œç”¨æˆ·ä½“éªŒè®¾è®¡ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå¼€å‘è€…å¯ä»¥åˆ›å»ºæ™ºèƒ½äº¤äº’å¼åº”ç”¨ï¼Œè§£å†³å„ç§å¤æ‚é—®é¢˜ã€‚

ä»ç®€å•çš„èŠå¤©æœºå™¨äººåˆ°å¤æ‚çš„ä»£ç†ç³»ç»Ÿï¼ŒLLMåº”ç”¨çš„èŒƒå›´éå¸¸å¹¿æ³›ã€‚å…³é”®æ˜¯ç†è§£LLMçš„èƒ½åŠ›å’Œå±€é™æ€§ï¼Œè®¾è®¡é€‚å½“çš„æç¤ºå’Œç³»ç»Ÿæ¶æ„ï¼Œå¹¶å®æ–½å¿…è¦çš„å®‰å…¨æªæ–½å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

éšç€æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼ŒLLMåº”ç”¨å°†å˜å¾—æ›´åŠ å¼ºå¤§å’Œæ˜“äºå¼€å‘ã€‚æŒæ¡æœ¬æ–‡ä»‹ç»çš„æŠ€æœ¯å’Œæœ€ä½³å®è·µï¼Œå°†ä½¿ä½ èƒ½å¤Ÿæ„å»ºé«˜è´¨é‡ã€å®ç”¨çš„AIåº”ç”¨ï¼Œä¸ºç”¨æˆ·æä¾›æœ‰ä»·å€¼çš„æœåŠ¡ã€‚

Similar code found with 2 license types