# 数据清洗技术基础指南

数据清洗是数据分析和机器学习过程中的关键步骤，它能确保我们使用的数据是准确、完整和一致的。本指南将介绍常见的数据清洗技术，并提供简单易懂的代码示例。

## 1. 处理缺失值

缺失值是数据集中常见的问题，有多种处理方法：

### 1.1 识别缺失值

```python
import pandas as pd
import numpy as np

# 创建一个包含缺失值的数据框
df = pd.DataFrame({
    'A': [1, 2, np.nan, 4],
    'B': [5, np.nan, np.nan, 8],
    'C': [9, 10, 11, 12]
})

# 检查缺失值
print("显示缺失值情况：")
print(df.isnull())

# 计算每列缺失值数量
print("\n每列缺失值数量：")
print(df.isnull().sum())

# 计算每行缺失值数量
print("\n每行缺失值数量：")
print(df.isnull().sum(axis=1))
```

### 1.2 删除缺失值

```python
# 删除包含任何缺失值的行
df_dropped_rows = df.dropna()
print("删除包含缺失值的行后：")
print(df_dropped_rows)

# 删除全部为缺失值的列
df_dropped_cols = df.dropna(axis=1, how='all')
print("\n删除全部为缺失值的列后：")
print(df_dropped_cols)
```

### 1.3 填充缺失值

```python
# 用特定值填充缺失值
df_filled = df.fillna(0)
print("用0填充缺失值后：")
print(df_filled)

# 用列均值填充缺失值
df_mean_filled = df.fillna(df.mean())
print("\n用均值填充缺失值后：")
print(df_mean_filled)

# 前向填充（使用前一个值）
df_ffill = df.fillna(method='ffill')
print("\n前向填充后：")
print(df_ffill)

# 后向填充（使用后一个值）
df_bfill = df.fillna(method='bfill')
print("\n后向填充后：")
print(df_bfill)
```

## 2. 处理异常值

异常值可能会扭曲分析结果，需要识别并适当处理：

### 2.1 识别异常值

```python
# 创建包含异常值的数据框
df = pd.DataFrame({
    'A': [1, 2, 3, 100, 5]
})

# 使用箱线图可视化
import matplotlib.pyplot as plt

plt.boxplot(df['A'])
plt.title('箱线图识别异常值')
plt.grid(True)
# plt.show()  # 在实际运行时取消注释

# 使用Z-Score识别异常值
from scipy import stats

z_scores = stats.zscore(df['A'])
print("Z-scores：")
print(z_scores)
print("\n异常值（|Z| > 2）：")
print(df[abs(z_scores) > 2])

# 使用IQR(四分位距)方法
Q1 = df['A'].quantile(0.25)
Q3 = df['A'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print("\nIQR范围：", lower_bound, "到", upper_bound)
print("异常值：")
print(df[(df['A'] < lower_bound) | (df['A'] > upper_bound)])
```

### 2.2 处理异常值

```python
# 删除异常值
df_no_outliers = df[(df['A'] >= lower_bound) & (df['A'] <= upper_bound)]
print("删除异常值后：")
print(df_no_outliers)

# 替换异常值（例如用上下限值）
df_capped = df.copy()
df_capped.loc[df_capped['A'] > upper_bound, 'A'] = upper_bound
df_capped.loc[df_capped['A'] < lower_bound, 'A'] = lower_bound
print("\n限制异常值后：")
print(df_capped)
```

## 3. 数据转换

转换数据以适应分析需求：

### 3.1 类型转换

```python
# 创建混合类型的数据框
df = pd.DataFrame({
    'A': ['1', '2', '3', '4'],
    'B': [5, 6, 7, 8]
})

print("原始数据类型：")
print(df.dtypes)

# 转换列A为数值型
df['A'] = pd.to_numeric(df['A'])
print("\n转换后数据类型：")
print(df.dtypes)

# 日期时间转换
df_dates = pd.DataFrame({
    'date': ['2023-01-01', '2023-01-02', '20230103', '01/04/2023']
})

df_dates['date'] = pd.to_datetime(df_dates['date'], errors='coerce')
print("\n转换为日期格式：")
print(df_dates)
```

### 3.2 标准化和归一化

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# 创建数据框
df = pd.DataFrame({
    'A': [1, 2, 3, 4, 5],
    'B': [10, 20, 30, 40, 50]
})

# 标准化（Z-score标准化）
scaler = StandardScaler()
df_scaled = pd.DataFrame(
    scaler.fit_transform(df),
    columns=df.columns
)
print("标准化后的数据：")
print(df_scaled)

# 归一化（Min-Max缩放）
min_max_scaler = MinMaxScaler()
df_normalized = pd.DataFrame(
    min_max_scaler.fit_transform(df),
    columns=df.columns
)
print("\n归一化后的数据：")
print(df_normalized)
```

## 4. 处理重复数据

重复数据可能导致分析偏差：

```python
# 创建包含重复行的数据框
df = pd.DataFrame({
    'A': [1, 2, 2, 3],
    'B': [5, 6, 6, 7]
})

# 检查重复行
print("重复行情况：")
print(df.duplicated())

# 获取重复行
print("\n重复的行：")
print(df[df.duplicated()])

# 删除重复行
df_unique = df.drop_duplicates()
print("\n删除重复行后：")
print(df_unique)

# 只保留特定列不重复的行
df_unique_col = df.drop_duplicates(subset=['A'])
print("\n基于列A删除重复行后：")
print(df_unique_col)
```

## 5. 文本数据清洗

处理文本数据中的常见问题：

```python
# 创建文本数据框
df = pd.DataFrame({
    'text': [' Hello, World! ', 'Python  Programming', 'Data\tCleaning', 'NLP\nProcessing']
})

# 去除空白字符
df['text_stripped'] = df['text'].str.strip()
print("去除两端空白字符：")
print(df[['text', 'text_stripped']])

# 替换字符
df['text_replaced'] = df['text'].str.replace('\t', ' ').str.replace('\n', ' ')
print("\n替换特殊字符：")
print(df[['text', 'text_replaced']])

# 统一大小写
df['text_lower'] = df['text'].str.lower()
print("\n转换为小写：")
print(df[['text', 'text_lower']])

# 正则表达式处理
import re
df['text_clean'] = df['text'].apply(lambda x: re.sub(r'[^\w\s]', '', x))
print("\n使用正则表达式去除标点符号：")
print(df[['text', 'text_clean']])
```

## 6. 综合实例：完整数据清洗流程

下面是一个综合数据清洗的例子，结合了上述多种技术：

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# 创建一个模拟数据集
data = {
    'age': [25, 30, np.nan, 40, 22, 22, -5, 120],
    'income': [50000, 60000, 40000, np.nan, 45000, 45000, 1000000, 30000],
    'education': [' Bachelor ', 'Master', '  PhD ', 'Bachelor', np.nan, 'bachelor', 'Doctor', 'High School'],
    'join_date': ['2020-01-15', '2019/05/20', 'invalid', '2021.07.30', '2020-03-10', '2020-03-10', '20180520', '2022-01-01']
}

df = pd.DataFrame(data)
print("原始数据：")
print(df)

# 第1步：查看基本信息
print("\n数据信息：")
print(df.info())
print("\n数据描述：")
print(df.describe())

# 第2步：处理缺失值
# 使用均值填充年龄
df['age'] = df['age'].fillna(df['age'].mean())
# 使用中位数填充收入
df['income'] = df['income'].fillna(df['income'].median())
# 使用众数填充教育程度
df['education'] = df['education'].fillna(df['education'].mode()[0])

# 第3步：处理异常值
# 对年龄的异常值进行处理（例如负值和过大值）
df.loc[df['age'] < 0, 'age'] = df['age'].median()
df.loc[df['age'] > 100, 'age'] = df['age'].median()

# 对收入的异常值进行处理
Q1 = df['income'].quantile(0.25)
Q3 = df['income'].quantile(0.75)
IQR = Q3 - Q1
df.loc[df['income'] > Q3 + 1.5 * IQR, 'income'] = Q3 + 1.5 * IQR

# 第4步：标准化文本字段
# 清理并标准化教育程度
df['education'] = df['education'].str.strip().str.lower()

# 第5步：转换日期格式
df['join_date'] = pd.to_datetime(df['join_date'], errors='coerce')
# 填充无效日期
df['join_date'] = df['join_date'].fillna(pd.to_datetime('2020-01-01'))

# 第6步：删除重复行
df = df.drop_duplicates()

# 第7步：特征工程
# 从日期提取年和月
df['join_year'] = df['join_date'].dt.year
df['join_month'] = df['join_date'].dt.month

# 输出清洗后的数据
print("\n清洗后的数据：")
print(df)

# 可视化清洗结果
plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
plt.hist(df['age'], bins=10, alpha=0.7)
plt.title('年龄分布')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.hist(df['income'], bins=10, alpha=0.7)
plt.title('收入分布')
plt.grid(True)

plt.tight_layout()
# plt.show()  # 在实际运行时取消注释
```

## 7. 总结与最佳实践

- **始终检查数据质量**：在开始分析前，检查缺失值、异常值和数据类型
- **保留原始数据**：清洗前保存原始数据副本，方便比较和回溯
- **记录清洗步骤**：使用注释和文档记录每一步清洗操作
- **自动化重复任务**：对于常规数据清洗，创建可重用的脚本或函数
- **结合领域知识**：根据实际业务场景决定最佳清洗策略
- **验证清洗结果**：清洗后检查数据是否符合预期

通过掌握这些数据清洗技术，你可以确保数据分析和机器学习模型基于高质量的数据，从而获得更可靠的结果。