# Hugging Face åº“ä½¿ç”¨ï¼šä»é›¶å¼€å§‹çš„å®Œæ•´æŒ‡å—

## 1. åŸºç¡€æ¦‚å¿µç†è§£

### ä»€ä¹ˆæ˜¯Hugging Faceï¼Ÿ

Hugging Faceæ˜¯ç›®å‰æœ€æµè¡Œçš„å¼€æºNLPæŠ€æœ¯ç¤¾åŒºå’Œå¹³å°ï¼Œæä¾›äº†ä¸€å¥—å¼ºå¤§çš„å·¥å…·å’Œåº“ï¼Œä½¿è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å’Œæœºå™¨å­¦ä¹ å˜å¾—æ›´åŠ å¹³æ˜“è¿‘äººã€‚å®ƒçš„æ ¸å¿ƒç†å¿µæ˜¯"æ°‘ä¸»åŒ–æœºå™¨å­¦ä¹ "ï¼Œè®©æœ€å…ˆè¿›çš„AIæŠ€æœ¯å¯ä»¥è¢«æ›´å¤šäººä½¿ç”¨å’Œç†è§£ã€‚

### Hugging Faceç”Ÿæ€ç³»ç»Ÿ

Hugging Faceç”Ÿæ€ç³»ç»Ÿç”±å‡ ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼š

```
Hugging Faceç”Ÿæ€ç³»ç»Ÿ
â”œâ”€â”€ ğŸ¤— Transformers - é¢„è®­ç»ƒæ¨¡å‹åº“
â”œâ”€â”€ ğŸ¤— Datasets - æ•°æ®é›†å·¥å…·
â”œâ”€â”€ ğŸ¤— Tokenizers - é«˜æ•ˆåˆ†è¯å™¨
â”œâ”€â”€ ğŸ¤— Hub - æ¨¡å‹å’Œæ•°æ®é›†å…±äº«å¹³å°
â”œâ”€â”€ ğŸ¤— Accelerate - åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·
â””â”€â”€ ğŸ¤— Spaces - åº”ç”¨å±•ç¤ºå’Œéƒ¨ç½²å¹³å°
```

### æ ¸å¿ƒä»·å€¼ä¸ç‰¹ç‚¹

1. **å¼€ç®±å³ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹**ï¼šæä¾›è¶…è¿‡10,000ä¸ªé¢„è®­ç»ƒæ¨¡å‹
2. **ä¸€è‡´çš„APIæ¥å£**ï¼šç»Ÿä¸€çš„æ¨¡å‹æ¥å…¥æ–¹å¼ï¼Œæ”¯æŒPyTorchå’ŒTensorFlow
3. **ç¤¾åŒºé©±åŠ¨**ï¼šå¤§å‹æ´»è·ƒç¤¾åŒºä¸æ–­è´¡çŒ®å’Œæ”¹è¿›æ¨¡å‹
4. **æ¨¡å‹å…±äº«å¹³å°**ï¼šå¼€å‘è€…å¯ä»¥è½»æ¾åˆ†äº«å’Œä½¿ç”¨å½¼æ­¤çš„æ¨¡å‹
5. **æ˜“äºä½¿ç”¨**ï¼šå¤§å¹…é™ä½äº†ä½¿ç”¨æœ€æ–°æŠ€æœ¯çš„é—¨æ§›

### åŸºç¡€æ¦‚å¿µä¸æœ¯è¯­

- **é¢„è®­ç»ƒæ¨¡å‹**ï¼šåœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼Œå¯è¿›è¡Œå¾®è°ƒ
- **Pipeline**ï¼šå°è£…å®Œæ•´å¤„ç†æµç¨‹çš„é«˜çº§API
- **Tokenizer**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥çš„å·¥å…·
- **å¾®è°ƒ(Fine-tuning)**ï¼šè°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡
- **Hub**ï¼šç”¨äºåˆ†äº«å’Œå‘ç°æ¨¡å‹çš„å¹³å°

## 2. æŠ€æœ¯ç»†èŠ‚æ¢ç´¢

### Transformersåº“æ¶æ„

Transformersåº“é‡‡ç”¨äº†æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œä¸»è¦ç»„ä»¶åŒ…æ‹¬ï¼š

```python
# æ ¸å¿ƒç»„ä»¶å…³ç³»
Model <--> Configuration
   â†‘
Tokenizer <--> PreTrainedTokenizer
   â†“
Pipeline --> Processor --> Feature Extractor
```

#### æ¨¡å‹æ¶æ„ä¸ç»„ç»‡

Transformersä¸­çš„æ¨¡å‹æŒ‰æ¶æ„ç±»å‹ç»„ç»‡ï¼š

```
æ¨¡å‹ä½“ç³»ç»“æ„
â”œâ”€â”€ ç¼–ç å™¨æ¨¡å‹(Encoder-only): BERT, RoBERTa, DistilBERT
â”œâ”€â”€ è§£ç å™¨æ¨¡å‹(Decoder-only): GPT, OPT, LLaMA
â””â”€â”€ ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹(Encoder-Decoder): T5, BART, Pegasus
```

æ¯ç§æ¨¡å‹æä¾›äº†å¤šç§å˜ä½“ç±»ï¼Œç”¨äºä¸åŒä»»åŠ¡ï¼š

```python
# BERTæ¨¡å‹å˜ä½“ç¤ºä¾‹
BertModel               # åŸºç¡€BERTæ¨¡å‹
BertForSequenceClassification  # ç”¨äºåºåˆ—åˆ†ç±»
BertForQuestionAnswering       # ç”¨äºé—®ç­”ä»»åŠ¡
BertForTokenClassification     # ç”¨äºæ ‡è®°åˆ†ç±»(å¦‚NER)
BertForMaskedLM               # ç”¨äºæ©ç è¯­è¨€å»ºæ¨¡
```

### åˆ†è¯å™¨(Tokenizer)æŠ€æœ¯

åˆ†è¯å™¨è´Ÿè´£å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥ï¼Œå¤„ç†æµç¨‹ä¸ºï¼š

1. **æ ‡è®°åŒ–(Tokenization)**ï¼šå°†æ–‡æœ¬åˆ†å‰²æˆå•è¯/å­è¯
2. **è½¬æ¢ä¸ºID**ï¼šå°†æ ‡è®°æ˜ å°„åˆ°è¯æ±‡è¡¨ä¸­çš„æ•°å­—ID
3. **æ·»åŠ ç‰¹æ®Šæ ‡è®°**ï¼šå¦‚[CLS], [SEP], [PAD]ç­‰
4. **ç”Ÿæˆæ³¨æ„åŠ›æ©ç **ï¼šæ ‡è¯†å“ªäº›æ˜¯çœŸå®æ ‡è®°ï¼Œå“ªäº›æ˜¯å¡«å……

```python
# åˆ†è¯å™¨å·¥ä½œæµç¨‹
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# è¾“å…¥æ–‡æœ¬
text = "Hello, how are you?"

# å®Œæ•´å¤„ç†æµç¨‹
encoded = tokenizer(
    text,
    padding="max_length",  # å¡«å……ç­–ç•¥
    truncation=True,       # æˆªæ–­ç­–ç•¥
    max_length=10,         # æœ€å¤§é•¿åº¦
    return_tensors="pt"    # è¿”å›PyTorchå¼ é‡
)

# encodedåŒ…å«:
# - input_ids: æ ‡è®°IDåˆ—è¡¨
# - attention_mask: æ³¨æ„åŠ›æ©ç 
# - token_type_ids: æ ‡è®°ç±»å‹ID(ç”¨äºæŸäº›æ¨¡å‹)
```

### é…ç½®ç³»ç»Ÿ(Configuration)

æ¯ä¸ªæ¨¡å‹éƒ½æœ‰ç›¸åº”çš„é…ç½®ç±»ï¼Œå®šä¹‰äº†æ¨¡å‹çš„æ ¸å¿ƒå‚æ•°å’Œè¡Œä¸ºï¼š

```python
from transformers import BertConfig

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = BertConfig(
    vocab_size=30522,          # è¯æ±‡è¡¨å¤§å°
    hidden_size=768,           # éšè—å±‚ç»´åº¦
    num_hidden_layers=6,       # Transformerå±‚æ•°
    num_attention_heads=12,    # æ³¨æ„åŠ›å¤´æ•°
    intermediate_size=3072,    # å‰é¦ˆç½‘ç»œç»´åº¦
)

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆ›å»ºæ¨¡å‹
from transformers import BertModel
model = BertModel(config)  # ä»é…ç½®åˆ›å»ºæ¨¡å‹
```

### è‡ªåŠ¨ç±»æœºåˆ¶

Transformersåº“çš„æ ¸å¿ƒä¾¿åˆ©ç‰¹æ€§ä¹‹ä¸€æ˜¯"Auto"ç±»ï¼Œå¯è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ¨¡å‹ç±»ï¼š

```python
from transformers import AutoModel, AutoTokenizer

# è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹å’Œåˆ†è¯å™¨ç±»
model_name = "bert-base-uncased"
model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ä»¥ä¸‹ç±»ä¼¼çš„Autoç±»ç”¨äºä¸åŒä»»åŠ¡
# AutoModelForSequenceClassification
# AutoModelForQuestionAnswering
# AutoModelForTokenClassification
# AutoModelForMaskedLM
# AutoModelForCausalLM
```

### Datasetsåº“æ¶æ„

Datasetsåº“æä¾›äº†é«˜æ•ˆçš„æ•°æ®é›†å¤„ç†å·¥å…·ï¼š

```python
from datasets import load_dataset

# åŠ è½½å†…ç½®æ•°æ®é›†
squad_dataset = load_dataset("squad")  # åŠ è½½é—®ç­”æ•°æ®é›†
print(squad_dataset.column_names)      # æŸ¥çœ‹æ•°æ®åˆ—

# æ•°æ®æ˜ å°„å¤„ç†ï¼šåœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨å‡½æ•°
def preprocess_function(examples):
    return tokenizer(examples["question"], examples["context"], truncation=True)

tokenized_dataset = squad_dataset.map(preprocess_function, batched=True)

# æ•°æ®è¿‡æ»¤ã€é€‰æ‹©å’Œæ ¼å¼è½¬æ¢
filtered = tokenized_dataset.filter(lambda x: len(x["question"]) > 10)
selected = tokenized_dataset.select([0, 10, 20, 30])  # é€‰æ‹©ç‰¹å®šæ ·æœ¬
pytorch_dataset = tokenized_dataset.with_format("torch")  # è½¬ä¸ºPyTorchæ ¼å¼
```

## 3. å®è·µä¸å®ç°

### ç¯å¢ƒæ­å»ºä¸å®‰è£…

```bash
# åŸºæœ¬å®‰è£…
pip install transformers

# å®Œæ•´å®‰è£…(æ¨è)
pip install transformers[torch,sentencepiece,vision]

# å®‰è£…ç›¸å…³ç»„ä»¶
pip install datasets tokenizers accelerate
```

### æ–‡æœ¬åˆ†ç±»å®æˆ˜

ä»¥æƒ…æ„Ÿåˆ†æä¸ºä¾‹ï¼š

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# 1. åŠ è½½åˆ†è¯å™¨å’Œé¢„è®­ç»ƒæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# 2. å‡†å¤‡è¾“å…¥æ–‡æœ¬
text = "I really enjoyed this movie, it was fantastic!"

# 3. åˆ†è¯å¤„ç†
inputs = tokenizer(text, return_tensors="pt")

# 4. æ¨¡å‹æ¨ç†
with torch.no_grad():
    outputs = model(**inputs)
    
# 5. å¤„ç†é¢„æµ‹ç»“æœ
probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
positive_prob = probabilities[0][1].item()
print(f"Positive sentiment probability: {positive_prob:.4f}")

# è·å–é¢„æµ‹æ ‡ç­¾
predicted_class = torch.argmax(probabilities, dim=-1).item()
print(f"Predicted class: {'positive' if predicted_class == 1 else 'negative'}")
```

### ä½¿ç”¨Pipelineç®€åŒ–å·¥ä½œæµ

Pipelineæ˜¯æ›´é«˜çº§çš„æŠ½è±¡ï¼Œæ•´åˆäº†åˆ†è¯å’Œæ¨¡å‹æ¨ç†ï¼š

```python
from transformers import pipeline

# åˆ›å»ºå„ç§NLPä»»åŠ¡çš„pipeline
sentiment_analyzer = pipeline("sentiment-analysis")
question_answerer = pipeline("question-answering")
summarizer = pipeline("summarization")
generator = pipeline("text-generation")
translator = pipeline("translation", model="Helsinki-NLP/opus-mt-en-fr")

# æƒ…æ„Ÿåˆ†æ
result = sentiment_analyzer("I love this product!")
print(result)  # [{'label': 'POSITIVE', 'score': 0.999}]

# é—®ç­”
context = "Hugging Face was founded in 2016. It was originally focused on building conversational AI."
question = "When was Hugging Face founded?"
answer = question_answerer(question=question, context=context)
print(answer)  # {'answer': '2016', 'start': 22, 'end': 26, 'score': 0.98}

# æ–‡æœ¬æ‘˜è¦
summary = summarizer("Transformers library is developed by Hugging Face...", max_length=50, min_length=10)
print(summary)

# æ–‡æœ¬ç”Ÿæˆ
text = generator("Once upon a time", max_length=30, num_return_sequences=2)
print(text)

# ç¿»è¯‘
translation = translator("Hello, how are you?")
print(translation)  # [{'translation_text': 'Bonjour, comment allez-vous?'}]
```

### å¾®è°ƒBERTè¿›è¡Œæ–‡æœ¬åˆ†ç±»

ä»¥ä¸‹æ˜¯å¾®è°ƒBERTæ¨¡å‹è¿›è¡Œå¤šç±»åˆ†ç±»çš„å®Œæ•´ç¤ºä¾‹ï¼š

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# 1. åŠ è½½æ•°æ®é›†
dataset = load_dataset("glue", "mnli")

# 2. åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased", 
    num_labels=3  # MNLIæœ‰3ä¸ªç±»åˆ«
)

# 3. æ•°æ®é¢„å¤„ç†
def preprocess_function(examples):
    return tokenizer(
        examples["premise"],
        examples["hypothesis"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

tokenized_dataset = dataset.map(preprocess_function, batched=True)

# 4. å®šä¹‰è¯„ä¼°æŒ‡æ ‡
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="macro")
    }

# 5. è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",              # è¾“å‡ºç›®å½•
    learning_rate=2e-5,                  # å­¦ä¹ ç‡
    per_device_train_batch_size=16,      # è®­ç»ƒæ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size=16,       # è¯„ä¼°æ‰¹æ¬¡å¤§å°
    num_train_epochs=3,                  # è®­ç»ƒè½®æ•°
    weight_decay=0.01,                   # æƒé‡è¡°å‡
    evaluation_strategy="epoch",         # æ¯epochè¯„ä¼°ä¸€æ¬¡
    save_strategy="epoch",               # æ¯epochä¿å­˜ä¸€æ¬¡
    load_best_model_at_end=True,         # åŠ è½½æœ€ä½³æ¨¡å‹
)

# 6. åˆ›å»ºTrainerå®ä¾‹
trainer = Trainer(
    model=model,                        # æ¨¡å‹
    args=training_args,                 # è®­ç»ƒå‚æ•°
    train_dataset=tokenized_dataset["train"],  # è®­ç»ƒé›†
    eval_dataset=tokenized_dataset["validation_matched"],  # éªŒè¯é›†
    compute_metrics=compute_metrics,    # è¯„ä¼°æŒ‡æ ‡
)

# 7. å¼€å§‹å¾®è°ƒ
trainer.train()

# 8. ä¿å­˜æ¨¡å‹
model_path = "./bert-finetuned-mnli"
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

# 9. æ¨¡å‹è¯„ä¼°
eval_results = trainer.evaluate()
print(f"Evaluation results: {eval_results}")

# 10. ä½¿ç”¨å¾®è°ƒæ¨¡å‹è¿›è¡Œæ¨ç†
from transformers import pipeline
classifier = pipeline("text-classification", model=model_path, tokenizer=model_path)

result = classifier(
    "The company reported profits this quarter, contradicting analysts' expectations of losses."
)
print(result)
```

### æ–‡æœ¬ç”Ÿæˆä¸å¯¹è¯æ¨¡å‹

ä½¿ç”¨GPTæ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½GPT-2æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# ç¼–ç è¾“å…¥æ–‡æœ¬
input_text = "Once upon a time in a land far away,"
input_ids = tokenizer(input_text, return_tensors="pt").input_ids

# ç”Ÿæˆæ–‡æœ¬
output_sequences = model.generate(
    input_ids,
    max_length=100,               # æœ€å¤§é•¿åº¦
    num_return_sequences=3,       # è¿”å›3ä¸ªåºåˆ—
    temperature=0.8,              # æ¸©åº¦å‚æ•°(è¶Šé«˜è¶Šéšæœº)
    top_k=50,                    # Top-Ké‡‡æ ·
    top_p=0.95,                  # Top-P(æ ¸é‡‡æ ·)
    repetition_penalty=1.2,      # é‡å¤æƒ©ç½š
    do_sample=True,              # ä½¿ç”¨é‡‡æ ·
    no_repeat_ngram_size=2       # é¿å…é‡å¤çš„nå…ƒç»„
)

# è§£ç å¹¶æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬
for i, seq in enumerate(output_sequences):
    generated_text = tokenizer.decode(seq, skip_special_tokens=True)
    print(f"Generated {i+1}: {generated_text}")
```

### ä½¿ç”¨Hugging Faceæ•°æ®é›†API

```python
from datasets import load_dataset, DatasetDict, Features, Value, ClassLabel

# 1. åŠ è½½å†…ç½®æ•°æ®é›†
imdb = load_dataset("imdb")
print(f"IMDBæ•°æ®é›†: {imdb}")  # æŸ¥çœ‹æ•°æ®ç»“æ„

# 2. ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ•°æ®é›†
csv_dataset = load_dataset("csv", data_files={"train": "data/train.csv", "test": "data/test.csv"})

# 3. æ•°æ®å¤„ç†
# è¿‡æ»¤æ•°æ®
short_reviews = imdb["train"].filter(lambda x: len(x["text"]) < 1000)

# æ•°æ®æ˜ å°„
def add_length(example):
    example["length"] = len(example["text"])
    return example

dataset_with_length = imdb.map(add_length)

# 4. æ‰“ä¹±å’Œåˆ†å‰²æ•°æ®
train_test = imdb["train"].train_test_split(test_size=0.1)
print(f"è®­ç»ƒé›†å¤§å°: {len(train_test['train'])}, æµ‹è¯•é›†å¤§å°: {len(train_test['test'])}")

# 5. ä¿å­˜å’ŒåŠ è½½å¤„ç†åçš„æ•°æ®é›†
train_test.save_to_disk("./imdb_split")
reloaded_dataset = DatasetDict.load_from_disk("./imdb_split")

# 6. åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†
from datasets import Dataset
import pandas as pd

df = pd.DataFrame({
    "text": ["è¿™æ˜¯ç¬¬ä¸€ä¸ªæ ·æœ¬", "è¿™æ˜¯ç¬¬äºŒä¸ªæ ·æœ¬", "è¿™æ˜¯ç¬¬ä¸‰ä¸ªæ ·æœ¬"],
    "label": [0, 1, 0]
})

custom_dataset = Dataset.from_pandas(df)
```

## 4. é«˜çº§åº”ç”¨ä¸å˜ä½“

### æ¨¡å‹é‡åŒ–ä¸ä¼˜åŒ–

å¯¹å¤§å‹æ¨¡å‹è¿›è¡Œé‡åŒ–ä»¥å‡å°ä½“ç§¯å’Œæé«˜æ¨ç†é€Ÿåº¦ï¼š

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# é…ç½®8ä½é‡åŒ–
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

# åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "facebook/opt-6.7b",
    device_map="auto",
    quantization_config=quantization_config
)

# éªŒè¯æ¨¡å‹å¤§å°
model_size = sum(p.numel() for p in model.parameters()) * 1 / 8 / 1024 / 1024  # è½¬æ¢ä¸ºMB
print(f"é‡åŒ–åæ¨¡å‹å¤§å°: {model_size:.2f} MB")
```

### å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)

ä½¿ç”¨Low-Rank Adaptation (LoRA)æŠ€æœ¯è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼š

```python
from peft import get_peft_model, LoraConfig, TaskType
from transformers import AutoModelForCausalLM, AutoTokenizer

# 1. åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

# 2. å®šä¹‰LoRAé…ç½®
lora_config = LoraConfig(
    r=16,                      # LoRAçŸ©é˜µçš„ç§©
    lora_alpha=32,             # LoRA alphaå‚æ•°
    target_modules=["q_proj", "v_proj"],  # è¦åº”ç”¨LoRAçš„æ¨¡å—
    lora_dropout=0.05,         # LoRA dropout
    bias="none",               # æ˜¯å¦åŒ…æ‹¬åç½®å‚æ•°
    task_type=TaskType.CAUSAL_LM  # ä»»åŠ¡ç±»å‹
)

# 3. åˆ›å»ºPEFTæ¨¡å‹
peft_model = get_peft_model(model, lora_config)
print(f"å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {peft_model.print_trainable_parameters()}")

# 4. å¾®è°ƒPEFTæ¨¡å‹
# (ä½¿ç”¨ä¸å¸¸è§„å¾®è°ƒç±»ä¼¼çš„Trainer API)
...

# 5. æ¨ç†
input_text = "Once upon a time"
inputs = tokenizer(input_text, return_tensors="pt")
outputs = peft_model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# 6. ä¿å­˜å’ŒåŠ è½½PEFTæ¨¡å‹
peft_model.save_pretrained("./peft_model")
```

### åˆ†å¸ƒå¼è®­ç»ƒ

ä½¿ç”¨Accelerateåº“è¿›è¡Œå¤šGPUè®­ç»ƒï¼š

```python
from accelerate import Accelerator
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
from torch.utils.data import DataLoader

# 1. åˆå§‹åŒ–Accelerator
accelerator = Accelerator()

# 2. å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

# 3. å‡†å¤‡æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
train_dataloader = DataLoader(train_dataset, batch_size=16)
eval_dataloader = DataLoader(eval_dataset, batch_size=16)

# 4. ä½¿ç”¨acceleratorå‡†å¤‡æ‰€æœ‰ç»„ä»¶
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)

# 5. è®­ç»ƒå¾ªç¯
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)  # æ›¿ä»£loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
    # è¯„ä¼°
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)
```

### æ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡

ä½¿ç”¨Hugging Face Inference APIè¿›è¡Œæ¨¡å‹éƒ¨ç½²ï¼š

```python
import requests
import json

# ä½¿ç”¨Inference API (éœ€è¦Hugging Face APIä»¤ç‰Œ)
API_TOKEN = "your_api_token_here"
API_URL = "https://api-inference.huggingface.co/models/gpt2"

headers = {"Authorization": f"Bearer {API_TOKEN}"}

def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()

# ç¤ºä¾‹è¯·æ±‚
output = query({
    "inputs": "The quick brown fox jumps over the",
    "parameters": {
        "max_length": 50,
        "temperature": 0.7
    }
})

print(output)
```

æœ¬åœ°æ¨¡å‹æœåŠ¡å™¨éƒ¨ç½²ï¼š

```python
from transformers import pipeline
from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI()

# åŠ è½½æ¨¡å‹
classifier = pipeline("sentiment-analysis")

# å®šä¹‰è¯·æ±‚æ¨¡å‹
class TextRequest(BaseModel):
    text: str

# åˆ›å»ºAPIç«¯ç‚¹
@app.post("/analyze")
def analyze_sentiment(request: TextRequest):
    result = classifier(request.text)
    return {"result": result}

# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### ä½¿ç”¨Hugging Face Spaceså±•ç¤ºæ¨¡å‹

```python
# åˆ›å»ºapp.pyæ–‡ä»¶
import gradio as gr
from transformers import pipeline

# åŠ è½½æ¨¡å‹
generator = pipeline("text-generation", model="gpt2")

# å®šä¹‰é¢„æµ‹å‡½æ•°
def predict(prompt, max_length=100):
    outputs = generator(prompt, max_length=max_length, do_sample=True)
    return outputs[0]["generated_text"]

# åˆ›å»ºGradioæ¥å£
demo = gr.Interface(
    fn=predict,
    inputs=[
        gr.Textbox(placeholder="è¯·è¾“å…¥æç¤ºæ–‡æœ¬...", lines=3),
        gr.Slider(minimum=10, maximum=500, value=100, label="æœ€å¤§é•¿åº¦")
    ],
    outputs="text",
    title="GPT-2 æ–‡æœ¬ç”Ÿæˆå™¨",
    description="è¿™ä¸ªåº”ç”¨ä½¿ç”¨GPT-2æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ã€‚è¾“å…¥æç¤ºï¼Œæ¨¡å‹å°†ç»§ç»­å†™ä½œã€‚",
)

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    demo.launch()

# ç„¶åå¯ä»¥éƒ¨ç½²åˆ°Hugging Face Spaces
```

### å¤šè¯­è¨€æ¨¡å‹å’Œè·¨è¯­è¨€ä»»åŠ¡

```python
from transformers import MarianMTModel, MarianTokenizer

# åŠ è½½å¾·è¯­åˆ°è‹±è¯­çš„ç¿»è¯‘æ¨¡å‹
model_name = "Helsinki-NLP/opus-mt-de-en"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# ç¿»è¯‘æ–‡æœ¬
def translate(text):
    inputs = tokenizer(text, return_tensors="pt")
    outputs = model.generate(**inputs)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# å¾·è¯­åˆ°è‹±è¯­ç¿»è¯‘
german_text = "Ich liebe es, mit Hugging Face zu arbeiten."
english_text = translate(german_text)
print(f"å¾·è¯­: {german_text}")
print(f"è‹±è¯­: {english_text}")

# å¤šè¯­è¨€æ¨¡å‹(XLM-RoBERTa)
from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer

# åŠ è½½å¤šè¯­è¨€åˆ†ç±»æ¨¡å‹
model_name = "xlm-roberta-base"
tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)
model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)

# å¯ä»¥å¾®è°ƒå¤„ç†å¤šç§è¯­è¨€
```

### å¤šæ¨¡æ€åº”ç”¨

```python
from transformers import VisionTextDualEncoderModel, CLIPProcessor
import torch
from PIL import Image

# 1. åŠ è½½CLIPæ¨¡å‹å’Œå¤„ç†å™¨
model = VisionTextDualEncoderModel.from_pretrained("clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("clip-vit-base-patch32")

# 2. å‡†å¤‡å›¾åƒå’Œæ–‡æœ¬
image = Image.open("cat.jpg")
texts = ["ä¸€åªçŒ«", "ä¸€åªç‹—", "ä¸€è¾†æ±½è½¦", "ä¸€æ ‹æˆ¿å­"]

# 3. å¤„ç†è¾“å…¥
inputs = processor(text=texts, images=image, return_tensors="pt", padding=True)

# 4. è®¡ç®—ç›¸ä¼¼åº¦
with torch.no_grad():
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image  # å›¾åƒä¸æ–‡æœ¬çš„ç›¸ä¼¼åº¦åˆ†æ•°
    probs = logits_per_image.softmax(dim=1)      # å°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡
    
# 5. æ˜¾ç¤ºç»“æœ
for text, prob in zip(texts, probs[0]):
    print(f"'{text}': {prob:.4f}")
```

### è‡ªå®šä¹‰æ¨¡å‹ä¸Šä¼ åˆ°Hubåˆ†äº«

```python
from transformers import AutoModel, AutoTokenizer
from huggingface_hub import notebook_login

# 1. ç™»å½•Hugging Face Hub
notebook_login()

# 2. åŠ è½½å¹¶ä¿®æ”¹æ¨¡å‹
model_checkpoint = "bert-base-uncased"
model = AutoModel.from_pretrained(model_checkpoint)
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# 3. è‡ªå®šä¹‰ä¿®æ”¹...
# (ä¾‹å¦‚å¾®è°ƒã€æ¶æ„è°ƒæ•´ç­‰)

# 4. å°†æ¨¡å‹æ¨é€åˆ°Hub
model_name = "my-custom-bert"
model.push_to_hub(model_name)
tokenizer.push_to_hub(model_name)

# ç°åœ¨å¯ä»¥é€šè¿‡"your-username/my-custom-bert"è®¿é—®
```

## å®æˆ˜æ¡ˆä¾‹ï¼šæ„å»ºé—®ç­”ç³»ç»Ÿ

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„é—®ç­”ç³»ç»Ÿå®æˆ˜æ¡ˆä¾‹ï¼š

```python
import torch
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline
from datasets import load_dataset

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
model_name = "deepset/roberta-base-squad2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

# 2. åˆ›å»ºé—®ç­”pipeline
qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)

# 3. åŸºæœ¬é—®ç­”ç¤ºä¾‹
context = """
Hugging Faceæ˜¯ä¸€å®¶æ€»éƒ¨ä½äºçº½çº¦çš„AIåˆ›ä¸šå…¬å¸ï¼Œæˆç«‹äº2016å¹´ã€‚
è¯¥å…¬å¸å¼€å‘äº†ç”¨äºæ„å»ºåº”ç”¨ç¨‹åºçš„æœºå™¨å­¦ä¹ åº“ï¼Œæœ€åˆæ˜¯åŸºäº
PyTorchã€TensorFlowå’Œscikit-learnçš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ã€‚
ç°åœ¨ï¼Œä»–ä»¬æä¾›äº†transformersã€tokenizerså’Œdatasetsåº“ï¼Œ
è¿™äº›åº“å·²æˆä¸ºNLPç¤¾åŒºçš„é‡è¦å·¥å…·ã€‚2021å¹´ï¼Œ
å…¬å¸ç­¹é›†äº†4000ä¸‡ç¾å…ƒçš„èµ„é‡‘ï¼Œä¼°å€¼è¶…è¿‡5äº¿ç¾å…ƒã€‚
"""

questions = [
    "Hugging Faceæ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ",
    "Hugging Faceçš„ä¸»è¦äº§å“æ˜¯ä»€ä¹ˆï¼Ÿ",
    "Hugging Faceæ€»éƒ¨åœ¨å“ªé‡Œï¼Ÿ",
    "Hugging Faceåœ¨2021å¹´ç­¹é›†äº†å¤šå°‘èµ„é‡‘ï¼Ÿ"
]

for question in questions:
    result = qa_pipeline(question=question, context=context)
    print(f"é—®é¢˜: {question}")
    print(f"ç­”æ¡ˆ: {result['answer']}")
    print(f"ç½®ä¿¡åº¦: {result['score']:.4f}\n")

# 4. å¾®è°ƒé—®ç­”æ¨¡å‹
# åŠ è½½SQuADæ•°æ®é›†
squad_dataset = load_dataset("squad")

# æ•°æ®é¢„å¤„ç†
def preprocess_squad(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=384,
        truncation="only_second",
        stride=128,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length"
    )
    
    offset_mapping = inputs.pop("offset_mapping")
    sample_map = inputs.pop("overflow_to_sample_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []
    
    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        answer = answers[sample_idx]
        start_char = answer["answer_start"][0]
        end_char = start_char + len(answer["text"][0])
        
        sequence_ids = inputs.sequence_ids(i)
        
        # æ‰¾åˆ°ä¸Šä¸‹æ–‡çš„èµ·å§‹å’Œç»“æŸä½ç½®
        context_start = 0
        while sequence_ids[context_start] != 1:
            context_start += 1
        context_end = len(sequence_ids) - 1
        while sequence_ids[context_end] != 1:
            context_end -= 1
            
        # å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œæ ‡è®°ä¸ºä¸å¯èƒ½
        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)
            
            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)
    
    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs

# åº”ç”¨é¢„å¤„ç†
tokenized_squad = squad_dataset.map(
    preprocess_squad, 
    batched=True, 
    remove_columns=squad_dataset["train"].column_names
)

# å¾®è°ƒæ¨¡å‹
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_squad["train"],
    eval_dataset=tokenized_squad["validation"],
)

# å¼€å§‹è®­ç»ƒ
trainer.train()

# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
model.save_pretrained("./my-qa-model")
tokenizer.save_pretrained("./my-qa-model")
```

## æ€»ç»“ä¸æœ€ä½³å®è·µ

### Hugging Faceä½¿ç”¨æœ€ä½³å®è·µ

1. **é€‰æ‹©åˆé€‚çš„æ¨¡å‹**ï¼šæ ¹æ®ä»»åŠ¡å’Œèµ„æºé€‰æ‹©åˆé€‚å¤§å°å’Œæ¶æ„çš„é¢„è®­ç»ƒæ¨¡å‹
2. **ä¼˜åŒ–å¾®è°ƒè¿‡ç¨‹**ï¼šä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ã€æ··åˆç²¾åº¦è®­ç»ƒç­‰æŠ€æœ¯é™ä½èµ„æºéœ€æ±‚
3. **ç‰ˆæœ¬æ§åˆ¶**ï¼šé€šè¿‡è®¾ç½®ç‰¹å®šçš„æ¨¡å‹ç‰ˆæœ¬ä¿è¯å¯é‡ç°æ€§
4. **å‚æ•°é«˜æ•ˆå¾®è°ƒ**ï¼šå¯¹äºå¤§æ¨¡å‹ï¼Œä½¿ç”¨LoRAç­‰æŠ€æœ¯é™ä½è®¡ç®—éœ€æ±‚
5. **ç¼“å­˜ç®¡ç†**ï¼šè®¾ç½®åˆç†çš„ç¼“å­˜ç›®å½•å¹¶å®šæœŸæ¸…ç†ä¸éœ€è¦çš„æ¨¡å‹å’Œæ•°æ®é›†
6. **æ¨¡å‹é‡åŒ–**ï¼šåœ¨éƒ¨ç½²é˜¶æ®µä½¿ç”¨é‡åŒ–æŠ€æœ¯å‡å°æ¨¡å‹ä½“ç§¯

### å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|-----|---------|
| å†…å­˜ä¸è¶³ | å‡å°æ‰¹æ¬¡å¤§å°ã€ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ã€æ¨¡å‹å¹¶è¡Œ |
| è®­ç»ƒå¤ªæ…¢ | ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒã€å‡å°‘è¯„ä¼°é¢‘ç‡ã€LoRAç­‰ |
| æ¨¡å‹è¡¨ç°ä¸ä½³ | å°è¯•ä¸åŒé¢„è®­ç»ƒæ¨¡å‹ã€è°ƒæ•´å­¦ä¹ ç‡ã€å¢åŠ æ•°æ®å¢å¼º |
| æ¨ç†å¤ªæ…¢ | æ¨¡å‹é‡åŒ–ã€ä½¿ç”¨æ›´å°æ¨¡å‹ã€ONNXå¯¼å‡º |
| æ³„æ¼é¢„è®­ç»ƒæ•°æ® | ä½¿ç”¨å®ä½“è¿‡æ»¤ã€Promptå·¥ç¨‹æç¤ºåˆè§„æ€§ |

### æœªæ¥å‘å±•è¶‹åŠ¿

1. **æ›´é«˜æ•ˆçš„æ¨¡å‹ç»“æ„**ï¼šHybridsã€MoEç­‰èµ„æºåˆ©ç”¨æ›´é«˜æ•ˆçš„ç»“æ„
2. **æ›´å¼ºçš„å¤šæ¨¡æ€èƒ½åŠ›**ï¼šè·¨è¶Šæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘çš„ç»Ÿä¸€æ¨¡å‹
3. **é¢†åŸŸé€‚åº”ä¸ä¸ªæ€§åŒ–**ï¼šæ›´å¥½çš„ä¸“ä¸šé¢†åŸŸå’Œä¸ªäººåå¥½é€‚åº”æ–¹æ³•
4. **æœ¬åœ°éƒ¨ç½²çš„å¤§æ¨¡å‹**ï¼šé‡åŒ–å’Œå‰ªæä½¿å¤§æ¨¡å‹åœ¨æœ¬åœ°è®¾å¤‡å¯ç”¨
5. **è‡ªåŠ¨åŒ–ä¸AutoML**ï¼šè‡ªåŠ¨é€‰æ‹©å’Œä¼˜åŒ–æ¨¡å‹çš„å·¥å…·é“¾

Hugging Faceç”Ÿæ€ç³»ç»Ÿå·²ç»å½»åº•æ”¹å˜äº†æˆ‘ä»¬ä½¿ç”¨å’Œå¼€å‘AIæ¨¡å‹çš„æ–¹å¼ï¼Œä½¿å…ˆè¿›çš„AIæŠ€æœ¯æ°‘ä¸»åŒ–å¹¶æ˜“äºä½¿ç”¨ã€‚é€šè¿‡ç†è§£å…¶åŸºç¡€æ¦‚å¿µã€æŒæ¡æŠ€æœ¯ç»†èŠ‚ã€å®è·µåº”ç”¨å¹¶æ¢ç´¢é«˜çº§å˜ä½“ï¼Œä½ ç°åœ¨å·²ç»å…·å¤‡äº†å……åˆ†åˆ©ç”¨è¿™ä¸ªå¼ºå¤§å·¥å…·é“¾çš„èƒ½åŠ›ã€‚

æ— è®ºä½ æ˜¯ç ”ç©¶äººå‘˜ã€å¼€å‘äººå‘˜è¿˜æ˜¯AIçˆ±å¥½è€…ï¼ŒHugging Faceéƒ½æä¾›äº†ä¸€ä¸ªä¾¿æ·é€”å¾„ï¼Œå¸®åŠ©ä½ å®ç°ä»æƒ³æ³•åˆ°å®é™…åº”ç”¨çš„è¿‡ç¨‹ã€‚

Similar code found with 4 license types